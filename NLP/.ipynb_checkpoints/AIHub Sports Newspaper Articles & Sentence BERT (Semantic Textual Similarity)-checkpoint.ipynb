{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now(timezone(\"Asia/Seoul\"))\n",
    "\n",
    "print(\"- Loading data 'aihub_or_kr-sports_ko.xlsx'...\", end=\" \")\n",
    "data = pd.read_excel(\"aihub_or_kr-sports_ko.xlsx\")\n",
    "print(\"completed!\")\n",
    "\n",
    "print(\"- Downloading pre-trained Sentence BERT model 'jhgan/ko-sroberta-sts'...\", end=\" \")\n",
    "model = SentenceTransformer(\"jhgan/ko-sroberta-sts\")\n",
    "print(\"completed!\")\n",
    "\n",
    "print(\"- Encoding each text to 768-dimensional vectors....\")\n",
    "vecs = model.encode(data[\"text\"].tolist(), show_progress_bar=True, normalize_embeddings=True)\n",
    "print(\"completed!\")\n",
    "\n",
    "n_trees = 8\n",
    "print(f\"- Building a forest of {n_trees} trees for nearest neighbor search...\", end=\" \")\n",
    "dim = 768\n",
    "tree = AnnoyIndex(f=dim, metric=\"dot\")\n",
    "for i, vec in enumerate(vecs):\n",
    "    tree.add_item(i + 1, vec)\n",
    "tree.build(n_trees=n_trees, n_jobs=-1)\n",
    "print(\"completed!\")\n",
    "\n",
    "print(\"- Searching up to 5 nearest neighbors for each sentence...\")\n",
    "res = list()\n",
    "sim_thr = 0.9\n",
    "for id1 in tqdm(range(1, len(data) + 1)):\n",
    "    ids, sims = tree.get_nns_by_item(i=id1, n=5, include_distances=True)\n",
    "    for id2, sim in zip(ids, sims):\n",
    "        if sim >= sim_thr and id1 < id2:\n",
    "            res.append((id1, id2, sim))\n",
    "res = sorted(res, key=lambda x:x[2], reverse=True)\n",
    "res = pd.DataFrame(res, columns=[\"id1\", \"id2\", \"similarity\"])\n",
    "\n",
    "id2text = {row[\"id\"]:row[\"text\"] for _, row in data.iterrows()}\n",
    "res.insert(2, \"text1\", res[\"id1\"].map(id2text))\n",
    "res.insert(3, \"text2\", res[\"id2\"].map(id2text))\n",
    "\n",
    "res = res.sort_values(by=[\"similarity\", \"id1\", \"id2\"], ascending=[False, True, True])\n",
    "print(\"completed!\")\n",
    "\n",
    "print(\"- Saving the result as 'Semantic_textual_similarity_Result.xlsx'...\", end=\" \")\n",
    "res.to_excel(\"Semantic_textual_similarity_Result.xlsx\", index=False, encoding=\"euc-kr\")\n",
    "print(\"completed!\")\n",
    "\n",
    "end = datetime.now(timezone(\"Asia/Seoul\"))\n",
    "\n",
    "print(\"- All the precesses are done;\")\n",
    "print(f\"    - {'The program started at:':<24s}{datetime.strftime(start, format='%Y-%m-%d %H:%M:%S'):>20s}.\")\n",
    "print(f\"    - {'The program ended at:':<24s}{datetime.strftime(end, format='%Y-%m-%d %H:%M:%S'):>20s}.\")\n",
    "elapsed = (end - start).total_seconds()\n",
    "print(f\"    - {elapsed//60:,.0f}mins and {elapsed%60:,.0f}secs ({elapsed:,.0f}secs) elapsed.\")\n",
    "print(f\"    - {len(res):,} pair(s) of sentences showed a similarity of {sim_thr} or more.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
