{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import Dataset, load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import cer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"audio\": [\n",
    "            \"/Users/jongbeom.kim/project/corpus_raw/b2b_projects/2022/2022-PB-01/male/delivery/All_Light_Off/unknown_male_(20)_c_026_normal_000126.wav\",\n",
    "            \"/Users/jongbeom.kim/project/corpus_raw/b2b_projects/2022/2022-PB-01/male/delivery/All_Light_Off/unknown_male_(20)_c_026_normal_000226.wav\",\n",
    "            \"/Users/jongbeom.kim/project/corpus_raw/b2b_projects/2022/2022-PB-01/male/delivery/All_Light_Off/unknown_male_(20)_c_026_normal_000326.wav\",\n",
    "            \"/Users/jongbeom.kim/project/corpus_raw/b2b_projects/2022/2022-PB-01/male/delivery/All_Light_Off/unknown_male_(20)_c_026_normal_000426.wav\"\n",
    "        ]\n",
    "    }\n",
    ").cast_column(\"audio\", Audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(batch):\n",
    "    batch[\"file\"] = batch[\"audio\"][\"path\"]\n",
    "    audio_path = batch[\"audio\"][\"path\"]\n",
    "    txt_path = Path(str(audio_path).replace(\"wav\", \"txt\"))\n",
    "    # print(txt_path)\n",
    "    # print(open(txt_path, mode=\"r\").read())\n",
    "    batch[\"text\"] = open(txt_path, mode=\"r\").read()\n",
    "    return batch\n",
    "\n",
    "\n",
    "def map_to_array(batch):\n",
    "    # speech, _ = sf.read(batch[\"file\"])\n",
    "    # print(batch[\"audio\"][\"p)\n",
    "    speech, _ = sf.read(batch[\"file\"])\n",
    "    # print(speech)\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    inputs = processor(batch[\"speech\"], sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\")\n",
    "    # input_values = inputs.input_values.to(\"cuda\")\n",
    "    input_values = inputs.input_values\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "# # model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\").to('cuda')\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "# ds = load_dataset(path=\"kresnik/zeroth_korean\", name=\"clean\", split=\"test\")\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 22.79ex/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1222.92ex/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.61s/ba]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9310344827586207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ds = ds\n",
    "test_ds = test_ds.map(add_text)\n",
    "test_ds = test_ds.map(map_to_array)\n",
    "\n",
    "result = test_ds.map(map_to_pred, batched=True, batch_size=16, remove_columns=[\"speech\"])\n",
    "\n",
    "print(cer(result[\"text\"], result[\"transcription\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['일괄 소등', '일괄 소등 (약간빠른속도)', '일괄 소등', '일괄 소등']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아  수 다', '습니', '이 습대', '일끈다']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"transcription\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9802e389ec05078d05e103b384a9a92139ff4abf327eee27d78ec43fbb8fac3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('data_mgmt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
