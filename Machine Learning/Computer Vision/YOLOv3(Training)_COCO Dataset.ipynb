{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YOLOv3(Training)_COCO dataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpvZnHe4OeLv","executionInfo":{"status":"ok","timestamp":1618726125132,"user_tz":-540,"elapsed":4036,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"7e2ec383-95d5-48b3-a4af-f0e3f2a4cc65"},"source":["from google.colab import drive\n","import os\n","import sys\n","from IPython.display import HTML, display\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n","from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, LeakyReLU, BatchNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras import Input, Model, Sequential\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import time\n","import random\n","import colorsys\n","import numpy as np\n","from tensorflow.keras.regularizers import l2\n","from google.colab.patches import cv2_imshow\n","\n","drive.mount(\"/content/drive\", force_remount=True)\n","try:\n","    my_path = \"/content/notebooks\"\n","    os.symlink(\"/content/drive/MyDrive/ColabNotebooks/my_env\", my_path)\n","    sys.path.insert(0, my_path)\n","except:\n","    pass\n","os.chdir(my_path)\n","\n","def set_css():\n","  display(HTML(\"\"\"\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  \"\"\"))\n","get_ipython().events.register(\"pre_run_cell\", set_css)\n","\n","plt.style.use(\"dark_background\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"gg4JnH5wpq-R","executionInfo":{"status":"ok","timestamp":1618726125133,"user_tz":-540,"elapsed":4026,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"fdb65249-9e33-481b-8024-dbdb1f008d58"},"source":["input_size = 416\n","batch_size = 4\n","n_grids = [52, 26, 13]\n","strides = [416//grid for grid in n_grids]\n","anchors = [[[10, 13], [16, 30], [33, 23]], [[30, 61], [62, 45], [59, 119]], [[116, 90], [156, 198],\n","                                                                             [373, 326]]]\n","anchors = anchors/(np.array(strides).T[:, None, None])\n","max_bbox_per_scale = 100\n","\n","init_lr = 1e-4\n","fin_lr = 1e-6\n","warmup_epochs = 2\n","epochs = 30\n","\n","pref = \"/content/drive/My Drive/Computer Vision/racoon_data\"\n","tr_annot_path = pref + \"/racoon_train.txt\"\n","test_annot_path = pref + \"/racoon_test.txt\""],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"h_qnAiNGpoGI","executionInfo":{"status":"ok","timestamp":1618726125135,"user_tz":-540,"elapsed":4019,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"6c38854d-f83d-4c77-d745-37091a8da785"},"source":["# Define Genralized IoU\n","def compute_giou(bbox1, bbox2):\n","    # boxes : (x1, y1, x2, y2)\n","    bbox1 = np.array(bbox1)\n","    bbox2 = np.array(bbox2)\n","\n","    area_bbox1 = (bbox1[2] - bbox1[0])*(bbox1[3] - bbox1[1])\n","    area_bbox2 = (bbox2[2] - bbox2[0])*(bbox2[3] - bbox2[1])\n","\n","    pt1_intersec = np.maximum(bbox1[:2], bbox2[:2])\n","    pt2_intersec = np.minimum(bbox1[2:], bbox2[2:])\n","    w_intersec, h_intersec = np.maximum(pt2_intersec - pt1_intersec, 0)\n","    area_intersec = w_intersec*h_intersec\n","\n","    area_union = area_bbox1 + area_bbox2 - area_intersec\n","\n","    iou = np.maximum(area_intersec/area_union, np.finfo(np.float32).eps)\n","\n","    pt1_enclose = np.minimum(bbox1[:2], bbox2[:2])\n","    pt2_enclose = np.maximum(bbox1[2:], bbox2[2:])\n","    w_enclose, h_enclose = np.maximum(pt2_enclose - pt1_enclose, 0)\n","    area_enclose = w_enclose*h_enclose\n","\n","    return iou - (area_enclose - area_union)/area_enclose\n","\n","def preprocess_image(img, gt_boxes=None):\n","    tar_h = input_size\n","    tar_w = input_size\n","    h, w, _ = img.shape\n","\n","    scale = min(tar_h/h, tar_w/w)\n","\n","    new_w, new_h = int(scale*w), int(scale*h)\n","    img_resized = cv2.resize(img, dsize=(new_w, new_h))\n","\n","    img_paded = np.full(shape=(tar_h, tar_w, 3), fill_value=128.)\n","    pad_w, pad_h = (tar_w - new_w)//2, (tar_h - new_h)//2\n","    img_paded[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :] = img_resized\n","    img_paded = img_paded/255.\n","\n","    if gt_boxes is None:\n","        return img_paded\n","    else:\n","        # (x1, y1, x2, y2)\n","        gt_boxes[:, (0, 2)] = gt_boxes[:, (0, 2)]*scale + pad_w\n","        gt_boxes[:, (1, 3)] = gt_boxes[:, (1, 3)]*scale + pad_h\n","        return img_paded, gt_boxes"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"bVciagx0Xj6S","executionInfo":{"status":"ok","timestamp":1618726125136,"user_tz":-540,"elapsed":4000,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"6c67c573-a15c-46f3-d6a2-30aff2aa4b2f"},"source":["class BatchNormalization(BatchNormalization):\n","    # When `layer.trainable=False` is set(inference mode), the layer is frozen and will use stored\n","    # moving `var` and `mean` and both `gamma` and `beta` will not be updated.\n","    def call(self, x, training=False):\n","        if not training:\n","            training = tf.constant(False)\n","        training = tf.logical_and(training, self.trainable)\n","        return super().call(x, training)\n","\n","def convolutional(x, filters, kernel_size, downsample=False, activate=True, bn=True):\n","    if downsample == False:\n","        strides = 1\n","        padding = \"same\"\n","        z = x\n","    elif downsample == True:\n","        # top and left padding\n","        # shape: (batch_size, h, w, channels) -> (batch_size, h+1, w+1, channels)\n","        # the image size of the output is half the input.\n","        z = ZeroPadding2D(padding=((1, 0), (1, 0)))(x)\n","        strides = 2\n","        padding = \"valid\"\n","\n","    z = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n","                padding=padding, use_bias=not bn, kernel_regularizer=l2(0.0005),\n","                kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n","                bias_initializer=tf.constant_initializer(0.))(z)\n","    if bn == True:\n","        z = BatchNormalization()(z)\n","    if activate == True:\n","        z = LeakyReLU(alpha=0.1)(z)\n","\n","    return z\n","\n","def residual_block(x, filters):\n","    z = convolutional(x, filters=filters[0], kernel_size=1)\n","    z = convolutional(z, filters=filters[1], kernel_size=3)\n","\n","    return Add()([x, z])\n","\n","def upsample(x):\n","    return tf.image.resize(images=x, size=(x.shape[1]*2, x.shape[2]*2), method=\"nearest\")"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"OCQiMdKTng3J","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1618726126261,"user_tz":-540,"elapsed":5113,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"f67cdfe3-e4a7-4dfb-9a78-e02063a41abb"},"source":["# pred: pred, conv:, label: gt\n","def compute_loss(pred, conv, label, bboxes, i=0):\n","    # batch_size  = tf.shape(conv)[0]\n","    # output_size = n_grids[i]\n","    # (batch_size, n_grids[i], n_grids[i], 3*(5 + n_clss))\n","    # -> (batch_size, n_grids[i], n_grids[i], 3, 5 + n_clss)\n","    conv = tf.reshape(conv, (batch_size, n_grids[i], n_grids[i], 3, 5 + n_clss))\n","    conv_raw_conf = conv[:, :, :, :, 4]\n","    conv_raw_prob = conv[:, :, :, :, 5:]\n","\n","    pred_xywh     = pred[:, :, :, :, 0:4]\n","    pred_conf     = pred[:, :, :, :, 4] #시그머이드 반영\n","\n","    label_xywh    = label[:, :, :, :, 0:4]\n","    label_conf  = label[:, :, :, :, 4] #있으면 1 아니면 0\n","    label_prob    = label[:, :, :, :, 5:] #해당 클래스 있는것만 1 나머지 0\n","\n","    # print(pred_xywh.shape, label_xywh[..., None].shape)\n","    # giou = compute_giou(pred_xywh, label_xywh)[..., None]\n","    giou = np.array([compute_giou(pred_xywh, xywh) for xywh in label_xywh])[..., None]\n","    # input_size = tf.cast(input_size, tf.float32)\n","\n","    bbox_loss_scale = 2.0 - 1.0*label_xywh[:, :, :, :, 2]*label_xywh[:, :, :, :, 3]/(input_size**2)\n","    giou_loss = label_conf*bbox_loss_scale*(1 - giou)\n","\n","    # iou = bbox_iou(pred_xywh[:, :, :, :, None, :], bboxes[:, None, None, None, :, :])\n","    iou = np.array([compute_giou(pred_xywh[:, :, :, :, None, :], bbox)\\\n","                    for bbox in bboxes[:, None, None, None, :, :]])\n","    max_iou = tf.reduce_max(iou, axis=-1)[..., None]\n","\n","    respond_bgd = (1.0 - label_conf)*tf.cast(max_iou < iou_thrsd, tf.float32)\n","# RETINANET\n","    # conf_focal = tf.pow(label_conf - pred_conf, 2)\n","    conf_focal = (label_conf - pred_conf)**2\n","\n","    conf_loss = conf_focal*(label_conf*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf,\n","                                                                                 logits=conv_raw_conf)\n","            +respond_bgd*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf, logits=conv_raw_conf))\n","\n","    prob_loss = label_conf*tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n","\n","    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1, 2, 3, 4]))\n","    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))\n","    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))\n","\n","    return giou_loss, conf_loss, prob_loss\n","\n","idx2cls = {}\n","with open(\"/content/drive/My Drive/Computer Vision/model_data/coco.names\", \"r\") as data:\n","    for idx, cls in enumerate(data):\n","        idx2cls[idx] = cls.strip(\"\\n\")\n","n_clss = len(idx2cls)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"8L0X8LQxXvny","executionInfo":{"status":"ok","timestamp":1618726126262,"user_tz":-540,"elapsed":5105,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"8b38e25e-cfb9-475e-fcf1-eb18e2883da4"},"source":["def YOLOv3(training=False):\n","    input_size = 416\n","    channels = 3\n","    inputs  = Input([input_size, input_size, channels])\n","\n","    # Darknet 53 from here(totally 75 layers).\n","    z = convolutional(inputs, filters=32, kernel_size=3)\n","    z = convolutional(z, filters=64, kernel_size=3, downsample=True)\n","    for _ in range(1):\n","        z = residual_block(z, filters=[32, 64])\n","    z = convolutional(z, filters=128, kernel_size=3, downsample=True)\n","    for _ in range(2):\n","        z = residual_block(z, filters=[64, 128])\n","    z = convolutional(z, filters=256, kernel_size=3, downsample=True)\n","    for _ in range(8):\n","        z = residual_block(z, filters=[128, 256])\n","    route1 = z\n","\n","    z = convolutional(z, filters=512, kernel_size=3, downsample=True)\n","    for _ in range(8):\n","        z = residual_block(z, filters=[256, 512])\n","    route2 = z\n","\n","    z = convolutional(z, filters=1024, kernel_size=3, downsample=True)\n","    for _ in range(4):\n","        z = residual_block(z, filters=[512, 1024])\n","\n","    # YOLO v3 from here(totally 31 layers).\n","    z = convolutional(z, filters=512, kernel_size=1)\n","    z = convolutional(z, filters=1024, kernel_size=3)\n","    z = convolutional(z, filters=512, kernel_size=1)\n","    z = convolutional(z, filters=1024, kernel_size=3)\n","    z = convolutional(z, filters=512, kernel_size=1)\n","\n","    conv_lobj_branch = convolutional(z, filters=1024, kernel_size=3)\n","    # (batch_size, 13, 13, 3*(n_clss + 5))\n","    conv_lbbox = convolutional(conv_lobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n","                                activate=False, bn=False)\n","\n","    z = convolutional(z, filters=256, kernel_size=1)\n","    z = upsample(z)\n","\n","    z = Concatenate(axis=-1)([z, route2])\n","\n","    z = convolutional(z, filters=256, kernel_size=1)\n","    z = convolutional(z, filters=512, kernel_size=3)\n","    z = convolutional(z, filters=256, kernel_size=1)\n","    z = convolutional(z, filters=512, kernel_size=3)\n","    z = convolutional(z, filters=256, kernel_size=1)\n","\n","    conv_mobj_branch = convolutional(z, filters=512, kernel_size=3)\n","    # (batch_size, 26, 26, 3*(n_clss + 5))\n","    conv_mbbox = convolutional(conv_mobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n","                                activate=False, bn=False)\n","\n","    z = convolutional(z, filters=128, kernel_size=1)\n","    z = upsample(z)\n","\n","    z = Concatenate(axis=-1)([z, route1])\n","\n","    z = convolutional(z, filters=128, kernel_size=1)\n","    z = convolutional(z, filters=256, kernel_size=3)\n","    z = convolutional(z, filters=128, kernel_size=1)\n","    z = convolutional(z, filters=256, kernel_size=3)\n","    z = convolutional(z, filters=128, kernel_size=1)\n","\n","    conv_sobj_branch = convolutional(z, filters=256, kernel_size=3)\n","    # (batch_size, 52, 52, 3*(n_clss + 5))\n","    conv_sbbox = convolutional(conv_sobj_branch, filters=3*(n_clss + 5), kernel_size=1,\n","                                activate=False, bn=False)\n","\n","    outputs = []\n","    for i, conv_bbox in enumerate([conv_sbbox, conv_mbbox, conv_lbbox]):\n","        if training == True:\n","            outputs.append(conv_bbox)\n","        # (batch_size, output_size, output_size, 3*(5 + n_clss)\n","        # batch_size = tf.shape(conv_bbox)[0]\n","        output_size = n_grids[i]\n","        conv_bbox = tf.reshape(conv_bbox, shape=(batch_size, output_size, output_size, 3, 5 + n_clss))\n","        delta_xy = conv_bbox[:, :, :, :, 0:2]   \n","        delta_wh = conv_bbox[:, :, :, :, 2:4]\n","        conf = conv_bbox[:, :, :, :, 4:5]\n","        probs = conv_bbox[:, :, :, :, 5: ] \n","\n","        y = tf.range(output_size, dtype=tf.int32)\n","        y = tf.expand_dims(y, -1)\n","        y = tf.tile(y, [1, output_size])\n","        x = tf.range(output_size,dtype=tf.int32)\n","        x = tf.expand_dims(x, 0)\n","        x = tf.tile(x, [output_size, 1])\n","        xy_grid = tf.concat([x[:, :, None], y[:, :, None]], axis=-1)\n","        # (output_size, output_size, 2) -> (batch_size, output_size, output_size, 3, 2)\n","        xy_grid = tf.tile(xy_grid[None, :, :, None, :], [batch_size, 1, 1, 3, 1])\n","        xy_grid = tf.cast(xy_grid, tf.float32)\n","\n","        # The center of the predicted bboxes in the original 416x416 image space.\n","        xy = (tf.math.sigmoid(delta_xy) + xy_grid)*strides[i]\n","        wh = (tf.math.exp(delta_wh)*anchors[i])*strides[i]\n","        # xywh = tf.concat([xy, wh], axis=-1)\n","        conf = tf.math.sigmoid(conf)\n","        probs = tf.math.sigmoid(probs)\n","                \n","        # [(batch_size, 52, 52, 3, 85), (batch_size, 26, 26, 3 85), (batch_size, 13, 13, 3, 85)]\n","        outputs.append(tf.concat([xy, wh, conf, probs], axis=-1))\n","        # outputs.append(tf.concat([xywh, conf, probs], axis=-1))\n","\n","    return Model(inputs=inputs, outputs=outputs)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"CQ1zG7qH8XQV","executionInfo":{"status":"ok","timestamp":1618726128412,"user_tz":-540,"elapsed":7245,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"ccdf4dcc-507a-49f2-a435-75fc69cb5d65"},"source":["class Dataset(object):\n","    # Dataset preprocess implementation\n","    def __init__(self, dataset_type):\n","        # (x, y, w, h)\n","        self.annot_path = tr_annot_path if dataset_type == \"train\" else test_annot_path\n","        self.data_aug = True if dataset_type == \"train\" else False\n","\n","        with open(tr_annot_path, \"r\") as f:\n","            txt = f.readlines()\n","            annots = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n","            random.shuffle(annots)\n","        self.final_annots = list()\n","        for annot in annots:\n","            img_path, bbox = annot.split()\n","            self.final_annots.append([img_path, [bbox]])\n","\n","        self.n_samples = len(self.final_annots)\n","        self.n_batchs = int(np.ceil(self.n_samples/batch_size))\n","        self.batch_count = 0\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        # self.tr_input_size = random.choice([self.tr_input_sizes])\n","\n","        batch_img = np.zeros((batch_size, 416, 416, 3), dtype=np.float32)\n","\n","        batch_label_sbbox = np.zeros((batch_size, n_grids[0], n_grids[0],\n","                                        3, 5 + n_clss), dtype=np.float32)\n","        batch_label_mbbox = np.zeros((batch_size, n_grids[1], n_grids[1],\n","                                        3, 5 + n_clss), dtype=np.float32)\n","        batch_label_lbbox = np.zeros((batch_size, n_grids[2], n_grids[2],\n","                                        3, 5 + n_clss), dtype=np.float32)\n","\n","        batch_sbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n","        batch_mbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n","        batch_lbboxes = np.zeros((batch_size, max_bbox_per_scale, 4), dtype=np.float32)\n","\n","        num = 0\n","        if self.batch_count < self.n_batchs:\n","            while num < batch_size:\n","                idx = self.batch_count*batch_size + num\n","                if idx >= self.n_samples:\n","                    idx -= self.n_samples\n","                annot = self.final_annots[idx]\n","                # Parse annotation\n","                img = cv2.imread(pref + annot[0][1:])\n","                bboxes = np.array([list(map(int, box.split(','))) for box in annot[1]])\n","                # print(bboxes)\n","                if self.data_aug:\n","                    img, bboxes = self.random_horizontal_flip(img, bboxes)\n","                    img, bboxes = self.random_crop(img, bboxes)\n","                    img, bboxes = self.random_translate(img, bboxes)\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","                img, bboxes = preprocess_image(img=img, gt_boxes=bboxes)\n","\n","                # Preprocess GT bboxes.\n","                # def preprocess_true_boxes(self, bboxes):\n","                label = [np.zeros((n_grids[i], n_grids[i], 3,\n","                                5 + n_clss)) for i in range(3)]\n","                bboxes_xywh = [np.zeros((max_bbox_per_scale, 4)) for _ in range(3)]\n","                bbox_count = np.zeros((3,))\n","\n","                for bbox in bboxes:\n","                    bbox_coor = bbox[:4]\n","                    bbox_class_ind = bbox[4]\n","\n","                    onehot = np.zeros(n_clss, dtype=np.float)\n","                    onehot[bbox_class_ind] = 1.0\n","                    uniform_distribution = np.full(n_clss, 1.0 / n_clss)\n","                    deta = 0.01\n","                    smooth_onehot = onehot*(1 - deta) + deta*uniform_distribution\n","\n","                    bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2])*0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n","                    bbox_xywh_scaled = 1.0*bbox_xywh[None, :] / strides[:, None]\n","\n","                    iou = []\n","                    exist_positive = False\n","                    for i in range(3):\n","                        anchors_xywh = np.zeros((3, 4))\n","                        anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n","                        anchors_xywh[:, 2:4] = anchors[i]\n","\n","                        # iou_scale = bbox_iou(bbox_xywh_scaled[i][None, :], anchors_xywh)\n","                        # iou_scale = compute_giou(bbox_xywh_scaled[i][None, :], anchors_xywh)\n","                        iou_scale = np.array([compute_giou(bbox_xywh_scaled[i][None, :][0], anchor_xywh)\\\n","                                                 for anchor_xywh in anchors_xywh])\n","                        iou.append(iou_scale)\n","                        iou_mask = iou_scale > 0.3\n","\n","                        if np.any(iou_mask):\n","                            xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n","\n","                            label[i][yind, xind, iou_mask, :] = 0\n","                            label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n","                            label[i][yind, xind, iou_mask, 4:5] = 1.0\n","                            label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n","\n","                            bbox_ind = int(bbox_count[i] % max_bbox_per_scale)\n","                            bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n","                            bbox_count[i] += 1\n","\n","                            exist_positive = True\n","\n","                    if not exist_positive:\n","                        best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n","                        best_detect = int(best_anchor_ind / 3)\n","                        best_anchor = int(best_anchor_ind % 3)\n","                        xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n","\n","                        label[best_detect][yind, xind, best_anchor, :] = 0\n","                        label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n","                        label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n","                        label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n","                        \n","                        bbox_ind = int(bbox_count[best_detect] % max_bbox_per_scale)\n","                        bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n","                        bbox_count[best_detect] += 1\n","\n","                label_sbbox, label_mbbox, label_lbbox = label\n","                sbboxes, mbboxes, lbboxes = bboxes_xywh\n","                    # return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n","\n","\n","                # label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes =\\\n","                # self.preprocess_true_boxes(bboxes)\n","\n","                batch_img[num, :, :, :] = img\n","                batch_label_sbbox[num, :, :, :, :] = label_sbbox\n","                batch_label_mbbox[num, :, :, :, :] = label_mbbox\n","                batch_label_lbbox[num, :, :, :, :] = label_lbbox\n","                batch_sbboxes[num, :, :] = sbboxes\n","                batch_mbboxes[num, :, :] = mbboxes\n","                batch_lbboxes[num, :, :] = lbboxes\n","                num += 1\n","            self.batch_count += 1\n","            batch_smaller_target = batch_label_sbbox, batch_sbboxes\n","            batch_medium_target  = batch_label_mbbox, batch_mbboxes\n","            batch_larger_target  = batch_label_lbbox, batch_lbboxes\n","\n","            return batch_img, (batch_smaller_target, batch_medium_target, batch_larger_target)\n","        else:\n","            self.batch_count = 0\n","            np.random.shuffle(self.final_annots)\n","            raise StopIteration\n","\n","    def random_horizontal_flip(self, img, bboxes):\n","        # With 50% probability.\n","        if random.random() < 0.5:\n","            _, w, _ = img.shape\n","            img = img[:, ::-1, :]\n","            bboxes[:, [0, 2]] = w - bboxes[:, [2, 0]]\n","\n","        return img, bboxes\n","\n","    def random_crop(self, img, bboxes):\n","        if random.random() < 0.5:\n","            h, w, _ = img.shape\n","            # bboxes: (x1, y1, x2, y2)\n","            # cv2_imshow(img)\n","            # print(bboxes)\n","            enclose = np.concatenate([np.min(bboxes[:, 0:2], axis=0),\n","                                      np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n","            \n","            max_l_trans = enclose[0]\n","            max_u_trans = enclose[1]\n","            max_r_trans = w - enclose[2]\n","            max_d_trans = h - enclose[3]\n","\n","            crop_xmin = max(0, int(enclose[0] - random.uniform(0, max_l_trans)))\n","            crop_ymin = max(0, int(enclose[1] - random.uniform(0, max_u_trans)))\n","            crop_xmax = max(w, int(enclose[2] + random.uniform(0, max_r_trans)))\n","            crop_ymax = max(h, int(enclose[3] + random.uniform(0, max_d_trans)))\n","\n","            img = img[crop_ymin:crop_ymax, crop_xmin:crop_xmax, :]\n","\n","            bboxes[:, (0, 2)] = bboxes[:, (0, 2)] - crop_xmin\n","            bboxes[:, (1, 3)] = bboxes[:, (1, 3)] - crop_ymin\n","\n","        return img, bboxes\n","\n","    def random_translate(self, img, bboxes):\n","        if random.random() < 0.5:\n","            h, w, _ = img.shape\n","            enclose = np.concatenate([np.min(bboxes[:, 0:2], axis=0),\n","                                      np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n","\n","            max_l_trans = enclose[0]\n","            max_u_trans = enclose[1]\n","            max_r_trans = w - enclose[2]\n","            max_d_trans = h - enclose[3]\n","\n","            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n","            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n","\n","            img = cv2.warpAffine(src=img, M=np.array([[1, 0, tx], [0, 1, ty]]), dsize=(w, h))\n","\n","            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n","            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n","\n","        return img, bboxes\n","\n","    def __len__(self):\n","        return self.n_batchs\n","\n","trainset = Dataset(\"train\")"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"eMp-XBHSXicX","executionInfo":{"status":"ok","timestamp":1618726132095,"user_tz":-540,"elapsed":10916,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"004d16d6-db0f-4ab7-aa0f-8b10357ff339"},"source":["global train_from_checkpoint\n","\n","save_best_only = True\n","# Saves all the best validated checkpoints in training process.(This may require a lot of disk spaces.)\n","save_checkpoints = False\n","\n","trainset = Dataset(\"train\")\n","testset = Dataset(\"test\")\n","\n","steps_per_epoch = len(trainset)\n","global_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\n","warmup_steps = warmup_epochs*steps_per_epoch\n","tot_steps = epochs*steps_per_epoch\n","\n","model = YOLOv3(training=True)\n","\n","# model.summary()"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":388},"id":"hF1xPglTzkLk","executionInfo":{"status":"error","timestamp":1618726133239,"user_tz":-540,"elapsed":12052,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}},"outputId":"bd686778-a234-42f7-f3ba-0308c6b86743"},"source":["trainset = Dataset(\"train\")\n","transfer_learning = False\n","if transfer_learning:\n","    # Transfer learning from Darknet 53 weights.\n","    # Resets all state generated by Keras.\n","    tf.keras.backend.clear_session()\n","\n","    with open(\"/content/drive/My Drive/Computer Vision/model_data/yolov3.weights\", \"rb\") as f:\n","        _, _, _, _, _ = np.fromfile(f, dtype=np.int32, count=5)\n","\n","        j = 0\n","        for i in range(75):\n","            conv_layer = model.get_layer(\"conv2d\" if i == 0 else f\"conv2d_{i}\")\n","            filters = conv_layer.filters\n","            kernel_size = conv_layer.kernel_size[0]\n","            in_dim = conv_layer.input_shape[-1]\n","\n","            if i not in [58, 66, 74]:\n","                # order: [beta, gamma, mean, variance](darknet) -> [gamma, beta, mean, variance](tf)\n","                bn_weights = np.fromfile(f, dtype=np.float32, count=4*filters).reshape((4, filters))[[1, 0, 2, 3]]\n","                bn_layer = model.get_layer(\"batch_normalization\" if j == 0 else f\"batch_normalization_{j}\")\n","                j += 1\n","            else:\n","                conv_bias = np.fromfile(f, dtype=np.float32, count=filters)\n","\n","            conv_shape = (filters, in_dim, kernel_size, kernel_size)\n","            # shape: (out_dim, in_dim, h, w) -> (h, w, in_dim, out_dim)\n","            conv_weights = np.fromfile(f, dtype=np.float32, count=np.prod(conv_shape)).reshape(conv_shape)\\\n","            .transpose((2, 3, 1, 0))\n","\n","            if i not in [58, 66, 74]:\n","                conv_layer.set_weights([conv_weights])\n","                bn_layer.set_weights(bn_weights)\n","            else:\n","                conv_layer.set_weights([conv_weights, conv_bias])\n","\n","# if train_from_checkpoint:\n","#     model.load_weights(\"./checkpoints/yolov3_custom\")\n","\n","opt = tf.keras.optimizers.Adam()\n","best_val_loss = 1000 # should be large at start\n","for epoch in range(epochs):\n","    for img_data, target in trainset:\n","        with tf.GradientTape() as tape:\n","            pred_result = model(img_data, training=True)\n","            giou_loss = conf_loss = prob_loss=0\n","            # optimizing process\n","            for i in range(3):\n","                conv, pred = pred_result[i*2], pred_result[i*2 + 1]\n","                loss_items = compute_loss(pred, conv,*target[i], i)\n","                giou_loss += loss_items[0]\n","                conf_loss += loss_items[1]\n","                prob_loss += loss_items[2]\n","            tot_loss = giou_loss + conf_loss + prob_loss\n","            gras = tape.gradient(tot_loss, yolo.trainable_variables)\n","            opt.apply_gradients(zip(grads, yolo.trainable_variables))\n","\n","            # update learning rate\n","            global_steps.assign_add(1)\n","            if global_steps < warmup_steps:# and not TRAIN_TRANSFER:\n","                lr = global_steps / warmup_steps*init_lr\n","            else:\n","                lr = fin_lr + 0.5*(init_lr - fin_lr)*(\n","                    (1 + tf.cos((global_steps - warmup_steps) / (tot_steps - warmup_steps)*np.pi)))\n","            optimizer.lr.assign(lr.numpy())\n","\n","        results = global_steps.numpy(), opt.lr.numpy(), giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), tot_loss.numpy()\n","        cur_step = results[0]%steps_per_epoch\n","        print(f\"epoch:{epoch:2.0f} step:{cur_step:5.0f}/{steps_per_epoch}, lr:{results[1]:.6f},\\\n","        giou_loss:{results[2]:7.2f}, conf_loss:{results[3]:7.2f}, prob_loss:{results[4]:7.2f},\\\n","        tot_loss:{results[5]:7.2f}\")\n","\n","    count, giou_val, conf_val, prob_val, tot_val = 0., 0, 0, 0, 0\n","    for img_data, target in testset:\n","        with tf.GradientTape() as tape:\n","            pred_result = model(img_data, training=False)\n","            giou_loss=conf_loss=prob_loss=0\n","\n","            # optimizing process\n","            for i in range(3):\n","                conv, pred = pred_result[i*2], pred_result[i*2+1]\n","                loss_items = compute_loss(pred, conv,*target[i], i)\n","                giou_loss += loss_items[0]\n","                conf_loss += loss_items[1]\n","                prob_loss += loss_items[2]\n","            tot_loss = giou_loss + conf_loss + prob_loss\n","\n","        results = giou_loss.numpy(), conf_loss.numpy(), prob_loss.numpy(), tot_loss.numpy()\n","        count += 1\n","        giou_val += results[0]\n","        conf_val += results[1]\n","        prob_val += results[2]\n","        tot_val += results[3]\n","        \n","    print(f\"\\n\\ngiou_val_loss:{giou_val/count:7.2f}, conf_val_loss:{conf_val/count:7.2f},\\\n","    prob_val_loss:{prob_val/count:7.2f}, tot_val_loss:{tot_val/count:7.2f}\\n\\n.\")\n","\n","    if save_best_only and best_val_loss > tot_val/count: \n","        model.save_weights(\"/content/drive/My Drive/Computer Vision/checkpoints/yolov3_custom\")\n","        best_val_loss = tot_val/count"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {white-space: pre-wrap;}\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-bd4c6c2986a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;31m# should be large at start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mpred_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-a854ac9643a0>\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mbbox_xywh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox_coor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mbbox_xywh_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbbox_xywh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"]}]},{"cell_type":"code","metadata":{"id":"9S33GwqLjbpY","executionInfo":{"status":"aborted","timestamp":1618726133237,"user_tz":-540,"elapsed":12040,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}}},"source":["YOLOv3(training=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKS9tanWXjI4","executionInfo":{"status":"aborted","timestamp":1618726133238,"user_tz":-540,"elapsed":12038,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}}},"source":["ID = random.randint(0, 200)\n","# model.load_weights(\"./checkpoints/yolov3_custom\")\n","ori_img = cv2.imread(\"/content/drive/My Drive/Computer Vision/Allhydrants-1920x1080-ca61f9ea607efb2f02f1ef97b781ee0f.jpg\")\n","\n","img_paded = preprocess_image(img=ori_img)\n","# Add a dimension for batch size.\n","img_paded = img_paded[None, ...]\n","\n","pred_bboxes = model.predict(img_paded)\n","pred_bboxes = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bboxes]\n","# [(52*52*3, 85), (26*26*3, 85), (13*13*3, 85)] -> ((52*52*3 + 26*26*3 + 13*13*3), 85)\n","pred_bboxes = tf.concat(pred_bboxes, axis=0)\n","\n","coors = pred_bboxes[:, :4]\n","confs = pred_bboxes[:, 4]\n","probs = pred_bboxes[:, 5:]\n","\n","# shape: (x, y, w, h) -> (x1, y1, x2, y2)\n","coors = np.concatenate([coors[:, :2] - coors[:, 2:]*0.5,\n","                        coors[:, :2] + coors[:, 2:]*0.5], axis=-1)\n","\n","# (x1, y1, x2, y2) -> (x1_ori, y1_ori, x2_ori, y2_ori)\n","ori_h, ori_w = ori_img.shape[:2]\n","resize_ratio = min(input_size/ori_w, input_size/ori_h)\n","\n","pad_w = (input_size - resize_ratio*ori_w)/2\n","pad_h = (input_size - resize_ratio*ori_h)/2\n","\n","coors[:, (0, 2)] = (coors[:, (0, 2)] - pad_w)/resize_ratio\n","coors[:, (1, 3)] = (coors[:, (1, 3)] - pad_h)/resize_ratio\n","\n","# Discard bboxes with larger (x2, y2) than (x1, y1).\n","coors[:, :4] = np.concatenate([np.maximum(coors[:, (0, 1)], 0),\n","                            np.minimum(coors[:, (2, 3)], [ori_w-1, ori_h-1])], axis=-1)\n","coors[np.logical_or((coors[:, 0] > coors[:, 2]), (coors[:, 1] > coors[:, 3]))] = 0\n","\n","# Discard bboxes with negative areas.\n","areas = np.sqrt(np.multiply.reduce(coors[:, (2, 3)] - coors[:, (0, 1)], axis=-1))\n","scale_mask = np.logical_and((areas > 0), (areas < np.inf))\n","\n","# Discard bboxes with scores less than 0.3\n","argmax = np.argmax(probs, axis=-1)\n","scores = confs*np.max(probs, axis=-1)\n","score_mask = scores > 0.3\n","\n","pick_mask = np.logical_and(scale_mask, score_mask)\n","coors, scores, argmax = coors[pick_mask], scores[pick_mask], argmax[pick_mask]\n","bboxes = np.concatenate([coors, scores[:, None], argmax[:, None]], axis=-1)\n","\n","# Perform non_maximum_suppression\n","# bboxes = np.array(bboxes)\n","clss_in_img = list(set(bboxes[:, 5]))\n","best_bboxes = []\n","for cls in clss_in_img:\n","    bboxes_cls = bboxes[bboxes[:, 5] == cls]\n","    # Process 1: Determine whether the number of bounding boxes is greater than 0 \n","    while len(bboxes_cls) > 0:\n","        # Process 2: Select the bounding box with the highest score according to socre order A\n","        argmax = np.argmax(bboxes_cls[:, 4])\n","        best_bbox = bboxes_cls[argmax]\n","        best_bboxes.append(best_bbox)\n","\n","        bboxes_cls = np.delete(bboxes_cls, argmax, axis=0)\n","\n","        # Process 3: Calculate this bounding box A and remain all iou of the bounding box and remove\n","        # those bounding boxes whose iou value is higher than the thrsd.\n","        ious = np.array([compute_giou(best_bbox[:4], bbox_cls[:4]) for bbox_cls in bboxes_cls])\n","\n","        bboxes_cls = bboxes_cls*(ious <= 0.45)[:, None]\n","        bboxes_cls = bboxes_cls[bboxes_cls[:, 4] > 0]\n","\n","bboxes = best_bboxes\n","# Draw bboxes\n","img_h, img_w, _ = ori_img.shape\n","\n","hsv_tuples = [(idx/n_clss, 1, 1) for idx in idx2cls.keys()]\n","colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n","colors = list(map(lambda x: (int(x[0]*255.), int(x[1]*255.), int(x[2]*255.)), colors))\n","\n","random.seed(0)\n","random.shuffle(colors)\n","random.seed(None)\n","\n","for bbox in bboxes:\n","    coor = np.array(bbox[:4], dtype=np.int32)\n","    score = bbox[4]\n","    cls_idx = int(bbox[5])\n","    bbox_color = colors[cls_idx]\n","    bbox_thk = int(0.6*(img_h + img_w)/1000)\n","    bbox_thk = 1 if bbox_thk < 1 else bbox_thk\n","    font_scale = 0.75*bbox_thk\n","    (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n","\n","    cv2.rectangle(img=ori_img, pt1=(x1, y1), pt2=(x2, y2), color=bbox_color, thickness=bbox_thk*2)\n","\n","    score_str = f\"{score:.1%}\" \n","    label = f\"{idx2cls[cls_idx]} \" + score_str\n","\n","    (text_w, text_h), baseline = cv2.getTextSize(text=label, fontFace=cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                                                        fontScale=font_scale, thickness=bbox_thk)\n","    cv2.rectangle(img=ori_img, pt1=(x1, y1), pt2=(x1+text_w, y1+text_h+baseline),\n","                color=bbox_color, thickness=cv2.FILLED)\n","    cv2.putText(img=ori_img, text=label, org=(x1, y1+12), fontFace=cv2.FONT_HERSHEY_COMPLEX_SMALL,\n","                fontScale=font_scale, color=(0, 0, 0), thickness=bbox_thk, lineType=cv2.LINE_AA)\n","\n","cv2_imshow(ori_img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dTRgAsURHhDf","executionInfo":{"status":"aborted","timestamp":1618726133238,"user_tz":-540,"elapsed":12036,"user":{"displayName":"Jongbeom Kim","photoUrl":"","userId":"17252605958116038360"}}},"source":[""],"execution_count":null,"outputs":[]}]}