{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열이 분석효과에 도움 될 시간영역(해상도)을 선택해야 함\n",
    "- Aim for the most granular level possible.\n",
    "- 일반적으로 Frequency가 감소하면 더 나은 예측이 가능할 것 같지만 반대의 경우가 많음.\n",
    "- 만약 너무 세분화된 시간영역을 사용할 시 오류가 증가될 수 있음\n",
    "- 연간 비즈니스 목표를 예측하는데 일별/시간별/분별/이하단위의 데이터를 사용하면 도움이 될까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 데이터/분석은 높은 정확도를 낳거나 높은 에러를 발생시킴\n",
    "- High Accuracy: 과거 패턴이 미래에도 그대로 유지가 된다면 예측 정확도가 높아짐\n",
    "- High Error: 패턴이 점차적으로 또는 갑자기 변경되면 예측값은 실제값에서 크게 벗어날 수 있음\n",
    "    - Black Swan: <U>일어날 것 같지 않은 일이 일어나는 현상</U>\n",
    "    - White Swan: <U>과거 경험들로 충분히 예상되는 위기지만 대응책이 없고 반복될 현상</U>\n",
    "    - Gray Swan: <U>과거 경험들로 충분히 예상되지만 발생되면 충격이 지속되는 현상</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 데이터 관리는 장/단점 존재\n",
    "- 수천/수백만/수십억 데이터를 기계학습에 사용할 수 있지만 시계열로 데이터를 정리하면 데이터 감소 발생 가능\n",
    "- 모든 시간범위가 예측성능에 도움되지 않을 수 있기에 특정기간의 시간영역 분석만 필요할 수도 있음\n",
    "- 고성능 시계열 Database 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (시계열) 회귀분석 요약\n",
    "\n",
    "> **\"$t$개의 값을 가지는 $k$차원 독립변수 $X_i$와 이에 대응하는 종속변수 $Y$간의 관계를 정량적으로 찾는 알고리즘\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링  \n",
    "\n",
    "- **알고리즘:** 독립 변수나 종속 변수가 반드시 대칭 분포를 보여야 한다는 가정은 없지만 정규 분포에 가까운 분포를 보일 수록 선형회귀모형의 성능이 좋아지는 경우가 많음  \n",
    "\n",
    "<center>\n",
    "$Y \\approx \\hat{Y} = f(X_1, X_2, ..., X_k) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k \\\\\n",
    "= [\\beta_0~\\beta_1~\\beta_2~\\cdots~\\beta_k]\\begin{bmatrix} 1 \\\\ X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_k \\end{bmatrix}\n",
    "= [1~X_1~X_2~\\cdots~X_k]\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}\n",
    "= \\begin{bmatrix} 1~X_{11}~X_{21}~\\cdots~X_{k1} \\\\ 1~X_{12}~X_{22}~\\cdots~X_{k2} \\\\ \\vdots \\\\ 1~X_{1t}~X_{2t}~\\cdots~X_{kt} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} = X\\beta$\n",
    "</center>\n",
    "\n",
    "- **비선형변수 효과:** 로그 또는 제곱근 등의 변환된 변수 사용시 회귀분석 성능 향상 가능\n",
    "    - 독립 변수나 종속 변수가 심하게 한쪽으로 치우친 분포를 보이는 경우\n",
    "    - 독립 변수와 종속 변수간의 관계가 곱셈 혹은 나눗셉으로 연결된 경우\n",
    "    - 종속 변수와 예측치가 비선형 관계를 보이는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증방항(계수추정) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정론적 모형(Deterministic Model)\n",
    "> **\"잔차제곱합(Residual Sum of Squares)을 최소로하는 $\\beta$를 추정\"**  \n",
    "\n",
    "**1) 잔차벡터(Residual Vector):** \n",
    "<center>$\\epsilon = Y - \\hat{Y} = Y - X\\beta$</center> \n",
    "\n",
    "**2) 잔차제곱합(Residual Sum of Squares):** \n",
    "<center>$RSS = \\epsilon^T\\epsilon = (Y - X\\beta)^T(Y - X\\beta) = Y^TY-2Y^TX\\beta+\\beta^TX^TX\\beta$</center> \n",
    "\n",
    "**3) 잔차제곱합의 그레디언트(Gradient):** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = -2X^TY + 2X^TX\\beta$</center> \n",
    "\n",
    "**4) 잔차가 최소가 되는 최적화 조건은 최저점에서의 그레디언트(미분,기울기)이 0이 되어야 함:** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = 0$</center> \n",
    "\n",
    "**5) 최적화를 위한 잔차제곱합의 그레디언트(Gradient):** \n",
    "<center>$\\dfrac{dRSS}{d\\beta} = -2X^TY + 2X^TX\\beta = 0 \\\\ X^TX\\beta = X^TY$</center> \n",
    "\n",
    "**6) 추정된 계수:** \n",
    "<center>$\\beta = (X^TX)^{-1}X^TY$</center>    \n",
    "\n",
    "- **Summary:**\n",
    "    - $X^TX$ 행렬이 역행렬이 존재해야 해 추정/존재 가능  \n",
    "    - 역행렬이 미존재  \n",
    "    = $X$가 서로 독립이 아님  \n",
    "    = $X$가 Full Rank가 아님  \n",
    "    = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률론적 모형(Probabilistic Model)\n",
    "> **\"종속변수의 발생가능성을 최대(최소)로하는 $\\beta$를 추정\"**\n",
    "- **필요성:** 결정론적 선형 회귀모형(OLS)는 데이터의 확률론적 가정이 없기에 단하나의 가중치(점추정)를 계산하나, 이 가중치의 신뢰도(구간추정)는 확인할 수 없음\n",
    "- **예시:** 집값에 대한 범죄율 영향력(가중치)이 -0.108이라면, 집값은 범죄율에 반비례한다 결론 내릴 수 있을까?\n",
    "    - -0.108는 오로지 우리가 보유한 테스트 1회성 결과일 뿐 오차가 존재가능\n",
    "    - 만약 오차가 0.0001이라면 실제 가중치 신뢰구간은 -0.108$\\pm$0.0001 (-0.1081 ~ -0.1079)이기에 집값과 범죄율 반비례 결론 가능\n",
    "    - 만약 오차가 0.2라면 실제 가중치는 (-0.308 ~ 0.092)이기에 가중치는 0이나 양수도 가능 -> 집값과 범죄율은 정비례도 가능\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && Y \\approx \\hat{Y} &= f(X_1, X_2, ..., X_k) \\\\\n",
    "&& &= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k \\\\\n",
    "&& &= E(Y|X_1, X_2, ... , X_k) \\\\ \n",
    "&& &\\sim \\mathcal{N}(X \\beta, \\sigma^2) \\\\\n",
    "&& p(Y \\mid X, \\theta) &= \\mathcal{N}(y \\mid X \\beta, \\sigma^2 ) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Error Poperties} && p(\\epsilon \\mid \\theta) &= \\mathcal{N}(0, \\sigma^2 ) \\text{  from  } \\epsilon = Y - X \\beta \\\\\n",
    "&& \\text{E}(\\epsilon \\mid X) &= 0 \\\\\n",
    "&& \\text{E}(\\epsilon) &= \\text{E}(\\text{E}(\\epsilon \\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon X) &= \\text{E}(\\text{E}(\\epsilon X \\mid X)) = \\text{E}(X \\text{E}(\\epsilon\\mid X)) = 0 \\\\\n",
    "&& \\text{E}(\\epsilon^2) &= \\sigma^2 (N-K) \\\\\n",
    "&& \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) &= 0 \\;\\; (i,j=1,2,\\ldots,N)\n",
    "\\end{align*}\n",
    "\n",
    "- **Summary:**\n",
    "    - $X, Y$ 중 어느 것도 정규분포일 필요는 없음  \n",
    "    - $Y$는 $X$에 대해 조건부로 정규분포를 따르며 $Y$자체가 무조건부로 정규분포일 필요는 없음  \n",
    "    - 잔차의 기대값은 0  \n",
    "    - 잔차의 조건부 기대값은 0  \n",
    "    - 잔차와 독립변수 $X$는 상관관계 없음  \n",
    "    - $X$와 무관하게 잔차들간의 공분산은 0  \n",
    "    \n",
    "**1) Y의 발생가능성(Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "p(Y_{1:N} \\,\\big|\\, X_{1:N}, \\theta) &= \\prod_{i=1}^N \\mathcal{N}(Y_i \\,\\big|\\, X_i \\beta_i , \\sigma^2) \\\\\n",
    "&= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i- X_i \\beta_i)^2}{2\\sigma^2} \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "**2) 더하기 사용을 위한 Log 변환(Log-Likelihood):** \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{LL} &= \\log p(Y_{1:N} \\,\\big|\\, X_{1:N}, \\theta) \\\\\n",
    "&= \\log \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(Y_i-X_i \\beta_i)^2}{2\\sigma^2} \\right\\}  \\\\\n",
    "&= -\\dfrac{1}{2\\sigma^2} \\sum_{i=1}^N (Y_i-X_i \\beta_i)^2 - \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2}  \\\\\n",
    "\\text{LL(Matrix Form)} &= -C_1 (Y - X\\beta)^T(y-X\\beta) - C_0 \\\\\n",
    "&= -C_1(\\beta^TX^TX\\beta -2 Y^TX\\beta + Y^TY) - C_0 \\\\\n",
    "& \\text{where } C_1=  -\\dfrac{1}{2\\sigma^2}, C_0 =  \\dfrac{N}{2} \\log{2\\pi}{\\sigma^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**3) Log-Likelihood의 그레디언트(미분,기울기)는 0이 되어야 함:** \n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{d\\beta} \\text{LL} &= -C_1 \\left( 2X^TX \\hat{\\beta} - 2X^TY \\right) = 0 \\\\\n",
    "\\hat{\\beta} &= (X^TX)^{-1}X^T Y \\\\\n",
    "\\end{align*}\n",
    "\n",
    "- **Summary:**\n",
    "    - $X^TX$ 행렬이 역행렬이 존재해야 해 추정/존재 가능  \n",
    "    - 역행렬이 미존재  \n",
    "    = $X$가 서로 독립이 아님  \n",
    "    = $X$가 Full Rank가 아님  \n",
    "    = $X^TX$가 양의 정부호(Positive Definite)가 아님\n",
    "    \n",
    "\n",
    "- **회귀계수의 분포:**\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Main Equation} && \\hat{\\beta} &= (X^TX)^{-1} X^T Y \\\\\n",
    "&& &= (X^TX)^{-1} X^T (X \\beta + \\epsilon) \\\\\n",
    "&& &= \\beta + (X^TX)^{-1} X^T \\epsilon \\\\\n",
    "\\text{Expectation} && \\text{E}(\\hat{\\beta}) &=  \\text{E}( \\beta + (X^TX)^{-1} X^T \\epsilon ) \\\\\n",
    "&& &=  \\beta + (X^TX)^{-1} X^T \\text{E}( \\epsilon ) \\\\\n",
    "&& &= \\beta \\\\\n",
    "\\text{Variance} && \\text{Var}(\\hat{\\beta}_i)  &= \\left( \\text{Cov}(\\hat{\\beta}) \\right)_{ii} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Covariance} && \\text{Cov}(\\hat{\\beta}) &= E\\left((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T \\right) \\\\\n",
    "&& &= E\\left(((X^TX)^{-1} X^T \\epsilon)((X^TX)^{-1} X^T \\epsilon)^T \\right) \\\\\n",
    "&& &= E\\left((X^TX)^{-1} X^T \\epsilon \\epsilon^T X(X^TX)^{−1} \\right) \\\\\n",
    "&& &= (X^TX)^{-1} X^T E(\\epsilon \\epsilon^T) X(X^TX)^{−1} \\\\\n",
    "&& &= (X^TX)^{-1} X^T (\\sigma^2 I) X(X^TX)^{−1} \\\\\n",
    "&& &= \\sigma^2  (X^TX)^{-1} \\\\\n",
    "\\text{Standard Deviation} && \\sqrt{\\text{Var}(\\hat{\\beta}_i)} \\approx {se_{\\hat{\\beta}_i}} &= \\sqrt{\\sigma^2 \\big((X^TX)^{-1}\\big)_{ii}} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\text{Asymptotic} && \\dfrac{\\hat{\\beta}_i - \\beta_i}{se_{\\hat{\\beta}_i}} &\\sim t_{N-K} \\;\\; (i=0, \\ldots, K-1) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시간현실 반영\n",
    "- 미래의 시간패턴을 미리 반영하는건 비현실적, 이는 과적합(Overfitting)을 유발\n",
    "\n",
    "### 예측 정확성 향상\n",
    "- 조건수 감소 목적\n",
    "    - (비수학적 이해) 독립변수들의 절대적 수치크기나 서로간의 의존도가 분석결과에 주는 영향을 줄이고 독립변수의 상대적인 비교효과 반영\n",
    "    - (수학적 이해) 공분산 행렬의 변동성을 줄여 분석결과의 변동을 줄임\n",
    "           \n",
    "$$Condition\\ Number = \\dfrac{\\lambda_{max}}{\\lambda_{min}}, \\\\ where \\\\\n",
    "\\lambda_{max} = max(eigenvalue(Cov(X^T X)))\\\\\n",
    "\\lambda_{min} = min(eigenvalue(Cov(X^T X)))$$\n",
    "\n",
    "- **조건수 감소 방법(분석 결과 안정성 확보 방법):** \n",
    "    > **1) 변수들의 단위차이로 숫자의 스케일들이 크게 다른 경우, 스케일링(Scaling)으로 해결 가능**  \n",
    "    > **2) 독립변수들 간에 상관관계가 높은 \"다중공선성\" 존재할 경우,  \n",
    "    Variance Inflation Factor(VIF)나 Principal Component Analysis(PCA)를 통한 변수선별로 해결 가능**  \n",
    "    > **3) 독립변수들 간 의존성이 높은 변수들에 패널티를 부여하는 정규화(Resularization)로 해결 가능**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "- 목적:\n",
    "    - (하드웨어) PC 메모리를 고려하여 오버플로우(Overflow)나 언더플로우(Underflow)를 방지 \n",
    "    - (소프트웨어) 독립 변수의 공분산 행렬 조건수(Condition Number)를 감소시켜 최적화 안정성 및 수렴 속도 향상 \n",
    "\n",
    "1) Standard Scaler: <center>$\\dfrac{X_{it} - E(X_i)}{SD(X_i)}$</center>\n",
    "- 각 변수(Feature)가 정규분포를 따른다는 가정이기에 정규분포가 아닐 시 최선이 아닐 수 있음  \n",
    "<center><img src='Image/Scaling_StandardScaler.png' width='500'></center>\n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.StandardScaler()\n",
    "~~~\n",
    "\n",
    "2) Min-Max Scaler: <center>$\\dfrac{X_{it} - min(X_i)}{max(X_i) - min(X_i)}$</center>\n",
    "- 가장 많이 활용되는 알고리즘으로 최소\\~최대 값이 0\\~1 또는 -1\\~1 사이의 값으로 변환  \n",
    "- 각 변수(Feature)가 정규분포가 아니거나 표준편차가 매우 작을 때 효과적\n",
    "- Outliers에 취약.\n",
    "<center><img src='Image/Scaling_MinMaxScaler.png' width='500'></center>\n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.MinMaxScaler()\n",
    "~~~\n",
    "\n",
    "3) Robust Scaler: <center>$\\dfrac{X_{it} - Q_1(X_i)}{Q_3(X_i) - Q_1(X_i)}$</center>\n",
    "- 최소-최대 스케일러와 유사하지만 최소/최대 대신에 IQR(Interquartile Range) 중 25%값/75%값을 사용하여 변환  \n",
    "- 이상치(Outlier)의 영향을 최소화하였기에 이상치가 있는 데이터에 효과적이고 적은 데이터에도 효과적인 편  \n",
    "<center><img src='Image/Scaling_RobustScaler.png' width='500'></center>\n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.RobustScaler()\n",
    "~~~\n",
    "\n",
    "4) Normalizer: <center>$\\dfrac{X_{it}}{\\sqrt{X_{i}^2 + X_{j}^2 + ... + X_{k}^2}}$</center>\n",
    "> 각 변수(Feature)를 전체 $n$개 모든 변수들의 크기들로 나누어서 변환(by Cartesian Coordinates)  \n",
    "> 각 변수들의 값은 원점으로부터 반지름 1만큼 떨어진 범위 내로 변환  \n",
    "\n",
    "<center><img src='Image/Scaling_Normalizer.png' width='500'></center>\n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.Normalizer()\n",
    "~~~\n",
    "\n",
    "- [비교 예시](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-download-auto-examples-preprocessing-plot-all-scaling-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "- 다중공선성(Multicollinearity) 발생:\n",
    "    - 독립변수의 일부가 다른 독립변수의 조합으로 표현될 수 있는 경우 \n",
    "    - 독립변수들이 서로 독립이 아니라 상호상관관계가 강한 경우 \n",
    "    - 독립변수의 공분산 행렬(Covariance Matrix) 벡터공간(Vector Space)의 차원과 독립변수의 차원이 같지 않는 경우(Full Rank가 아니다) \n",
    "        - 다중공선성이 있으면 독립변수 공분산 행렬의 조건수(Condition Number)가 증가\n",
    "        - 조건수(Condition Number)가 증가하면 분석결과 과적합(Overfitting)이 발생할 가능성 증가\n",
    "\n",
    "#### Variance Inflation Factor(VIF)\n",
    "- 독립변수를 다른 독립변수들로 선형회귀한 성능.\n",
    "- 이를 통해 상호 가장 의존적인 독립변수를 제거.  \n",
    "- 의존성이 낮은(분산이 작은) 독립변수들을 선택하거나, 의존성이 높은(분산이 높은) 독립변수들을 제거할 수 있음.\n",
    "\n",
    "$$VIF_i = Var(\\hat{\\beta}_i) = \\dfrac{\\sigma^2_{\\epsilon}}{(n-1)Var(X_i)} \\cdot \\dfrac{1}{1-R_i^2} \\\\\n",
    "(R_i^2:~독립변수~X_i를~다른~독립변수들로~선형회귀한~성능(결정계수))$$\n",
    "\n",
    "#### Principal Component Analysis(PCA)\n",
    "- \"PCA는 다차원인 독립변수 행렬을 서로 독립인 소차원의 독립변수 행렬로 바꾸는 알고리즘으로, 이를 통해 상호 의존성을 제거\"\n",
    "- Feature의 의미가 훼손됨."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "504px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
