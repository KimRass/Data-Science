# Paper Understanding
- [Unsupervised Representation Learning by Predicting Image Rotations](https://arxiv.org/pdf/1803.07728.pdf)
## Related Works
- In order to learn features, [1] and [2] train ConvNets to colorize gray scale images, [3] and [4] predict the relative position of image patches, and [5] predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames.
- The core intuition of our self-supervised feature learning approach is that if someone is not aware of the concepts of the objects depicted in the images, he cannot recognize the rotation that was applied to them.
- Other successful cases of unsupervised feature learning are clustering based methods ([6], [7] and [8]), reconstruction based methods (Bengio et al., 2007; Huang et al., 2007; Masci et al., 2011), and methods that involve learning generative probabilistic models Goodfellow et al. (2014); Donahue et al. (2016); Radford et al. (2015).
## Introduction
- Self-supervised learning defines an annotation free pretext task, using only the visual information present on the images or videos, in order to provide a surrogate supervision signal for feature learning.
## Methodolgy
- We propose to train a ConvNet model $F$ to estimate the geometric transformation applied to an image that is given to it as input. Specifically, we define a set of $K$ discrete geometric transformations $G = \{g(; k)\}^{K}_{k = 1}$, where $g(; k)$ is the operator that applies the geometric transformation $k$ to image $X$ that yields the transformed image $X^{k} = g(X ; k)$. The ConvNet model $F$ gets an image $X^{k^*}$ (where $k^{∗}$ is unknown to model $F$) as input and yields as output a probability distribution over all possible geometric transformations:
$$p(X^{k^{∗}}; \theta) = \{p^{k}(X^{k^{*}}; \theta)\}^{K}_{k = 1}$$
- where $p^{y}(X^{k^{∗}}; \theta)$ is the predicted probability for the geometric transformation $k$ and $\theta$ are the learnable parameters of model $F$. Therefore, given a set of $N$ training images $D = \{X_{i}\}^{N}_{i = 1}$, the self-supervised training objective that the ConvNet model must learn to solve is:
$$\argmin_{\theta}\frac{1}{N}\sum^{N}_{i = 1}loss(X_{i}, \theta)$$
- where the loss function $loss$ is defined as:
$$loss(X_{i}, \theta) = −\frac{1}{K}\sum^{K}_{k = 1}\log\bigg(p^{k}\big(g(X_{i}; k); \theta\big)\bigg)$$
- Figure 2
    - <img src="https://user-images.githubusercontent.com/67457712/227820083-dea02047-3d9d-43d0-8936-e4172c94aeec.png" width="700">
    - In the above formulation, the geometric transformations G must define a classification task that should force the ConvNet model to learn semantic features useful for visual perception tasks (e.g., object detection or image classification). In our work we propose to define the set of geometric transformations G as all the image rotations by multiples of 90 degrees, i.e., 2d image rotations by 0, 90, 180, and 270 degrees (see Figure 2). More formally, if Rot(X, φ) is an operator that rotates image X by φ degrees, then our set of geometric transformations consists of the K = 4 image rotations G = {g(X|y)} 4 y=1, where g(X|y) = Rot(X,(y − 1)90).
- The core intuition behind using these image rotations as the set of geometric transformations relates to the simple fact that **it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task unless it has first learned to recognize and detect classes of objects as well as their semantic parts in images. More specifically to successfully predict the rotation of an image the ConvNet model must necessarily learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images.***
- Absence of low-level visual artifacts: An additional important advantage of using image rotations by multiples of 90 degrees over other geometric transformations, is that they can be implemented by flip and transpose operations ***that do not leave any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks. In contrast, had we decided to use as geometric transformations, e.g., scale and aspect ratio image transformations, in order to implement them we would need to use image resizing routines that leave easily detectable image artifacts.***
- No ambiguity: Given an image rotated by 0, 90, 180, or 270 degrees, ***there is usually no ambiguity of what is the rotation transformation (with the exception of images that only depict round objects). In contrast, that is not the case for the object scale that varies significantly on human captured images.***
### Training
- ***We found that we get significant improvement when during training we train the network by feeding it all the four rotated copies of an image simultaneously instead of each time randomly sampling a single rotation transformation. Therefore, at each training batch the network sees 4 times more images than the batch size.***
## Experiments
- Figure 3
    - <img src="https://user-images.githubusercontent.com/105417680/228284692-dde6840d-8b27-4f2a-acee-27d90688d22b.png" width="800">
    - '(b)': We visualize some attention maps generated by a model trained on the rotation recognition task.
    - These attention maps are computed based on the magnitude of activations at each spatial cell of a convolutional layer and essentially reflect where the network puts most of its focus in order to classify an input image. ***We observe that in order for the model to accomplish the rotation prediction task it learns to focus on high level object parts in the image, such as eyes, nose, tails, and heads.***
    - '(a)': By comparing them with the attention maps generated by a model trained on the object recognition task in a supervised way we observe that both models seem to focus on roughly the same image regions.
- Figure 4: Visualization of the first layer filters of AlexNet
    - <img src="https://user-images.githubusercontent.com/67457712/227821123-ea732b32-49ca-4783-a849-9ec18da55a78.png" width="500">
    - '(a)': First layer filters learned by a AlexNet model trained on the supervised object recognition task
    - '(b)': Trained on the proposed rotation recognition task. As can be seen, they appear to have a big variety of edge filters on multiple orientations and multiple frequencies. Remarkably, these filters seem to have a greater amount of variety even than the filters learned by the supervised object recognition task ('(a)').
- Table 2: The quality of the self-supervised learned features w.r.t. the number of recognized rotations
    - <img src="https://user-images.githubusercontent.com/67457712/227821317-0973c388-b632-47b6-80a6-cb531009e778.png" width="400">
    - We explore how the quality of the self-supervised features depends on the number of discrete rotations used in the rotation prediction task. For that purpose we defined three extra rotation recognition tasks: 
        - '(a)': One with 8 rotations that includes all the multiples of 45 degrees,
        - '(b)': One with only the 0 and 180 degrees rotations
        - '(c)': One with only the 90 and 270 degrees rotations.
    - We observe that for 4 discrete rotations we achieve better object recognition performance than the 8 or 2 cases. We believe that this is because ***the 2 orientations case offers too few classes for recognition (i.e., less supervisory information is provided) while in the 8 orientations case the geometric transformations are not distinguishable enough and furthermore the 4 extra rotations introduced may lead to visual artifacts on the rotated images.***
    - We observe that among the RotNet models trained with 2 discrete rotations, the RotNet model trained with 90 and 270 degrees rotations achieves worse object recognition performance than the model trained with the 0 and 180 degrees rotations, which is probably due to the fact that the former model does not 'see' the 0 degree rotation during the unsupervised phase that is typically used during the object recognition training phase. (Comment: 모델이 object recognition에 대해 supervised learning 중에는 회전되지 않은 이미지(0 degree rotation)를 보게 되는데, image rotation에 대해 self-supervised learning 중에는 90도나 270도 회전된 이미지만 보지 회전되지 않은 이미지는 보지 못하지 때문입니다.)
## References
- [1] [Colorful Image Colorization](https://arxiv.org/pdf/1603.08511.pdf)
- [2] [Learning Representations for Automatic Colorization](https://arxiv.org/pdf/1603.06668.pdf)
- [3] [Unsupervised Visual Representation Learning by Context Prediction](https://arxiv.org/pdf/1505.05192.pdf)
- [4] [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https://arxiv.org/pdf/1603.09246.pdf)
- [5] [Learning to See by Moving](https://arxiv.org/pdf/1505.01596.pdf)
- [6] [Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks](https://arxiv.org/pdf/1406.6909.pdf)
- [7] [Learning Deep Parsimonious Representations]
- [8] [Joint Unsupervised Learning of Deep Representations and Image Clusters]




<!-- - Table 1
    - <img src="https://user-images.githubusercontent.com/67457712/227821225-f98e1b5d-b626-4d7f-a209-9d2213079a89.png" width="400"> -->
- Table 3
    - <img src="https://user-images.githubusercontent.com/67457712/227821425-cd89da20-88b6-4eb5-8538-8d3187895bce.png" width="500">
    - Evaluation of unsupervised feature learning methods on CIFAR-10. The Supervised NIN and the (Ours) RotNet + conv entries have exactly the same architecture but the first was trained fully supervised while on the second the first 2 conv. blocks were trained unsupervised with our rotation prediction task and the 3rd block only was trained in a supervised manner. In the Random Init. + conv entry a conv. classifier (similar to that of (Ours) RotNet + conv) is trained on top of two NIN conv. blocks that are randomly initialized and stay frozen. Note that each of the prior approaches has a different ConvNet architecture and thus the comparison with them is just indicative.
- Figure 5
    - <img src="https://user-images.githubusercontent.com/67457712/227821533-bd5118a8-bb46-41ba-b37f-275d0bf6e391.png" width="700">
    - (a) Plot with the rotation prediction accuracy and object recognition accuracy as a function of the training epochs used for solving the rotation prediction task. The red curve is the object recognition accuracy of a fully supervised model (a NIN model), which is independent from the training epochs on the rotation prediction task. The yellow curve is the object recognition accuracy of an object classifier trained on top of feature maps learned by a RotNet model at different snapshots of the training procedure. (b) Accuracy as a function of the number of training examples per category in CIFAR-10. Ours semi-supervised is a NIN model that the first 2 conv. blocks are RotNet model that was trained in a self-supervised way on the entire training set of CIFAR-10 and the 3rd conv. block along with a prediction linear layer that was trained with the object recognition task only on the available set of labeled images.
- Table 4
    - <img src="https://user-images.githubusercontent.com/67457712/227821622-1da08ecd-3d06-463e-bd57-3724eb0d0412.png" width="500">
    - Task Generalization: ImageNet top-1 classification with non-linear layers. We compare our unsupervised feature learning approach with other unsupervised approaches by training non-linear classifiers on top of the feature maps of each layer to perform the 1000-way ImageNet classification task, as proposed by Noroozi & Favaro (2016). For instance, for the conv5 feature map we train the layers that follow the conv5 layer in the AlexNet architecture (i.e., fc6, fc7, and fc8). Similarly for the conv4 feature maps. We implemented those non-linear classifiers with batch normalization units after each linear layer (fully connected or convolutional) and without employing drop out units. All approaches use AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labels and Random entries. During testing we use a single crop and do not perform flipping augmentation. We report top-1 classification accuracy.
- Table 5
    - <img src="https://user-images.githubusercontent.com/67457712/227821722-02b98536-58d2-412b-8725-9c7efa96e90e.png" width="500">
    - Task Generalization: ImageNet top-1 classification with linear layers. We compare our unsupervised feature learning approach with other unsupervised approaches by training logistic regression classifiers on top of the feature maps of each layer to perform the 1000-way ImageNet classification task, as proposed by Zhang et al. (2016a). All weights are frozen and feature maps are spatially resized (with adaptive max pooling) so as to have around 9000 elements. All approaches use AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labels and Random entries.
- Table 6
    - <img src="https://user-images.githubusercontent.com/67457712/227821818-66866861-cdf6-47a9-8575-a1ebcdcb8a9d.png" width="500">
    - Task & Dataset Generalization: Places top-1 classification with linear layers. We compare our unsupervised feature learning approach with other unsupervised approaches by training logistic regression classifiers on top of the feature maps of each layer to perform the 205-way Places classification task (Zhou et al., 2014). All unsupervised methods are pre-trained (in an unsupervised way) on ImageNet. All weights are frozen and feature maps are spatially resized (with adaptive max pooling) so as to have around 9000 elements. All approaches use AlexNet variants and were pretrained on ImageNet without labels except the Place labels, ImageNet labels, and Random entries.
- Table 7
    - <img src="https://user-images.githubusercontent.com/67457712/227821916-3fbfa1df-7878-4f54-8b4d-ee3bfda663c6.png" width="500">
    - Task & Dataset Generalization: PASCAL VOC 2007 classification and detection results, and PASCAL VOC 2012 segmentation results. We used the publicly available testing frameworks of Krahenb ¨ uhl et al. (2015) for classification, of Girshick (2015) for detection, and ¨ of Long et al. (2015) for segmentation. For classification, we either fix the features before conv5 (column fc6-8) or we fine-tune the whole model (column all). For detection we use multi-scale training and single scale testing. All approaches use AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labels and Random entries. After unsupervised training, we absorb the batch normalization units on the linear layers and we use the weight rescaling technique proposed by Krahenb ¨ uhl et al. (2015) (which is common among the unsupervised methods). ¨ As customary, we report the mean average precision (mAP) on the classification and detection tasks, and the mean intersection over union (mIoU) on the segmentation task.