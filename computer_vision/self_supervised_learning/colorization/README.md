# Paper Summary
- [Colorful Image Colorization](https://arxiv.org/pdf/1603.08511.pdf)

- However, for this paper, our goal is not necessarily to recover the actual ground truth color, but rather to produce a plausible colorization that could potentially fool a human observer. Therefore, our task becomes much more achievable: to model enough of the statistical dependencies between the semantics and the textures of grayscale images and their color versions in order to produce visually compelling results.

- Given the lightness channel L, our system predicts the corresponding a and b color channels of the image in the CIE Lab colorspace. Predicting color has the nice property that training data is practically free: any color photo can be used as a training example, simply by taking the image’s L channel as input and its ab channels as the supervisory signal.

- previous works have trained convolutional neural networks (CNNs) to predict color on large datasets [1,2]. However, the results from these previous attempts tend to look desaturated. One explanation is that [1,2] use loss functions that encourage conservative predictions. These losses are inherited from standard regression problems, where the goal is to minimize Euclidean error between an estimate and the ground truth. To appropriately model the multimodal nature of the problem, we predict a distribution of possible colors for each pixel. Furthermore, we re-weight the loss at training time to emphasize rare colors. This encourages our model to exploit the full diversity of the large-scale data on which it is trained. Lastly, we produce a final colorization by taking the annealed- mean of the distribution.

- We set up a “colorization Turing test,” in which we show participants real and synthesized colors for an image, and ask them to identify the fake. In this quite difficult paradigm, we are able to fool participants on 32% of the instances (ground truth colorizations would achieve 50% on this metric), signif- icantly higher than prior work [2].

- able 4. Our network architecture. X spatial resolution of output, C number of chan- nels of output; S computation stride, values greater than 1 indicate downsampling following convolution, values less than 1 indicate upsampling preceding convolution; D kernel dilation; Sa accumulated stride across all preceding layers (product over all strides in previous layers); De effective dilation of the layer with respect to the input (layer dilation times accumulated stride); BN whether BatchNorm layer was used after layer; L whether a 1x1 conv and cross-entropy loss layer was imposed

- color prediction is inherently multimodal – many objects can take on several plausible colorizations. For example, an apple is typically red, green, or yellow, but unlikely to be blue or orange.

- Given an input lightness channel X ∈ RH×W×1 , our objective is to learn a mapping Yb = F(X) to the two associated color channels Y ∈ RH×W×2 , where H, W are image dimensions. (We denote predictions with a b· symbol and ground truth without.) We per- form this task in CIE Lab color space. Because distances in this space model perceptual distance, a natural objective function, as used in [1,2], is the Eu- clidean loss L2(·, ·) between predicted and ground truth colors: L2(Yb , Y) = 1 2 X h,w kYh,w − Yb h,wk 2 2 (1) However, this loss is not robust to the inherent ambiguity and multimodal nature of the colorization problem. If an object can take on a set of distinct ab values, the optimal solution to the Euclidean loss will be the mean of the set. In color prediction, this averaging effect favors grayish, desaturated results. Additionally, if the set of plausible colorizations is non-convex, the solution will in fact be out of the set, giving implausible results.

- Instead, we treat the problem as multinomial classification. We quantize the ab output space into bins with grid size 10 and keep the Q = 313 values which are in-gamut, as shown in Figure 3(a). For a given input X, we learn a mapping Zb = G(X) to a probability distribution over possible colors Zb ∈ [0, 1]H×W×Q, where Q is the number of quantized ab values. To compare predicted Zb against ground truth, we define function Z = H −1 gt (Y), which converts ground truth color Y to vector Z, using a soft-encoding scheme2 . We then use multinomial cross entropy loss Lcl(·, ·), defined as: Lcl(Zb, Z) = − X h,w v(Zh,w) X q Zh,w,q log(Zbh,w,q) (2) where v(·) is a weighting term that can be used to rebalance the loss based on color-class rarity, as defined in Section 2.2 below. Finally, we map probability distribution Zb to color values Yb with function Yb = H(Zb), which will be further discussed in Section 2.3. 2.2 Class rebalancing The distribution of ab values in natural images is strongly biased towards val- ues with low ab values, due to the appearance of backgrounds such as clouds, pavement, dirt, and walls. Figure 3(b) shows the empirical distribution of pix- els in ab space, gathered from 1.3M training images in ImageNet [28]. Observe that the number of pixels in natural images at desaturated values are orders of magnitude higher than for saturated values. Without accounting for this, the loss function is dominated by desaturated ab values. We account for the class- imbalance problem by reweighting the loss of each pixel at train time based on the pixel color rarity. This is asymptotically equivalent to the typical approach of resampling the training space [32]. Each pixel is weighed by factor w ∈ RQ, based on its closest ab bin.

- 2 Each ground truth value Yh,w can be encoded as a 1-hot vector Zh,w by searching for the nearest quantized ab bin. However, we found that soft-encoding worked well for training, and allowed the network to quickly learn the relationship between elements in the output space [31]. We find the 5-nearest neighbors to Yh,w in the output space and weight them proportionally to their distance from the ground truth using a Gaussian kernel with σ = 5.