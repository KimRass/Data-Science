# Paper Reading
- [Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization](https://arxiv.org/pdf/1704.04232.pdf)
## Related Works
- However, due to intra-category variations or relying only on a classification objective, ***these methods often fail to identify the entire extent of the object and instead localize only the most discriminative part.***
- Weakly-supervied object localization
    - Fully-supervised convolutional networks (CNNs) require expensive human annotations for training (e.g. bounding box for object localization). To alleviate expensive annotation costs, weakly-supervised approaches learn using cheaper labels, for example, image-level labels for predicting an object’s location [3] [8] [9] [32] [43] [61].
    - Most weakly-supervised object localization approaches mine discriminative features or patches in the data that frequently appear in one class and rarely in other classes [3] [7] [9] [13] [41] [42] [43] [55]. However, ***these approaches tend to focus only on the most discriminative parts, and thus fail to cover the entire spatial extent of an object.***
    - Recent work modify CNN architectures designed for image classification so that the convolutional layers learn to localize objects while performing image classification [32] [61]. Other network architectures have been designed for weakly-supervised object detection [4] [20] [24]. Although these methods have significantly improved the state-of-the-art, ***they still essentially rely on a classification objective and thus can fail to capture the full extent of an object if the less discriminative parts do not help improve classification performance.***
- Masking pixels or activations
    - ***For object localization, [1] [59] train a CNN for image classification and then localize the regions whose masking leads to a large drop in classification performance. Since these approaches mask out the image regions only during testing and not during training, the localized regions are limited to the highly-discriminative object parts.***
    -  [1] hides patches during testing but not during training. For [1], since the network has already learned to focus on the most discimirinative parts during training, it is essentially too late, and hiding patches during testing has no significant effect on localization performance.
    - Our work is closely related to the adversarial erasing method [56], which iteratively trains a sequence of models for weakly-supervised semantic segmentation. Each model identifies the relevant object parts conditioned on the previous iteration model’s output.
    - Dropout [44] and its variants [47] [49] are also related. There are two main differences: (1) these methods are designed to prevent overfitting while our work is designed to improve localization; and (2) in dropout, units in a layer are dropped randomly, while in our work, contiguous image regions or video frames are dropped.
## Methodology
- Figure 1: Main idea
    - <img src="https://user-images.githubusercontent.com/105417680/230265308-f5fc7dc8-1994-4f90-8d8e-f37516545aa6.png" width="350">
    - (Top row): A network tends to focus on the most discriminative parts of an image (e.g., face of the dog) for classification.
    - (Bottom row):
        - By hiding images patches randomly, we can force the network to focus on other relevant object parts in order to correctly classify the image as 'dog'.
        - If we randomly remove some patches from the image then there is a possibility that the dog’s face, which is the most discriminative, will not be visible to the model. In this case, ***the model must seek other relevant parts like the tail and legs in order to do well on the classification task.*** By randomly hiding different patches in each training epoch, the model sees different parts of the image and is forced to focus on multiple relevant parts of the object beyond just the most discriminative one. Importantly, ***we only apply this random hiding of patches during training and not during testing.***
- Figure 2
    - <img src="https://user-images.githubusercontent.com/105417680/230295312-ce83fdeb-bf98-4b01-bbed-4935310372ef.png" width="700">
    - (Left): ***For each training image, we divide it into a grid of*** $S \times S$ ***patches. Each patch is then randomly hidden with probability*** $p_{hide}$ ***and given as input to a CNN to learn image classification. The hidden patches change randomly across different epochs.***
    - (Right): ***We hide patches only during training. During testing, the full image—without any patches hidden—is given as input to the network.*** Since the network has learned to focus on multiple relevant parts during training, it is not necessary to hide any patches during testing.
- We make changes to the input image. ***The key idea is to hide patches from an image during training so that the model needs to seek the relevant object parts from what remains.*** We thus name our approach 'Hide-and-Seek'.
- For weakly-supervised object localization, we are given a set of images in which each image is labeled only with its category label. ***Our goal is to learn an object localizer that can predict both the category label as well as the bounding box for the object-of-interest in a new test image. In order to learn the object localizer, we train a CNN which simultaneously learns to localize the object while performing the image classification task.***
- ***The purpose of hiding patches is to show different parts of an object to the network while training it for the classification task. By hiding patches randomly, we can ensure that the most discriminative parts of an object are not always visible to the network, and thus force it to also focus on other relevant parts of the object.*** In this way, we can overcome the limitation of existing weakly-supervised methods that focus only on the most discriminative parts of an object.
- Same activation distributions
    - Since the full image is observed during testing, the data distribution will be different to that seen during training. We show that setting the hidden pixels’ value to be the data mean can allow the two distributions to match, and provide a theoretical justification.
    - Due to the discrepancy of hiding patches during training while not hiding patches during testing, the first convolutional layer activations during training versus testing will have different distributions. For a trained network to generalize well to new test data, the activation distributions should be roughly equal.
    - ***We resolve this issue by setting the RGB value of a hidden pixel to be equal to the mean RGB vector of the images over the entire dataset.***
    - ***This process is related to the scaling procedure in dropout [44], in which the outputs are scaled proportional to the drop rate during testing to match the expected output during training. In dropout, the outputs are dropped uniformly across the entire feature map, independently of spatial location.***

- For each image, we show the bounding box and CAM obtained by AlexNet-GAP (left) and our method (right). Our Hide-and-Seek approach localizes multiple relevant parts of an object whereas AlexNet-GAP mainly focuses only on the most discriminative parts.
- We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012). These models map a color 2D input image xi , via a series of lay- ers, to a probability vector ˆyi over the C different classes. Each layer consists of (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters; (ii) pass- ing the responses through a rectified linear function (relu(x) = max(x, 0)); (iii) [optionally] max pooling over local neighborhoods and (iv) [optionally] a lo- cal contrast operation that normalizes the responses across feature maps. For more details of these opera- tions, see (Krizhevsky et al., 2012) and (Jarrett et al., 2009). The top few layers of the network are conven- tional fully-connected networks and the final layer is a softmax classifier. Fig. 3 shows the model used in many of our experiments.
- We present a novel way to map these activities back to the input pixel space, showing what input pattern orig- inally caused a given activation in the feature maps. We perform this mapping with a Deconvolutional Net- work (deconvnet) (Zeiler et al., 2011). A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the oppo- site. In (Zeiler et al., 2011), deconvnets were proposed as a way of performing unsupervised learning.
- To examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels. To start, an input image is presented to the convnet and features computed throughout the layers. To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is reached. Unpooling: In the convnet, the max pooling opera- tion is non-invertible, however we can obtain an ap- proximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. Rectification: The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the fea- ture maps are always positive. To obtain valid fea- ture reconstructions at each layer (which also should be positive), we pass the reconstructed signal through a relu non-linearity. Filtering: The convnet uses learned filters to con- volve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters
- Projecting down from higher layers uses the switch settings generated by the max pooling in the convnet on the way up. As these switch settings are peculiar to a given input image, the reconstruction obtained from a single activation thus resembles a small piece of the original input image, with structures weighted according to their contribution toward to the feature activation. Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative.
- Given the CAM for an image, we generate a bound- ing box using the method proposed in [61]. Briefly, we first threshold the CAM to produce a binary fore- ground/background map, and then find connected compo- nents among the foreground pixels. Finally, we fit a tight bounding box to the largest connected component. We refer the reader to [61] for more details.
## Training
## Evaluation
### Datasets
- We use ILSVRC 2016 [36] to evaluate object localization accuracy. For training, we use 1.2 million images with their class labels (1000 categories). We compare our approach with the base- lines on the validation data. We use three evaluation met- rics to measure performance: 1) Top-1 localization accu- racy (Top-1 Loc): fraction of images for which the predicted class with the highest probability is the same as the ground- truth class and the predicted bounding box for that class has more than 50% IoU with the ground-truth box. 2) Local- ization accuracy with known ground-truth class (GT-known Loc): fraction of images for which the predicted bounding box for the ground-truth class has more than 50% IoU with the ground-truth box. As our approach is primarily designed to improve localization accuracy, we use this criterion to measure localization accuracy independent of classification performance. 3) We also use classification accuracy (Top- 1 Clas) to measure the impact of Hide-and-Seek on image classification performance.
- To obtain the binary fg/bg map, 20% and 30% of the max value of the CAM is chosen as the threshold for AlexNet-GAP and GoogLeNet GAP, respectively; the thresholds were chosen by observing a few qualitative results on training data.
- AlexNet-GAP [61] is our baseline in which the network has seen the full image during training without any hidden patches. Alex-HaS-N is our approach, in which patches of size N × N are hidden with 0.5 probability during training.
- We explored four different patch sizes N = {16, 32, 44, 56}, and each performs significantly better than AlexNet-GAP for both GT-known Loc and Top-1 Loc. Our GoogLeNet-HaS-N models also outperfors GoogLeNet-GAP for all patch sizes. These results clearly show that hiding patches during train- ing leads to better localization. Although our approach can lose some classification accuracy (Top-1 Clas) since it has never seen a complete image and thus may not have learned to relate certain parts, the huge boost in localization perfor- mance (which can be seen by comparing the GT-known Loc accuracies) makes up for any potential loss in classification.

## References
- [1] [Self-taught Object Localization with Deep Networks](https://arxiv.org/pdf/1409.3964.pdf)
- [3] [Weakly Supervised Object Detection with Posterior Regularization](https://homepages.inf.ed.ac.uk/hbilen/assets/pdf/Bilen14b.pdf)
- [8] [Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning](https://arxiv.org/pdf/1503.00949.pdf)
- [32] [Is object localization for free? - Weakly-supervised learning with convolutional neural networks]
- [43] [Weakly-supervised Discovery of Visual Pattern Configurations](https://arxiv.org/pdf/1406.6507.pdf)
- [47] [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)
- [49] [Regularization of Neural Networks using DropConnect](http://proceedings.mlr.press/v28/wan13.pdf)
- [56] [Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach](https://arxiv.org/pdf/1703.08448.pdf)
- [59] [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901.pdf)
- [61] [Learning Deep Features for Discriminative Localization]
