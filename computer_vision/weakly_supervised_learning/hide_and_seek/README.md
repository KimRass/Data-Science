# Paper Reading
- [Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization](https://arxiv.org/pdf/1704.04232.pdf)
- However, due to intra-category variations or relying only on a classification objective, these methods often fail to identify the entire extent of the object and instead localize only the most discriminative part.
- Figure 1. Main idea. (Top row) A network tends to focus on the most discriminative parts of an image (e.g., face of the dog) for classification. (Bottom row) By hiding images patches randomly, we can force the network to focus on other relevant object parts in order to correctly classify the image as ’dog’.
- we make changes to the input image. The key idea is to hide patches from an image during training so that the model needs to seek the relevant object parts from what remains. We thus name our approach ‘Hide-and-Seek’. Figure 1 (bottom row) demonstrates the intuition: if we randomly remove some patches from the image then there is a possibility that the dog’s face, which is the most discriminative, will not be visible to the model. In this case, the model must seek other relevant parts like the tail and legs in order to do well on the classification task. By randomly hiding different patches in each training epoch, the model sees different parts of the image and is forced to focus on multiple relevant parts of the object beyond just the most discriminative one. Impor- tantly, we only apply this random hiding of patches during training and not during testing. Since the full image is ob- served during testing, the data distribution will be different to that seen during training. We show that setting the hidden pixels’ value to be the data mean can allow the two distri- butions to match, and provide a theoretical justification.
- require expensive human annotations for training (e.g. bounding box for object localization). To alle- viate expensive annotation costs, weakly-supervised approaches learn using cheaper labels, for example, image-level labels for predicting an object’s loca- tion [55, 13, 9, 41, 3, 43, 50, 8, 32, 61].
- Most weakly-supervised object localization approaches mine discriminative features or patches in the data that fre- quently appear in one class and rarely in other classes [55, 13, 9, 41, 3, 7, 42, 43, 8]. However, these approaches tend to focus only on the most discriminative parts, and thus fail to cover the entire spatial extent of an object. In our approach, we hide image patches (randomly) during training, which forces our model to focus on multiple parts of an object and not just the most discriminative ones.
- Recent work modify CNN architectures designed for im- age classification so that the convolutional layers learn to localize objects while performing image classification [32, 61]. Other network architectures have been designed for weakly-supervised object detection [20, 4, 24]. Although these methods have significantly improved the state-of-the- art, they still essentially rely on a classification objective and thus can fail to capture the full extent of an object if the less discriminative parts do not help improve classifica- tion performance.
- for object localization, [59, 1] train a CNN for image classification and then localize the regions whose masking leads to a large drop in classification performance. Since these approaches mask out the image regions only during testing and not during training, the lo- calized regions are limited to the highly-discriminative ob- ject parts
- our work is closely related to the adversarial erasing method of [56], which iteratively trains a sequence of models for weakly- supervised semantic segmentation. Each model identifies the relevant object parts conditioned on the previous iter- ation model’s output. In contrast, we only train a single model once—and is thus less expensive—and do not rely on saliency detection to refine the localizations as done in [56].
- Dropout [44] and its variants [49, 47] are also related. There are two main differences: (1) these methods are de- signed to prevent overfitting while our work is designed to improve localization; and (2) in dropout, units in a layer are
- Left: For each training image, we divide it into a grid of S × S patches. Each patch is then randomly hidden with probability phide and given as input to a CNN to learn image classification. The hidden patches change randomly across different epochs.
- For weakly-supervised object localization, we are given a set of images Iset = {I1, I2, ....., IN } in which each im- age I is labeled only with its category label. Our goal is to learn an object localizer that can predict both the category label as well as the bounding box for the object-of-interest in a new test image Itest. In order to learn the object lo- calizer, we train a CNN which simultaneously learns to lo- calize the object while performing the image classification task.
- The purpose of hiding patches is to show different parts of an object to the net- work while training it for the classification task. By hiding patches randomly, we can ensure that the most discrimina- tive parts of an object are not always visible to the network, and thus force it to also focus on other relevant parts of the object. In this way, we can overcome the limitation of exist- ing weakly-supervised methods that focus only on the most discriminative parts of an object. Concretely, given a training image I of size W × H × 3, we first divide it into a grid with a fixed patch size of S×S× 3. This results in a total of (W × H)/(S × S) patches. We then hide each patch with phide probability. For example, in Fig. 2 left, the image is of size 224 × 224 × 3, and it is divided into 16 patches of size 56 × 56 × 3. Each patch is hidden with phide = 0.5 probability. We take the new image I 0 with the hidden patches, and feed it as a training input to a CNN for classification. Importantly, for each image, we randomly hide a differ- ent set of patches. Also, for the same image, we randomly hide a different set of patches in each training epoch. We hide patches only during training. During testing, the full image—without any patches hidden—is given as input to the network; Fig. 2 right. Since the network has learned to focus on multiple relevant parts during training, it is not necessary to hide any patches during testing. This is in direct contrast to [1], which hides patches during test- ing but not during training. For [1], since the network has already learned to focus on the most discimirinative parts during training, it is essentially too late, and hiding patches during testing has no significant effect on localization per- formance.
- Due to the discrepancy of hiding patches during training while not hiding patches during testing, the first convolutional layer activations dur- ing training versus testing will have different distributions. For a trained network to generalize well to new test data, the activation distributions should be roughly equal.
- We resolve this issue by setting the RGB value v of a hidden pixel to be equal to the mean RGB vector of the images over the entire dataset:
- This process is related to the scaling procedure in dropout [44], in which the outputs are scaled proportional to the drop rate during testing to match the expected out- put during training. In dropout, the outputs are dropped uniformly across the entire feature map, independently of spatial location.