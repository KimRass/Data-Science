{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#데이터-로드\" data-toc-modified-id=\"데이터-로드-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>데이터 로드</a></span></li><li><span><a href=\"#인코더,-디코더-정의\" data-toc-modified-id=\"인코더,-디코더-정의-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>인코더, 디코더 정의</a></span></li><li><span><a href=\"#임베딩-+-인코딩-+-디코딩을-결합하는-Transformer-Class-정의\" data-toc-modified-id=\"임베딩-+-인코딩-+-디코딩을-결합하는-Transformer-Class-정의-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>임베딩 + 인코딩 + 디코딩을 결합하는 Transformer Class 정의</a></span></li><li><span><a href=\"#옵티마이저-설정하기\" data-toc-modified-id=\"옵티마이저-설정하기-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>옵티마이저 설정하기</a></span></li><li><span><a href=\"#Loss-함수-설정하기\" data-toc-modified-id=\"Loss-함수-설정하기-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Loss 함수 설정하기</a></span></li><li><span><a href=\"#모델-훈련\" data-toc-modified-id=\"모델-훈련-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>모델 훈련</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Inference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRvYAI5bn4dF"
   },
   "source": [
    "# 데이터 로드\n",
    "데이터 로드는 tensorflow 2에서 제공하는 tf.data.dataset 형식으로 로드하도록 하겠습니다. dataset 형식으로 데이터를 불러오면, 향후 모델을 훈련시킬 때 @tf.function 데코레이터를 활용하여 상당히 빠르게 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T02:58:09.160061Z",
     "start_time": "2022-02-08T02:56:40.176626Z"
    },
    "id": "C42H5OZMmJL1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T03:03:40.872530Z",
     "start_time": "2022-02-08T03:03:40.868548Z"
    }
   },
   "outputs": [],
   "source": [
    "n_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "n_heads = 8\n",
    "SEQ_LEN = 40\n",
    "dk = d_model//n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T03:09:28.364516Z",
     "start_time": "2022-02-08T03:09:25.413531Z"
    },
    "id": "C42H5OZMmJL1"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_pywrap_tf2' from 'tensorflow.python.platform' (C:\\Users\\00006363\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-340fa77edd23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\tf2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_pywrap_tf2' from 'tensorflow.python.platform' (C:\\Users\\00006363\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, LayerNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D, RepeatVector\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD, Adagrad, Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.activations import linear, sigmoid, relu\n",
    "from tensorflow.keras.initializers import RandomNormal, glorot_uniform, he_uniform, Constant\n",
    "\n",
    "plt.style.use(\"dark_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T08:57:36.308038Z",
     "start_time": "2022-02-07T08:56:35.787894Z"
    },
    "id": "cv7qcd-GmMNY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\00006363\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093e231e6a46433c81ae70262d80449d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c743b73c32a4e968184992fdd75cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f66747556be46fab92bee550438e232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\/Users/00006363/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteNBZRR4/ted_hrlr_translate-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80048e77c36444d682721c9d1736da80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=51785.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\/Users/00006363/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteNBZRR4/ted_hrlr_translate-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb34121935443afab527fa52adf03d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\/Users/00006363/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteNBZRR4/ted_hrlr_translate-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecaac4e2a6f4b498061ced2a60a4dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to C:\\Users\\00006363\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load(\"ted_hrlr_translate/pt_to_en\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T12:00:52.901938Z",
     "start_time": "2022-02-04T12:00:52.876937Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "XLUAtajEmkBN",
    "outputId": "e7a18d36-a687-4ce4-a6ee-223a5fe4a434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Split('train'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>,\n",
       " Split('validation'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>,\n",
       " Split('test'): <PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터가 test, train, validation에 있음\n",
    "# 이 데이터들의 형식은 tf.data.dataset 오브젝트\n",
    "examples\n",
    "\n",
    "type(examples['train'])\n",
    "\n",
    "# train, validation, test 데이터로 나누기\n",
    "train_examples, val_examples, test_examples = examples[\"train\"], examples[\"validation\"], examples[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHE5JQ1hs9rs"
   },
   "outputs": [],
   "source": [
    "# train_example 에는 포르투갈어 pt, 영어 en이 있음\n",
    "# 여기서 단어를 추출해서, 단어들마다 인덱싱을 부여할 것임\n",
    "# 인덱싱을 부여하는 클래스가 tokenizer_en(영어 인덱싱 부여), tokenizer_pt(포르투갈어 인덱싱 부여)\n",
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "VOGyn1FCqLXF",
    "outputId": "403ce195-5c76-4e5c-cfcf-e3d28c540800"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7903, 2429, 439, 406, 7345, 7907, 1283, 7870, 9, 527, 6514, 7945, 7864]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어를 인코딩\n",
    "tokenizer_en.encode(\"Hello man, Let's run transformer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ZhMCauShqWkx",
    "outputId": "d65fe61b-c683-4347-d4ca-cd21a1c720c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141, 77, 33, 1566, 873, 4501, 217, 642, 4, 217, 101, 1073, 4824, 17, 5, 488, 200, 8004]\n"
     ]
    }
   ],
   "source": [
    "# 포르투갈어를 인코딩\n",
    "print(tokenizer_pt.encode(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFmp4-h1tA20"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1] # 포르투갈 어를 인코딩 할 때 시작 단어를 의미하는 숫자와, 끝 단어를 의미하는 숫자가 붙음\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1] # 영어도 마찬가지임\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34DVDfJ0sR6y"
   },
   "outputs": [],
   "source": [
    "lang1 = tf.constant(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\", dtype=tf.string)\n",
    "lang2 = tf.constant(\"Hello man, Let's run transformer!\", dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "Nlm39twMrCKb",
    "outputId": "258b3a10-80a3-4409-ea59-f3741fa2b144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([8214, 141, 77, 33, 1566, 873, 4501, 217, 642, 4, 217, 101, 1073, 4824, 17, 5, 488, 200, 8004, 8215], [8087, 7903, 2429, 439, 406, 7345, 7907, 1283, 7870, 9, 527, 6514, 7945, 7864, 8088])\n"
     ]
    }
   ],
   "source": [
    "print(encode(lang1, lang2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXgdTB2ftDov"
   },
   "outputs": [],
   "source": [
    "# tf.py_function을 활용하여 모든 문장에 숫자를 부여함\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64]) # 인풋 값으로 pt와 en이 한문장 한문장씩 들어가서 result_pt, result_en을 아웃풋으로 내게 됨\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9kC4H11tFH0"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 전체 데이터에서 x가 포르투갈어 한 문장, y가 영어 한 문장인데\n",
    "# 만약 문장이 인코딩 되었을 때, 인코딩 된 길이가 40을 초과하면 데이터에서 배제하고자 하는 필터 함수임\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V83U1EEQtGA-"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode) # tf_encode 함수를 활용해서 포르투갈어, 영어 각 문장에 시작 토큰과 끝 토큰을 부여함\n",
    "train_dataset = train_dataset.filter(filter_max_length) # 문장의 길이가 40이 넘는 문장은 배제하고자 함\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache() # cache를 활용해서 데이터를 로드할 때 빠른 처리를 기대해 봄\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE) \n",
    "# shuffle(60000) 인데, 이 말은 전체 데이터를 완전히 섞겠다는 뜻임\n",
    "# 전체 데이터의 수가 50000인데 50000보다 큰 숫자를 입력하면 완전하게 전체 데이터를 섞는 것이며\n",
    "# 전체 데이터 수보다 작은 수를 입력하면 전체 데이터에서 일부만 섞음\n",
    "# padded_batch는 무엇이냐면, 이번 데이터셋은 문장마다 길이가 모두 다르기에\n",
    "# 배치 사이즈(32) 만큼의 문장을 뽑을 때마다\n",
    "# 배치 사이즈에 해당하는 만큼의 문장의 길이는 일정하게 유지됨\n",
    "# 무슨 말이냐면 배치가 2개라면  이 중 하나의 문장의 길이는 37이 될 수 있고\n",
    "# 두 개의 배치(32) 중 하나의 배치는 문장의 길이를 37개로 모두 유지\n",
    "# 그 다음 배치에서 문장의 길이가 39 라면, 그 배치에서는 문장의 길이를 39로 유지\n",
    "                                                                            \n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE) # 데이터 로드와 처리의 시간을 overlap하여 속도 향상\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE) #가변길이의 배치를 돌릴때 꼭 쓰자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxi5ftvtRNr"
   },
   "source": [
    "- ![Imgur](https://i.imgur.com/Tl2zsFL.png)\n",
    "- ![Imgur](https://i.imgur.com/SNIEhlA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T12:51:08.852219Z",
     "start_time": "2022-02-07T12:51:08.805840Z"
    },
    "id": "Mg5gN7cEr8m2"
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "def positional_encoding_matrix(seq_len, d_model):\n",
    "    a, b = np.meshgrid(np.arange(d_model), np.arange(seq_len))\n",
    "    pe_mat = b/10000**(2*(a//2)/d_model)\n",
    "    pe_mat[:, 0::2] = np.sin(pe_mat[:, 0::2])\n",
    "    pe_mat[:, 1::2] = np.cos(pe_mat[:, 1::2])\n",
    "    pe_mat = pe_mat[None, :]\n",
    "    return pe_mat\n",
    "\n",
    "class TransformerEmbedding(Layer):\n",
    "    def __init__(self, input_vocab_size, max_pe_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_dim=input_vocab_size, output_dim=d_model)\n",
    "         # vocab_size는 tokenizer 내부 vocab.txt의 사이즈\n",
    "        self.pe_mat = positional_encoding_matrix(max_pe_dim, d_model)\n",
    "    \n",
    "    # `x`: (batch_size, seq_len)\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        z = self.embedding(x)\n",
    "        # 포지셔널 인코딩은 순서만을 의미하기 때문에 그 영향을 줄이도록 합니다.\n",
    "        z = (d_model**0.5)*z + self.pe_mat[:, :seq_len, :]\n",
    "        z = Dropout(rate=0.1)(z, training=training)\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        return z\n",
    "\n",
    "# d_model = 512\n",
    "# x = np.float32(np.random.uniform(size=(1, 40))) # 문장 길이 40\n",
    "# embedded = TransformerEmbedding(100 + 2, 10000)(x, False)\n",
    "# print(embedded) # 문장이 위치 임베딩 + 포지션 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T14:20:05.803479Z",
     "start_time": "2022-02-07T14:20:05.790477Z"
    },
    "id": "s-NM-LwEA9FI"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask=None):\n",
    "    attn_scores = tf.matmul(queries, keys, transpose_b=True)/dk**0.5\n",
    "    if mask is not None:\n",
    "        attn_scores = attn_scores + (mask*-1e9)\n",
    "    # (batch_size, seq_len_dec, seq_len_enc)\n",
    "    attn_weights = tf.nn.softmax(attn_scores, axis=-1)\n",
    "    # (batch_size, seq_len_dec, dk) (Same shape as queries)\n",
    "    context_vec = tf.matmul(attn_weights, values)\n",
    "    return context_vec, attn_weights\n",
    "\n",
    "class MultiheadAttention(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.dense = Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x = tf.reshape(x, shape=(batch_size, -1, n_heads, dk))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, values, keys, queries, mask):\n",
    "        queries = Dense(units=d_model)(queries)\n",
    "        keys = Dense(units=d_model)(keys)\n",
    "        values = Dense(units=d_model)(values)\n",
    "\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        # (batch_size, n_heads, seq_len_dec, dk)\n",
    "        queries = self.split_heads(queries)\n",
    "        # (batch_size, n_heads, seq_len_enc, dk)\n",
    "        keys = self.split_heads(keys)\n",
    "        # (batch_size, n_heads, seq_len_enc, dk)\n",
    "        values = self.split_heads(values)\n",
    "\n",
    "        # (batch_size, n_heads, seq_len_dec, dk)\n",
    "        context_vec, attn_weights = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        # (batch_size, seq_len_dec, n_heads, dk)\n",
    "        z = tf.transpose(context_vec, perm=[0, 2, 1, 3])\n",
    "        # (batch_size, seq_len_dec, d_model)\n",
    "        z = tf.reshape(z, shape=(batch_size, -1, d_model))\n",
    "        z = Dense(units=d_model)(z)\n",
    "        return z, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T14:26:55.414829Z",
     "start_time": "2022-02-07T14:26:55.369829Z"
    },
    "id": "sCnRYvoWGoj1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 128)\n"
     ]
    }
   ],
   "source": [
    "# q = tf.cast(np.random.uniform(size=(1, 2, d_model)), dtype=tf.float32)\n",
    "# k = tf.cast(np.random.uniform(size=(1, 7, d_model)), dtype=tf.float32)\n",
    "# v = tf.cast(np.random.uniform(size=(1, 7, d_model)), dtype=tf.float32)\n",
    "# encoder_output, _ = MultiheadAttention()(v, k, q, mask=None)\n",
    "# print(encoder_output.shape)\n",
    "# # q = split_heads(q)\n",
    "# # k = split_heads(k)\n",
    "# # v = split_heads(v)\n",
    "\n",
    "# # print(scaled_dot_product_attention(q, k, v, None)[0].shape)\n",
    "# # print(scaled_dot_product_attention(q, k, v, None)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbkyN8bq7EYT"
   },
   "source": [
    "# 인코더, 디코더 정의\n",
    "지금까지 임베딩 -> 멀티 헤드 어텐션 -> 포인트와이즈 피드 포워드 네트워크까지 살펴 보았습니다.  \n",
    "트랜스포머에서의 인코딩은 (멀티헤드어텐션 + 포인트와이즈 피드 포워드 네트워크)를 층층이 쌓은 것입니다. 따라서 방금 공부했던 멀티헤드 어텐션과 포인트와이즈 피드 포워드 네트워크를 겹겹이 쌓아서, 인코더층을 쌓는 것입니다.  \n",
    "\n",
    "인코더 레이어를 정의해서 멀티헤드 어텐션과 포인트와이즈 피드 포워드 네트워크를 합쳐 보도록 하겠습니다. \n",
    "\n",
    "단어 임베딩 벡터(단어 임베딩+위치 임베딩)가 -> 1. 멀티헤드어텐션  -> 2. Residual Network를 거쳐 원래 input과 멀티헤드어텐션의 합이 출력 -> 3. 포인트와이즈 피드포워드 네트워크 를 거치고, -> 4. Residual Network를 거쳐서 원래의 인풋과 포인트와이즈 피드포워드 네트워크의 합이 출력됨  \n",
    "![Imgur](https://i.imgur.com/w4n19Rs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F76815Y-BDZ8"
   },
   "source": [
    "디코더를 정의하도록 하겠습니다.  \n",
    "![Imgur](https://i.imgur.com/kGTfPr8.png)\n",
    "\n",
    "디코더는 인코더랑 유사하지만, 구조가 약간 다릅니다. \n",
    "이번 Seq2Seq는 포르투갈어를 영어로 바꾸는 문제입니다.  디코더에서는 \n",
    "두단계의 멀티 헤드 어텐션 구조를 거치는데, **첫번째 멀티 헤드 어텐션은**, **영어문장과 영어문장의 셀프 어텐션을 하여**, 영어 문장간의 관계를 배우게 됩니다.  \n",
    "두번째 멀티 헤드 어텐션은 **포르투갈어가 인코딩 된 것과**, **영어 문장간의 셀프** **어텐션된 결과를 다시 어텐션 해서 포르투갈 어와 영어의 관계를 학습하게 됩니다.**  \n",
    "\n",
    "포르투갈어가 암호화된 것과, 영어 문장 한단어 한단어를 보면서 다음 단어를 예측하게 되기 때문에, look_ahead_mask를 사용하게 됩니다.  \n",
    "만약 영어 문장이 (I love you) 로 이루어져 있다면, look_ahead_mask를 사용하면,  \n",
    "(I, 0, 0) -> Love 예측, (I love, 0) -> You 예측, (I love you) -> 단어의 끝인 [SEP] 예측을 합니다.   \n",
    "**즉 look_ahead_mask는 다음 단어를 예측할 때, 전에 있던 단어만으로 예측할수 있도록 앞에 있는 단어는 가리는 것입니다.**\n",
    "  \n",
    "  이러한 역할을 가능하게 하는 mask가 look_ahead_mask 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxwEGGzZ30eh"
   },
   "source": [
    "look_ahead_mask를 알아보겠습니다.**(매우 중요)**  \n",
    "참고로 패딩은 1로 하겠습니다. 왜냐하면 어텐션 부분에서 mask * (-1e9)를 하는데, 패딩이 1이어야 -1e9가 곱해져서 상당히 음수로 큰 수가 되는 것이고, 이게 소프트 맥스에 들어가면 0이 되기 때문입니다.(지수함수라 지수함수에 -음수는 0으로 수렴)  \n",
    "![Imgur](https://i.imgur.com/eLAlzji.png)  \n",
    "![Imgur](https://i.imgur.com/gAVenk0.png)  \n",
    "![Imgur](https://i.imgur.com/hsZ6dGs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "PGtYNoL23oIR",
    "outputId": "338b869c-9ab5-4e33-c6ee-a027e7d10a5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상삼각행렬 만들기\n",
    "tf.linalg.band_part(tf.ones((10, 10)), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "Gs3HDOex3rnK",
    "outputId": "1b3e609a-3891-4f7e-e7fd-072c66e4045b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(40, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_mask = 1 - tf.linalg.band_part(tf.ones((40, 40)), -1, 0)\n",
    "print(temp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "vhYWwxQo373G",
    "outputId": "0c50bc1b-c4d9-4fa3-e36c-76266a5535d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.  9.  5. 15. 16. 14. 15.  8. 13. 12.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(1, 40), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]], shape=(1, 1, 1, 40), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 1. 1. ... 1. 1. 1.]\n",
      "   [0. 0. 1. ... 1. 1. 1.]\n",
      "   [0. 0. 0. ... 1. 1. 1.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 1. 1. 1.]\n",
      "   [0. 0. 0. ... 1. 1. 1.]\n",
      "   [0. 0. 0. ... 1. 1. 1.]]]], shape=(1, 1, 40, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 예제 문장 (1, 40) 즉, 1문장, 40개의 단어를 가짐\n",
    "example_sentence = np.hstack([np.random.randint(20, size=10), np.zeros(30)])[np.newaxis, :]\n",
    "print(example_sentence) # 예제 문장\n",
    "example_sentence = tf.cast(tf.math.equal(example_sentence, 0), dtype=tf.float32)\n",
    "print(example_sentence) # 패딩 된 것(문장에서 0이 아닌 부분은 0으로, 0인 부분은 1로)\n",
    "example_sentence = example_sentence[:, tf.newaxis, tf.newaxis, :] # 차원 변경\n",
    "print(example_sentence)\n",
    "\n",
    "look_ahead_mask = tf.maximum(temp_mask, example_sentence) # 상삼각행렬과 example sentence를 비교해가며 최대값만 취해서 패딩을 1로 처리함\n",
    "# look ahead mask\n",
    "print(look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy3HuPbQ2_wJ"
   },
   "outputs": [],
   "source": [
    "# look_ahead_mask 알아보기\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask\n",
    "  \n",
    "def create_masks(tar):\n",
    "  temp_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  \n",
    "  \n",
    "  reverse_tar = tf.cast(tf.math.equal(tar, 0), dtype=tf.float32)\n",
    "  reverse_tar = reverse_tar[:,tf.newaxis,tf.newaxis,:]\n",
    "  look_ahead_mask = tf.maximum(reverse_tar, temp_mask)\n",
    "\n",
    "\n",
    "  return look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZYtaJVW6Q4K"
   },
   "source": [
    "또한 패딩 마스크를 두어서, 패딩인 부분은 1으로 처리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N47qF-CZ6m26"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "T6c7E0716uUj",
    "outputId": "973a0eed-7683-4f41-b52a-adcc307995d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  1.  4. 16.  3.  9. 18.  6. 17. 13.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.]]\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "    1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]], shape=(1, 1, 1, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_sentence = np.hstack([np.random.randint(20, size=10), np.zeros(30)])[np.newaxis, :]\n",
    "# 패딩 되기 전\n",
    "print(example_sentence)\n",
    "# 패딩 된 후\n",
    "print(create_padding_mask(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr-x36BB_ekk"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "  def __init__(self, d_model, n_heads, dff, rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mha1 = MultiheadAttention(d_model, n_heads)\n",
    "    self.mha2 = MultiheadAttention(d_model, n_heads)\n",
    "    \n",
    "    self.ffn = PositionwiseFFNN(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = Dropout(rate)\n",
    "    self.dropout2 = Dropout(rate)\n",
    "    self.dropout3 = Dropout(rate)\n",
    "\n",
    "\n",
    "  def call(self, x, enc_output, training, padding_mask, look_ahead_mask):\n",
    "    # x : 훈련 과정에서는 Seq2Seq에서 번역이 될 문장이 입력됨,\n",
    "    # x : 추론 과정에서는 과거의 단어가 입력됨\n",
    "    # enc_output : 인코더의 출력\n",
    "    # padding_mask : 멀티 헤드 어텐션에 필요한 정보만 남기고 나머지는 패딩 처리\n",
    "    # look_ahead_mask : 위에 설명\n",
    "    # enc_output_shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    #### 첫번째 멀티 헤드 어텐션 파트임 ####\n",
    "    #### 첫번째 멀티 헤드 어텐션은, 인코더와의 결합 없이 번역이 될 문장끼리만 어텐션을 함 ####\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "    out1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(out1 + x)\n",
    "    \n",
    "    #### 두번째 멀티 헤드 어텐션 파트임 ####\n",
    "    #### 두번째 멀티 헤드 어텐션 파트는 enc_output에서 인코더와(포르투갈어), out1(영어문장으로만 셀프어텐션을 한 것)\n",
    "    #### 이 다시 멀티 헤드 어텐션 과정을 거치게 됨\n",
    "    #### 다시 상기하자면, 이번 과제는 포르투갈 어를 영어로 번역하는 것임\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)\n",
    "    out2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(out2 + out1) # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "\n",
    "    ffn_output = self.ffn(out2)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "kk9I7NdJNMN0",
    "outputId": "5082fd8d-6bb8-40b9-95a7-d33826563859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.861443    0.27191493  2.616368   ... -0.06494223  0.37421167\n",
      "   -0.33387262]\n",
      "  [-1.4556838   0.02654802  2.9826684  ... -0.11393957  0.32178423\n",
      "   -0.3594063 ]\n",
      "  [-1.4244808  -0.52545476  2.9944925  ... -0.17711166  0.2963217\n",
      "   -0.42363918]\n",
      "  ...\n",
      "  [-1.869552    0.39455086  2.1463938  ...  0.01799125  0.5466902\n",
      "   -0.32442454]\n",
      "  [-1.417133    0.5532684   2.2169363  ... -0.02880318  0.5430099\n",
      "   -0.24101742]\n",
      "  [-1.0640508   0.19279505  2.6072536  ... -0.03333719  0.5305211\n",
      "   -0.1643059 ]]], shape=(1, 40, 512), dtype=float32)\n",
      "(1, 40, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, n_heads=8, dff=2048)\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(embedded, sample_encoding, False, None, None)\n",
    "print(sample_decoder_layer_output)\n",
    "print(sample_decoder_layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T02:54:42.843129Z",
     "start_time": "2022-02-08T02:54:42.600565Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f65b92e50c28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPositionwiseFFNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmiddle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Layer' is not defined"
     ]
    }
   ],
   "source": [
    "class PositionwiseFFNN(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.middle = Dense(dff, activation=\"relu\")\n",
    "        self.out = Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        # (batch_size, seq_len, dff)\n",
    "        z = self.middle(x)\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        z = self.out(z)\n",
    "        return z\n",
    "\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, d_model, n_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(d_model, n_heads)\n",
    "        self.ffn = PositionwiseFFNN(d_model, dff)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        # x : 위치 임베딩 + 단어 임베딩 된 인코딩의 인풋\n",
    "        z, _ = self.mha(x, x, x, mask) \n",
    "        # (batch_size, seq_len_enc, d_model)\n",
    "        z = self.dropout1(z, training=training)\n",
    "        # \"Add & Normalize\" Part\n",
    "        out1 = self.layernorm1(x + z)\n",
    "\n",
    "        # (batch_size, seq_len_enc, d_model)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # \"Add & Normalize\" Part\n",
    "        # (batch_size, seq_len_enc, d_model)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2 \n",
    "\n",
    "# 임베딩\n",
    "x = np.float32(np.random.uniform(size=(1, 40))) # 문장 길이 40\n",
    "Embedder = TransformerEmbedding(512, tokenizer_en.vocab_size+2, 10000)\n",
    "embedded = Embedder(x, False)\n",
    "print(embedded) # 문장이 위치 임베딩 + 포지션 임베딩\n",
    "#인코더\n",
    "sample_encoder = EncoderLayer(d_model=512, n_heads=8, dff=2048, rate=0.1)\n",
    "sample_encoding = sample_encoder(embedded, training=None, mask=None)\n",
    "print(sample_encoding)\n",
    "#최종출력\n",
    "print(\"Encoded 차원 :\", sample_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7WJs-lXOXtS"
   },
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, dff, rate) for _ in range(n_layers)] # 인코더를 쌓아서 층을 만듦\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        # 인풋은 타겟 임베딩 + 포지셔닝 임베딩\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "Encoder_layer = Encoder(6, 512, 8, 2048) #6층, 512차원의 단어 임베딩, 8개의 병렬 멀티 헤드 어텐션의 인코더\n",
    "# embedded : 위치 임베딩 + 단어 임베딩\n",
    "Encoded = Encoder_layer(embedded, False, None)\n",
    "print(Encoded, Encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQnvGLCzRmZ7"
   },
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "  def __init__(self, n_layers, d_model, n_heads, dff, rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, n_heads, dff, rate) for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "  def call(self, x, enc_output, training, padding_mask, look_ahead_mask):\n",
    "    attention_weights = {}\n",
    "\n",
    "    for i in range(self.n_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training, padding_mask, look_ahead_mask)\n",
    "\n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "ovF60mjeTnfu",
    "outputId": "57e33d4f-4e88-41ef-c8cf-49cada08ccbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.3431721  -0.3290148   1.5598023  ...  2.303232    1.371545\n",
      "    0.11695281]\n",
      "  [-0.33767134 -0.33899462  1.5710102  ...  2.3005219   1.3617636\n",
      "    0.11278047]\n",
      "  [-0.3494037  -0.3352064   1.5691497  ...  2.297593    1.3669722\n",
      "    0.11108976]\n",
      "  ...\n",
      "  [-0.35400254 -0.33930886  1.5467725  ...  2.3000638   1.3758032\n",
      "    0.1186213 ]\n",
      "  [-0.3487999  -0.33877504  1.543845   ...  2.3040543   1.3668836\n",
      "    0.12298044]\n",
      "  [-0.33305132 -0.34201398  1.5553851  ...  2.310325    1.3722295\n",
      "    0.11848406]]], shape=(1, 40, 512), dtype=float32) (1, 40, 512)\n",
      "6층 Decoder 출력 : (1, 40, 512)\n"
     ]
    }
   ],
   "source": [
    "# 위치 임베딩 + 포지셔널 임베딩\n",
    "embedded\n",
    "#인코더 아웃풋 \n",
    "Encoded\n",
    "\n",
    "# 디코더의 위치+포지셔널 임베딩(Teaching Force 과정이라 디코더 부분에 타겟(영어)도 넣어줌)\n",
    "target_embedder = TransformerEmbedding(d_model=512, input_vocab_size=tokenizer_en.vocab_size, max_pe_dim=512, dropout_rate=0.1)\n",
    "target_embedding = target_embedder(tf.cast(np.random.uniform(size=(1,40)), dtype=tf.float32), True)\n",
    "\n",
    "# 6층 디코더\n",
    "Decoder_layer = Decoder(n_layers=6, d_model=512, n_heads=8, dff=2048)\n",
    "\n",
    "Decoded, _ = Decoder_layer(target_embedding, Encoded, False, None, None) # Decoded에서 인코더와 디코더의 정보 결합\n",
    "print(Decoded, Decoded.shape)\n",
    "#최종출력\n",
    "print(\"6층 Decoder 출력 :\", sample_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03BBwaxOUPbJ"
   },
   "source": [
    "# 임베딩 + 인코딩 + 디코딩을 결합하는 Transformer Class 정의  \n",
    "최종적으로 지금까지 배웠던 임베딩, 인코딩, 디코딩을 결합하는 Transformer Class를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgoRro-QUE3G"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    \n",
    "    # n_layers : 인코딩, 디코딩을 몇 층으로 할 지\n",
    "    # d_model : 임베딩의 차원\n",
    "    # n_heads : 병렬로 어텐션을 수행할 멀티 헤드 어텐션의 개수\n",
    "    # dff : 포인트와이즈 피드포워드 네트워크에서 몇 차원의 연산이 이루어 질 지\n",
    "    # input_vocab_size : 본 문제는 포르투갈어를 영어로 번역하는 문제이며, 포르투갈어 토크나이저의 총 단어수를 뜻함(처음 도입부에서 만들었음)\n",
    "    # target_vocab_size : 영어 토크나이저의 총 단어수를 뜻함(위와 마찬가지)\n",
    "    # pe_input : 별로 중요한 것은 아니지만, 위치 임베딩 할 때 위치 임베딩의 길이의 상한을 뜻함(포르투갈어)\n",
    "    # pe_target : 위치 임베딩의 상한(영어)\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_embedder = TransformerEmbedding(d_model, input_vocab_size, pe_input, rate) # 포르투갈어 임베딩\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, dff, rate) # x층 인코더\n",
    "\n",
    "        self.target_embedder = TransformerEmbedding(d_model, target_vocab_size, pe_target, rate) # 영어 임베딩\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, dff, rate) # x층 디코더\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size) # 최종 출력 decoder (영어 단어의 수), 우리가 맞추려는 것은 영어 단어의 인덱스이므로..\n",
    "\n",
    "    def call(self, inp, tar, training):\n",
    "        # inp : 포르투갈어\n",
    "        # tar : 영어\n",
    "        enc_mask = create_padding_mask(inp) # 인코더 패딩\n",
    "        dec_mask = create_padding_mask(inp) # 디코더 패딩\n",
    "        look_ahead_mask = create_masks(tar) # 디코더에 들어갈 look_ahead_mask 정의\n",
    "\n",
    "        inp_embedding = self.input_embedder(inp) # 인코더 임베딩 정의\n",
    "        enc_output = self.encoder(inp_embedding, training, enc_mask) # 인코더 아웃풋(디코더와 결합되게 됨)\n",
    "\n",
    "        tar_embedding = self.target_embedder(tar) # 디코더 임베딩 정의\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar_embedding, enc_output, training, dec_mask, look_ahead_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output) # 최종 영어 단어를 예측하는 아웃풋 정의\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "4hwFuX2H-df4",
    "outputId": "36fb6b50-b015-41a3-daad-fe403000bfdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.3345034  -0.01396056  0.5463812  ... -0.05968201  0.3116074\n",
      "   -0.4603326 ]\n",
      "  [-0.33054727 -0.03038269  0.5541858  ... -0.04792291  0.3003717\n",
      "   -0.46931484]\n",
      "  [-0.3331985  -0.02435888  0.55642915 ... -0.04478735  0.3014708\n",
      "   -0.4855488 ]\n",
      "  ...\n",
      "  [-0.3428004  -0.04054517  0.5622994  ... -0.05635282  0.2901105\n",
      "   -0.49792284]\n",
      "  [-0.34195268 -0.04063866  0.5711767  ... -0.06332943  0.2811991\n",
      "   -0.49841833]\n",
      "  [-0.34157473 -0.0376301   0.5684389  ... -0.06148854  0.28651989\n",
      "   -0.50064284]]], shape=(1, 40, 8089), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머 테스트하기\n",
    "sample_transformer = Transformer(6, 512, 8, 2048, tokenizer_pt.vocab_size + 2, tokenizer_en.vocab_size + 2, 10000, 10000)\n",
    "inp = tf.cast(np.random.randint(100,size=40)[np.newaxis, :], dtype=tf.int32)\n",
    "tar = tf.cast(np.random.randint(100,size=40)[np.newaxis, :], dtype=tf.int32)\n",
    "transformer = sample_transformer(inp, tar, False)\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LuW2OLQ_LC3"
   },
   "source": [
    "#하이퍼파라미터 설정하기\n",
    "테스트를 위해서 층을 가볍게 쌓아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-04T16:29:51.289828Z",
     "start_time": "2022-02-04T16:29:51.258832Z"
    },
    "id": "3IBl1m3T_KdU"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-67e59806f03d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mSEQ_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minput_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_pt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_pt' is not defined"
     ]
    }
   ],
   "source": [
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJlCBwNE_Sex"
   },
   "source": [
    "# 옵티마이저 설정하기\n",
    "옵티마이저는 논문에 따라서 성능이 좋았다는 옵티마이저를 복사 붙여넣기 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wH6O3Gvs_R0O"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZemqrgPw_bPe"
   },
   "source": [
    "# Loss 함수 설정하기\n",
    "loss 함수 또한 중요한 부분인데, transformer에서는 패딩되는 부분을 Loss를 계산할 때 연산하지 않겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vl-ogg3D_eru"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') #from_logits=True로 하면 Dense 이후 softmax layer 값 출력\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # 예를 들어서 실제 자료(0은 패딩)가 [1,2,3,4,5,0,0,0,0,0] 이라면 [0,0,0,0,0,1,1,1,1,1]로 바꿔 줌\n",
    "                                                     # 이후 tf.math.logical_not을 활용해서 [True,True,True,True,True,False,False,False,False,False]으로 바꿔 줌\n",
    "  loss_ = loss_object(real, pred) # loss_는 패딩을 고려하지 않은 loss 값\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) # [True,True,True,True,True,False,False,False,False,False]를 [1,1,1,1,1,0,0,0,0,0] 으로 바꿔 줌\n",
    "  loss_ *= mask # loss에 mask를 곱해서, 패딩인 부분은 0처리 해줌\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zZ0hEBSD5ka"
   },
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2GySrPlEx2l"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(n_layers, d_model, n_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=10000, \n",
    "                          pe_target=10000,\n",
    "                          rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k07jNkQIw9N6"
   },
   "outputs": [],
   "source": [
    "# 인풋, 아웃풋의 텐셔 shape 정의\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "# tf.function을 사용하면 그래프를 미리 컴파일 하기 때문에 속도가 상당히 빠름\n",
    "# 같은 GPU여도 케라스에 비해서 체감상 7~8배 정도의 차이가 나는 것 같음\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = transformer(inp, tar_inp, True)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Iq2uReVFS1J"
   },
   "outputs": [],
   "source": [
    "# 저장할 체크포인트 지정\n",
    "checkpoint_path = \"./\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xHAeCJP-xZCu",
    "outputId": "36a8fe30-f744-4a6c-bf73-4a5e74e96bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.9987 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.9453 Accuracy 0.0034\n",
      "Epoch 1 Batch 100 Loss 8.8529 Accuracy 0.0134\n",
      "Epoch 1 Batch 150 Loss 8.7466 Accuracy 0.0168\n",
      "Epoch 1 Batch 200 Loss 8.6171 Accuracy 0.0184\n",
      "Epoch 1 Batch 250 Loss 8.4618 Accuracy 0.0195\n",
      "Epoch 1 Batch 300 Loss 8.2848 Accuracy 0.0236\n",
      "Epoch 1 Batch 350 Loss 8.1004 Accuracy 0.0273\n",
      "Epoch 1 Batch 400 Loss 7.9196 Accuracy 0.0301\n",
      "Epoch 1 Batch 450 Loss 7.7574 Accuracy 0.0326\n",
      "Epoch 1 Batch 500 Loss 7.6136 Accuracy 0.0355\n",
      "Epoch 1 Batch 550 Loss 7.4792 Accuracy 0.0388\n",
      "Epoch 1 Batch 600 Loss 7.3546 Accuracy 0.0427\n",
      "Epoch 1 Batch 650 Loss 7.2348 Accuracy 0.0465\n",
      "Epoch 1 Batch 700 Loss 7.1196 Accuracy 0.0503\n",
      "Epoch 1 Loss 7.1152 Accuracy 0.0504\n",
      "Time taken for 1 epoch: 79.08091878890991 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.7278 Accuracy 0.0938\n",
      "Epoch 2 Batch 50 Loss 5.4732 Accuracy 0.1046\n",
      "Epoch 2 Batch 100 Loss 5.4041 Accuracy 0.1061\n",
      "Epoch 2 Batch 150 Loss 5.3532 Accuracy 0.1085\n",
      "Epoch 2 Batch 200 Loss 5.3088 Accuracy 0.1099\n",
      "Epoch 2 Batch 250 Loss 5.2682 Accuracy 0.1124\n",
      "Epoch 2 Batch 300 Loss 5.2265 Accuracy 0.1145\n",
      "Epoch 2 Batch 350 Loss 5.1899 Accuracy 0.1161\n",
      "Epoch 2 Batch 400 Loss 5.1555 Accuracy 0.1177\n",
      "Epoch 2 Batch 450 Loss 5.1229 Accuracy 0.1194\n",
      "Epoch 2 Batch 500 Loss 5.0927 Accuracy 0.1209\n",
      "Epoch 2 Batch 550 Loss 5.0652 Accuracy 0.1222\n",
      "Epoch 2 Batch 600 Loss 5.0397 Accuracy 0.1234\n",
      "Epoch 2 Batch 650 Loss 5.0119 Accuracy 0.1247\n",
      "Epoch 2 Batch 700 Loss 4.9883 Accuracy 0.1259\n",
      "Epoch 2 Loss 4.9868 Accuracy 0.1259\n",
      "Time taken for 1 epoch: 44.30380201339722 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.6232 Accuracy 0.1753\n",
      "Epoch 3 Batch 50 Loss 4.5817 Accuracy 0.1450\n",
      "Epoch 3 Batch 100 Loss 4.5632 Accuracy 0.1459\n",
      "Epoch 3 Batch 150 Loss 4.5604 Accuracy 0.1459\n",
      "Epoch 3 Batch 200 Loss 4.5491 Accuracy 0.1465\n",
      "Epoch 3 Batch 250 Loss 4.5413 Accuracy 0.1470\n",
      "Epoch 3 Batch 300 Loss 4.5299 Accuracy 0.1475\n",
      "Epoch 3 Batch 350 Loss 4.5184 Accuracy 0.1482\n",
      "Epoch 3 Batch 400 Loss 4.5085 Accuracy 0.1485\n",
      "Epoch 3 Batch 450 Loss 4.5005 Accuracy 0.1490\n",
      "Epoch 3 Batch 500 Loss 4.4873 Accuracy 0.1495\n",
      "Epoch 3 Batch 550 Loss 4.4756 Accuracy 0.1498\n",
      "Epoch 3 Batch 600 Loss 4.4677 Accuracy 0.1501\n",
      "Epoch 3 Batch 650 Loss 4.4568 Accuracy 0.1505\n",
      "Epoch 3 Batch 700 Loss 4.4470 Accuracy 0.1510\n",
      "Epoch 3 Loss 4.4468 Accuracy 0.1510\n",
      "Time taken for 1 epoch: 44.45479679107666 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 4.2324 Accuracy 0.1636\n",
      "Epoch 4 Batch 50 Loss 4.1740 Accuracy 0.1633\n",
      "Epoch 4 Batch 100 Loss 4.1639 Accuracy 0.1627\n",
      "Epoch 4 Batch 150 Loss 4.1519 Accuracy 0.1631\n",
      "Epoch 4 Batch 200 Loss 4.1324 Accuracy 0.1641\n",
      "Epoch 4 Batch 250 Loss 4.1250 Accuracy 0.1651\n",
      "Epoch 4 Batch 300 Loss 4.1188 Accuracy 0.1660\n",
      "Epoch 4 Batch 350 Loss 4.1145 Accuracy 0.1666\n",
      "Epoch 4 Batch 400 Loss 4.1005 Accuracy 0.1673\n",
      "Epoch 4 Batch 450 Loss 4.0890 Accuracy 0.1679\n",
      "Epoch 4 Batch 500 Loss 4.0780 Accuracy 0.1688\n",
      "Epoch 4 Batch 550 Loss 4.0656 Accuracy 0.1697\n",
      "Epoch 4 Batch 600 Loss 4.0532 Accuracy 0.1705\n",
      "Epoch 4 Batch 650 Loss 4.0402 Accuracy 0.1714\n",
      "Epoch 4 Batch 700 Loss 4.0248 Accuracy 0.1724\n",
      "Epoch 4 Loss 4.0242 Accuracy 0.1725\n",
      "Time taken for 1 epoch: 44.30045199394226 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.4033 Accuracy 0.2066\n",
      "Epoch 5 Batch 50 Loss 3.6744 Accuracy 0.1919\n",
      "Epoch 5 Batch 100 Loss 3.6507 Accuracy 0.1927\n",
      "Epoch 5 Batch 150 Loss 3.6447 Accuracy 0.1930\n",
      "Epoch 5 Batch 200 Loss 3.6315 Accuracy 0.1935\n",
      "Epoch 5 Batch 250 Loss 3.6214 Accuracy 0.1949\n",
      "Epoch 5 Batch 300 Loss 3.6175 Accuracy 0.1955\n",
      "Epoch 5 Batch 350 Loss 3.6045 Accuracy 0.1963\n",
      "Epoch 5 Batch 400 Loss 3.5996 Accuracy 0.1968\n",
      "Epoch 5 Batch 450 Loss 3.5912 Accuracy 0.1975\n",
      "Epoch 5 Batch 500 Loss 3.5839 Accuracy 0.1977\n",
      "Epoch 5 Batch 550 Loss 3.5748 Accuracy 0.1985\n",
      "Epoch 5 Batch 600 Loss 3.5650 Accuracy 0.1990\n",
      "Epoch 5 Batch 650 Loss 3.5589 Accuracy 0.1998\n",
      "Epoch 5 Batch 700 Loss 3.5506 Accuracy 0.2004\n",
      "Saving checkpoint for epoch 5 at ./ckpt-1\n",
      "Epoch 5 Loss 3.5497 Accuracy 0.2004\n",
      "Time taken for 1 epoch: 44.612069606781006 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.0750 Accuracy 0.2216\n",
      "Epoch 6 Batch 50 Loss 3.1687 Accuracy 0.2194\n",
      "Epoch 6 Batch 100 Loss 3.1730 Accuracy 0.2190\n",
      "Epoch 6 Batch 150 Loss 3.1820 Accuracy 0.2170\n",
      "Epoch 6 Batch 200 Loss 3.1866 Accuracy 0.2174\n",
      "Epoch 6 Batch 250 Loss 3.1848 Accuracy 0.2187\n",
      "Epoch 6 Batch 300 Loss 3.1812 Accuracy 0.2195\n",
      "Epoch 6 Batch 350 Loss 3.1806 Accuracy 0.2197\n",
      "Epoch 6 Batch 400 Loss 3.1767 Accuracy 0.2200\n",
      "Epoch 6 Batch 450 Loss 3.1744 Accuracy 0.2203\n",
      "Epoch 6 Batch 500 Loss 3.1683 Accuracy 0.2205\n",
      "Epoch 6 Batch 550 Loss 3.1619 Accuracy 0.2211\n",
      "Epoch 6 Batch 600 Loss 3.1561 Accuracy 0.2215\n",
      "Epoch 6 Batch 650 Loss 3.1484 Accuracy 0.2222\n",
      "Epoch 6 Batch 700 Loss 3.1421 Accuracy 0.2229\n",
      "Epoch 6 Loss 3.1415 Accuracy 0.2229\n",
      "Time taken for 1 epoch: 44.359644174575806 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.6573 Accuracy 0.2393\n",
      "Epoch 7 Batch 50 Loss 2.7642 Accuracy 0.2404\n",
      "Epoch 7 Batch 100 Loss 2.7670 Accuracy 0.2407\n",
      "Epoch 7 Batch 150 Loss 2.7668 Accuracy 0.2406\n",
      "Epoch 7 Batch 200 Loss 2.7715 Accuracy 0.2397\n",
      "Epoch 7 Batch 250 Loss 2.7681 Accuracy 0.2396\n",
      "Epoch 7 Batch 300 Loss 2.7650 Accuracy 0.2407\n",
      "Epoch 7 Batch 350 Loss 2.7639 Accuracy 0.2420\n",
      "Epoch 7 Batch 400 Loss 2.7613 Accuracy 0.2425\n",
      "Epoch 7 Batch 450 Loss 2.7583 Accuracy 0.2431\n",
      "Epoch 7 Batch 500 Loss 2.7541 Accuracy 0.2433\n",
      "Epoch 7 Batch 550 Loss 2.7502 Accuracy 0.2437\n",
      "Epoch 7 Batch 600 Loss 2.7465 Accuracy 0.2442\n",
      "Epoch 7 Batch 650 Loss 2.7427 Accuracy 0.2445\n",
      "Epoch 7 Batch 700 Loss 2.7391 Accuracy 0.2447\n",
      "Epoch 7 Loss 2.7391 Accuracy 0.2447\n",
      "Time taken for 1 epoch: 44.38622713088989 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.3553 Accuracy 0.2520\n",
      "Epoch 8 Batch 50 Loss 2.3678 Accuracy 0.2643\n",
      "Epoch 8 Batch 100 Loss 2.3673 Accuracy 0.2646\n",
      "Epoch 8 Batch 150 Loss 2.3807 Accuracy 0.2658\n",
      "Epoch 8 Batch 200 Loss 2.3894 Accuracy 0.2654\n",
      "Epoch 8 Batch 250 Loss 2.3896 Accuracy 0.2648\n",
      "Epoch 8 Batch 300 Loss 2.3910 Accuracy 0.2652\n",
      "Epoch 8 Batch 350 Loss 2.3955 Accuracy 0.2654\n",
      "Epoch 8 Batch 400 Loss 2.4008 Accuracy 0.2655\n",
      "Epoch 8 Batch 450 Loss 2.4007 Accuracy 0.2655\n",
      "Epoch 8 Batch 500 Loss 2.4016 Accuracy 0.2658\n",
      "Epoch 8 Batch 550 Loss 2.4038 Accuracy 0.2654\n",
      "Epoch 8 Batch 600 Loss 2.4054 Accuracy 0.2653\n",
      "Epoch 8 Batch 650 Loss 2.4071 Accuracy 0.2652\n",
      "Epoch 8 Batch 700 Loss 2.4059 Accuracy 0.2651\n",
      "Epoch 8 Loss 2.4056 Accuracy 0.2651\n",
      "Time taken for 1 epoch: 44.65272235870361 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.1085 Accuracy 0.3173\n",
      "Epoch 9 Batch 50 Loss 2.0877 Accuracy 0.2828\n",
      "Epoch 9 Batch 100 Loss 2.0966 Accuracy 0.2807\n",
      "Epoch 9 Batch 150 Loss 2.1006 Accuracy 0.2810\n",
      "Epoch 9 Batch 200 Loss 2.1152 Accuracy 0.2807\n",
      "Epoch 9 Batch 250 Loss 2.1257 Accuracy 0.2805\n",
      "Epoch 9 Batch 300 Loss 2.1357 Accuracy 0.2800\n",
      "Epoch 9 Batch 350 Loss 2.1423 Accuracy 0.2795\n",
      "Epoch 9 Batch 400 Loss 2.1448 Accuracy 0.2790\n",
      "Epoch 9 Batch 450 Loss 2.1508 Accuracy 0.2790\n",
      "Epoch 9 Batch 500 Loss 2.1557 Accuracy 0.2791\n",
      "Epoch 9 Batch 550 Loss 2.1559 Accuracy 0.2793\n",
      "Epoch 9 Batch 600 Loss 2.1574 Accuracy 0.2793\n",
      "Epoch 9 Batch 650 Loss 2.1622 Accuracy 0.2790\n",
      "Epoch 9 Batch 700 Loss 2.1654 Accuracy 0.2789\n",
      "Epoch 9 Loss 2.1653 Accuracy 0.2789\n",
      "Time taken for 1 epoch: 44.233455419540405 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.8756 Accuracy 0.2834\n",
      "Epoch 10 Batch 50 Loss 1.8793 Accuracy 0.2984\n",
      "Epoch 10 Batch 100 Loss 1.8912 Accuracy 0.2946\n",
      "Epoch 10 Batch 150 Loss 1.9065 Accuracy 0.2933\n",
      "Epoch 10 Batch 200 Loss 1.9212 Accuracy 0.2928\n",
      "Epoch 10 Batch 250 Loss 1.9342 Accuracy 0.2919\n",
      "Epoch 10 Batch 300 Loss 1.9413 Accuracy 0.2912\n",
      "Epoch 10 Batch 350 Loss 1.9483 Accuracy 0.2908\n",
      "Epoch 10 Batch 400 Loss 1.9546 Accuracy 0.2905\n",
      "Epoch 10 Batch 450 Loss 1.9630 Accuracy 0.2906\n",
      "Epoch 10 Batch 500 Loss 1.9661 Accuracy 0.2902\n",
      "Epoch 10 Batch 550 Loss 1.9707 Accuracy 0.2896\n",
      "Epoch 10 Batch 600 Loss 1.9729 Accuracy 0.2896\n",
      "Epoch 10 Batch 650 Loss 1.9780 Accuracy 0.2896\n",
      "Epoch 10 Batch 700 Loss 1.9842 Accuracy 0.2895\n",
      "Saving checkpoint for epoch 10 at ./ckpt-2\n",
      "Epoch 10 Loss 1.9850 Accuracy 0.2896\n",
      "Time taken for 1 epoch: 44.6807496547699 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.6583 Accuracy 0.2910\n",
      "Epoch 11 Batch 50 Loss 1.7554 Accuracy 0.3036\n",
      "Epoch 11 Batch 100 Loss 1.7630 Accuracy 0.3031\n",
      "Epoch 11 Batch 150 Loss 1.7672 Accuracy 0.3026\n",
      "Epoch 11 Batch 200 Loss 1.7777 Accuracy 0.3023\n",
      "Epoch 11 Batch 250 Loss 1.7821 Accuracy 0.3023\n",
      "Epoch 11 Batch 300 Loss 1.7942 Accuracy 0.3016\n",
      "Epoch 11 Batch 350 Loss 1.7979 Accuracy 0.3009\n",
      "Epoch 11 Batch 400 Loss 1.8074 Accuracy 0.3001\n",
      "Epoch 11 Batch 450 Loss 1.8148 Accuracy 0.2999\n",
      "Epoch 11 Batch 500 Loss 1.8219 Accuracy 0.2997\n",
      "Epoch 11 Batch 550 Loss 1.8271 Accuracy 0.2987\n",
      "Epoch 11 Batch 600 Loss 1.8334 Accuracy 0.2987\n",
      "Epoch 11 Batch 650 Loss 1.8386 Accuracy 0.2983\n",
      "Epoch 11 Batch 700 Loss 1.8418 Accuracy 0.2979\n",
      "Epoch 11 Loss 1.8421 Accuracy 0.2979\n",
      "Time taken for 1 epoch: 44.32446074485779 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.7401 Accuracy 0.3160\n",
      "Epoch 12 Batch 50 Loss 1.6039 Accuracy 0.3171\n",
      "Epoch 12 Batch 100 Loss 1.6199 Accuracy 0.3134\n",
      "Epoch 12 Batch 150 Loss 1.6305 Accuracy 0.3125\n",
      "Epoch 12 Batch 200 Loss 1.6466 Accuracy 0.3117\n",
      "Epoch 12 Batch 250 Loss 1.6590 Accuracy 0.3104\n",
      "Epoch 12 Batch 300 Loss 1.6704 Accuracy 0.3096\n",
      "Epoch 12 Batch 350 Loss 1.6805 Accuracy 0.3088\n",
      "Epoch 12 Batch 400 Loss 1.6886 Accuracy 0.3078\n",
      "Epoch 12 Batch 450 Loss 1.6941 Accuracy 0.3072\n",
      "Epoch 12 Batch 500 Loss 1.6983 Accuracy 0.3066\n",
      "Epoch 12 Batch 550 Loss 1.7061 Accuracy 0.3061\n",
      "Epoch 12 Batch 600 Loss 1.7130 Accuracy 0.3058\n",
      "Epoch 12 Batch 650 Loss 1.7191 Accuracy 0.3056\n",
      "Epoch 12 Batch 700 Loss 1.7226 Accuracy 0.3050\n",
      "Epoch 12 Loss 1.7227 Accuracy 0.3050\n",
      "Time taken for 1 epoch: 44.53757357597351 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.4844 Accuracy 0.3349\n",
      "Epoch 13 Batch 50 Loss 1.5040 Accuracy 0.3219\n",
      "Epoch 13 Batch 100 Loss 1.5169 Accuracy 0.3208\n",
      "Epoch 13 Batch 150 Loss 1.5308 Accuracy 0.3187\n",
      "Epoch 13 Batch 200 Loss 1.5406 Accuracy 0.3179\n",
      "Epoch 13 Batch 250 Loss 1.5506 Accuracy 0.3169\n",
      "Epoch 13 Batch 300 Loss 1.5573 Accuracy 0.3166\n",
      "Epoch 13 Batch 350 Loss 1.5668 Accuracy 0.3159\n",
      "Epoch 13 Batch 400 Loss 1.5764 Accuracy 0.3147\n",
      "Epoch 13 Batch 450 Loss 1.5884 Accuracy 0.3145\n",
      "Epoch 13 Batch 500 Loss 1.5955 Accuracy 0.3139\n",
      "Epoch 13 Batch 550 Loss 1.6034 Accuracy 0.3135\n",
      "Epoch 13 Batch 600 Loss 1.6101 Accuracy 0.3130\n",
      "Epoch 13 Batch 650 Loss 1.6177 Accuracy 0.3126\n",
      "Epoch 13 Batch 700 Loss 1.6227 Accuracy 0.3121\n",
      "Epoch 13 Loss 1.6229 Accuracy 0.3121\n",
      "Time taken for 1 epoch: 45.53956842422485 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.3664 Accuracy 0.3628\n",
      "Epoch 14 Batch 50 Loss 1.4419 Accuracy 0.3263\n",
      "Epoch 14 Batch 100 Loss 1.4345 Accuracy 0.3253\n",
      "Epoch 14 Batch 150 Loss 1.4512 Accuracy 0.3228\n",
      "Epoch 14 Batch 200 Loss 1.4614 Accuracy 0.3228\n",
      "Epoch 14 Batch 250 Loss 1.4696 Accuracy 0.3227\n",
      "Epoch 14 Batch 300 Loss 1.4805 Accuracy 0.3218\n",
      "Epoch 14 Batch 350 Loss 1.4866 Accuracy 0.3217\n",
      "Epoch 14 Batch 400 Loss 1.4968 Accuracy 0.3206\n",
      "Epoch 14 Batch 450 Loss 1.5069 Accuracy 0.3201\n",
      "Epoch 14 Batch 500 Loss 1.5126 Accuracy 0.3195\n",
      "Epoch 14 Batch 550 Loss 1.5201 Accuracy 0.3186\n",
      "Epoch 14 Batch 600 Loss 1.5266 Accuracy 0.3184\n",
      "Epoch 14 Batch 650 Loss 1.5326 Accuracy 0.3179\n",
      "Epoch 14 Batch 700 Loss 1.5357 Accuracy 0.3175\n",
      "Epoch 14 Loss 1.5363 Accuracy 0.3175\n",
      "Time taken for 1 epoch: 45.02189350128174 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.3980 Accuracy 0.3264\n",
      "Epoch 15 Batch 50 Loss 1.3330 Accuracy 0.3386\n",
      "Epoch 15 Batch 100 Loss 1.3475 Accuracy 0.3332\n",
      "Epoch 15 Batch 150 Loss 1.3621 Accuracy 0.3320\n",
      "Epoch 15 Batch 200 Loss 1.3744 Accuracy 0.3307\n",
      "Epoch 15 Batch 250 Loss 1.3803 Accuracy 0.3291\n",
      "Epoch 15 Batch 300 Loss 1.3911 Accuracy 0.3281\n",
      "Epoch 15 Batch 350 Loss 1.4005 Accuracy 0.3274\n",
      "Epoch 15 Batch 400 Loss 1.4124 Accuracy 0.3264\n",
      "Epoch 15 Batch 450 Loss 1.4211 Accuracy 0.3254\n",
      "Epoch 15 Batch 500 Loss 1.4297 Accuracy 0.3247\n",
      "Epoch 15 Batch 550 Loss 1.4350 Accuracy 0.3240\n",
      "Epoch 15 Batch 600 Loss 1.4447 Accuracy 0.3236\n",
      "Epoch 15 Batch 650 Loss 1.4527 Accuracy 0.3234\n",
      "Epoch 15 Batch 700 Loss 1.4604 Accuracy 0.3227\n",
      "Saving checkpoint for epoch 15 at ./ckpt-3\n",
      "Epoch 15 Loss 1.4602 Accuracy 0.3228\n",
      "Time taken for 1 epoch: 45.342923402786255 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.0781 Accuracy 0.3229\n",
      "Epoch 16 Batch 50 Loss 1.2595 Accuracy 0.3371\n",
      "Epoch 16 Batch 100 Loss 1.2736 Accuracy 0.3376\n",
      "Epoch 16 Batch 150 Loss 1.2938 Accuracy 0.3356\n",
      "Epoch 16 Batch 200 Loss 1.3076 Accuracy 0.3345\n",
      "Epoch 16 Batch 250 Loss 1.3196 Accuracy 0.3338\n",
      "Epoch 16 Batch 300 Loss 1.3290 Accuracy 0.3327\n",
      "Epoch 16 Batch 350 Loss 1.3344 Accuracy 0.3319\n",
      "Epoch 16 Batch 400 Loss 1.3451 Accuracy 0.3305\n",
      "Epoch 16 Batch 450 Loss 1.3558 Accuracy 0.3295\n",
      "Epoch 16 Batch 500 Loss 1.3615 Accuracy 0.3293\n",
      "Epoch 16 Batch 550 Loss 1.3697 Accuracy 0.3291\n",
      "Epoch 16 Batch 600 Loss 1.3796 Accuracy 0.3286\n",
      "Epoch 16 Batch 650 Loss 1.3871 Accuracy 0.3282\n",
      "Epoch 16 Batch 700 Loss 1.3930 Accuracy 0.3280\n",
      "Epoch 16 Loss 1.3932 Accuracy 0.3281\n",
      "Time taken for 1 epoch: 44.752070903778076 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2447 Accuracy 0.3290\n",
      "Epoch 17 Batch 50 Loss 1.1962 Accuracy 0.3427\n",
      "Epoch 17 Batch 100 Loss 1.2119 Accuracy 0.3427\n",
      "Epoch 17 Batch 150 Loss 1.2303 Accuracy 0.3413\n",
      "Epoch 17 Batch 200 Loss 1.2430 Accuracy 0.3406\n",
      "Epoch 17 Batch 250 Loss 1.2576 Accuracy 0.3384\n",
      "Epoch 17 Batch 300 Loss 1.2662 Accuracy 0.3378\n",
      "Epoch 17 Batch 350 Loss 1.2733 Accuracy 0.3369\n",
      "Epoch 17 Batch 400 Loss 1.2822 Accuracy 0.3366\n",
      "Epoch 17 Batch 450 Loss 1.2910 Accuracy 0.3361\n",
      "Epoch 17 Batch 500 Loss 1.3018 Accuracy 0.3349\n",
      "Epoch 17 Batch 550 Loss 1.3092 Accuracy 0.3344\n",
      "Epoch 17 Batch 600 Loss 1.3173 Accuracy 0.3343\n",
      "Epoch 17 Batch 650 Loss 1.3250 Accuracy 0.3332\n",
      "Epoch 17 Batch 700 Loss 1.3319 Accuracy 0.3330\n",
      "Epoch 17 Loss 1.3320 Accuracy 0.3329\n",
      "Time taken for 1 epoch: 44.743831634521484 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.9768 Accuracy 0.3506\n",
      "Epoch 18 Batch 50 Loss 1.1597 Accuracy 0.3449\n",
      "Epoch 18 Batch 100 Loss 1.1689 Accuracy 0.3432\n",
      "Epoch 18 Batch 150 Loss 1.1853 Accuracy 0.3427\n",
      "Epoch 18 Batch 200 Loss 1.1996 Accuracy 0.3417\n",
      "Epoch 18 Batch 250 Loss 1.2107 Accuracy 0.3412\n",
      "Epoch 18 Batch 300 Loss 1.2179 Accuracy 0.3406\n",
      "Epoch 18 Batch 350 Loss 1.2236 Accuracy 0.3400\n",
      "Epoch 18 Batch 400 Loss 1.2332 Accuracy 0.3391\n",
      "Epoch 18 Batch 450 Loss 1.2427 Accuracy 0.3387\n",
      "Epoch 18 Batch 500 Loss 1.2513 Accuracy 0.3382\n",
      "Epoch 18 Batch 550 Loss 1.2566 Accuracy 0.3379\n",
      "Epoch 18 Batch 600 Loss 1.2634 Accuracy 0.3374\n",
      "Epoch 18 Batch 650 Loss 1.2710 Accuracy 0.3370\n",
      "Epoch 18 Batch 700 Loss 1.2777 Accuracy 0.3366\n",
      "Epoch 18 Loss 1.2774 Accuracy 0.3367\n",
      "Time taken for 1 epoch: 44.47447395324707 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.0398 Accuracy 0.3698\n",
      "Epoch 19 Batch 50 Loss 1.1146 Accuracy 0.3477\n",
      "Epoch 19 Batch 100 Loss 1.1219 Accuracy 0.3496\n",
      "Epoch 19 Batch 150 Loss 1.1362 Accuracy 0.3464\n",
      "Epoch 19 Batch 200 Loss 1.1474 Accuracy 0.3457\n",
      "Epoch 19 Batch 250 Loss 1.1572 Accuracy 0.3451\n",
      "Epoch 19 Batch 300 Loss 1.1672 Accuracy 0.3435\n",
      "Epoch 19 Batch 350 Loss 1.1768 Accuracy 0.3431\n",
      "Epoch 19 Batch 400 Loss 1.1841 Accuracy 0.3427\n",
      "Epoch 19 Batch 450 Loss 1.1887 Accuracy 0.3424\n",
      "Epoch 19 Batch 500 Loss 1.1962 Accuracy 0.3417\n",
      "Epoch 19 Batch 550 Loss 1.2048 Accuracy 0.3415\n",
      "Epoch 19 Batch 600 Loss 1.2142 Accuracy 0.3406\n",
      "Epoch 19 Batch 650 Loss 1.2215 Accuracy 0.3404\n",
      "Epoch 19 Batch 700 Loss 1.2298 Accuracy 0.3393\n",
      "Epoch 19 Loss 1.2299 Accuracy 0.3394\n",
      "Time taken for 1 epoch: 44.57541823387146 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0221 Accuracy 0.3498\n",
      "Epoch 20 Batch 50 Loss 1.0534 Accuracy 0.3617\n",
      "Epoch 20 Batch 100 Loss 1.0729 Accuracy 0.3588\n",
      "Epoch 20 Batch 150 Loss 1.0862 Accuracy 0.3541\n",
      "Epoch 20 Batch 200 Loss 1.0935 Accuracy 0.3535\n",
      "Epoch 20 Batch 250 Loss 1.1069 Accuracy 0.3523\n",
      "Epoch 20 Batch 300 Loss 1.1191 Accuracy 0.3519\n",
      "Epoch 20 Batch 350 Loss 1.1304 Accuracy 0.3501\n",
      "Epoch 20 Batch 400 Loss 1.1404 Accuracy 0.3491\n",
      "Epoch 20 Batch 450 Loss 1.1501 Accuracy 0.3484\n",
      "Epoch 20 Batch 500 Loss 1.1566 Accuracy 0.3472\n",
      "Epoch 20 Batch 550 Loss 1.1619 Accuracy 0.3463\n",
      "Epoch 20 Batch 600 Loss 1.1689 Accuracy 0.3454\n",
      "Epoch 20 Batch 650 Loss 1.1756 Accuracy 0.3445\n",
      "Epoch 20 Batch 700 Loss 1.1817 Accuracy 0.3442\n",
      "Saving checkpoint for epoch 20 at ./ckpt-4\n",
      "Epoch 20 Loss 1.1820 Accuracy 0.3441\n",
      "Time taken for 1 epoch: 44.694963693618774 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 20 에포크 훈련\n",
    "for epoch in range(20):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # input : 포루투갈어, tar : 영어\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45sJJKmdL8u-"
   },
   "source": [
    "# Inference\n",
    "![Imgur](https://i.imgur.com/cUjg18g.png)  \n",
    "\n",
    "\n",
    "평가는 훈련과는 다르게 진행됩니다.  \n",
    "번역할 포르투갈어는 인코더 레이어를 거쳐 인코딩이 되고,  \n",
    "디코더에는 영어 문장을 넣지 않고, 영어 문장의 시작 토큰만 디코더의 인풋으로 들어가게 됩니다.  \n",
    "그러면 인코딩 된 것과 + 시작 토큰을 활용해서 다음 단어를 예측하고,  \n",
    "인코딩 된 것 + 시작 토큰 + 전에 예측된 단어를 활용해서 다음 단어를 예측하는 방식입니다. \n",
    "  \n",
    "그림에서는 bos가 시작 토큰입니다.\n",
    "![Imgur](https://i.imgur.com/F6QseH6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdaL7LLfGzaQ"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  # inp_sentence : 문자 (string)\n",
    "  start_token = [tokenizer_pt.vocab_size] # 포르투갈어의 시작 토큰\n",
    "  end_token = [tokenizer_pt.vocab_size + 1] # 포르투갈어의 끝 토큰\n",
    "  \n",
    "  # 시작 토큰 + 포르투갈 어 + 끝 토큰\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # 디코더의 인풋은 영어 문장의 시작 토큰만 들어감\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "  \n",
    "  for i in range(MAX_LENGTH):\n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions = transformer(encoder_input, output, False)\n",
    "    \n",
    "    # 예측 결과에서 마지막 부분만 추출\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약에 예측된 영어 단어가 영어의 끝 토큰에 해당한다면 예측을 끝냄\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0)\n",
    "    \n",
    "    # 예측된 단어를 전 단어와 결합하여 다음 예측에 써먹음\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECK0XFzCHBxv"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result= evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe6y5oDePZOI"
   },
   "source": [
    "실제로 번역해보기  \n",
    "제법 포르투갈 어를 영어 문법에 맞게 번역하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "czjSk64HHCq5",
    "outputId": "88cdd289-4a37-4537-c101-5d9d5a81c7d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é um problema que temos que resolver.\n",
      "Predicted translation: this is a problem we have to solve the united states .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "WkQuQyvjHcbJ",
    "outputId": "fae2b14c-acf1-46ca-ed26-ede08c782d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors heard about this idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "76ZizBuyPbd8",
    "outputId": "898b2073-8a09-4b22-9218-25cc434df106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Predicted translation: so i 'm going to quickly share with you some stories of a few magic things that happened .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "29vxXk81PlbE",
    "outputId": "af846b13-dbba-4d83-e267-f9fb68c8417b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este é o primeiro livro que eu fiz.\n",
      "Predicted translation: this is the first book that i did n't .\n",
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"este é o primeiro livro que eu fiz.\")\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn44IonrZhnV"
   },
   "source": [
    "출처  \n",
    "http://jalammar.github.io/illustrated-gpt2/  \n",
    "https://d2l.ai/chapter_recurrent-modern/seq2seq.html  \n",
    "https://www.tensorflow.org/tutorials/text/transformer\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Transformer(NMT).ipynb의 사본",
   "provenance": [
    {
     "file_id": "1Vm49MfibBnNvrtYB2nbyYxcspzYvqNr3",
     "timestamp": 1609030500900
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
