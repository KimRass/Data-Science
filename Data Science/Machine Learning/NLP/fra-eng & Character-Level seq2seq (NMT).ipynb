{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimRass/Programming/blob/master/Data%20Science/Machine%20Learning/NLP/fra-eng%20%26%20Character-Level%20seq2seq%20(NMT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "NlLx6Y8jgkDU"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inference</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive/Libraries\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.activations import linear, sigmoid, relu\n",
        "from tensorflow.keras.initializers import RandomNormal, glorot_uniform, he_uniform, Constant\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import zipfile\n",
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "D5PcG7sU2Wwq",
        "outputId": "2ca9d9fd-addf-4dbe-8ee9-acf60386dcec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-27T02:11:34.754058Z",
          "start_time": "2022-01-27T02:11:33.945189Z"
        },
        "id": "w650GKn9YTN8"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/NLP\")\n",
        "raw_data = pd.read_table(\"./Datasets/fra-eng/fra.txt\", usecols=[0, 1], names=[\"tar\", \"src\"])\n",
        "\n",
        "raw_data = raw_data.sample(len(raw_data)//3, random_state=777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-27T02:11:37.624378Z",
          "start_time": "2022-01-27T02:11:34.757018Z"
        },
        "id": "Pr959qoTgkDe"
      },
      "outputs": [],
      "source": [
        "# `lower`: Whether to convert the texts to lowercase.\n",
        "# `char_level`: If `True`, every character will be treated as a token.\n",
        "tokenizer_src = Tokenizer(char_level=True)\n",
        "tokenizer_src.fit_on_texts(raw_data[\"src\"])\n",
        "char2idx_src = tokenizer_src.word_index\n",
        "vocab_size_src = len(char2idx_src)\n",
        "enc_input = tokenizer_src.texts_to_sequences(raw_data[\"src\"])\n",
        "\n",
        "tokenizer_tar = Tokenizer(char_level=True)\n",
        "tokenizer_tar.fit_on_texts(\"시\" + raw_data[\"tar\"] + \"종\")\n",
        "char2idx_tar = tokenizer_tar.word_index\n",
        "vocab_size_tar = len(char2idx_tar)\n",
        "dec_input = tokenizer_tar.texts_to_sequences(\"시\" + raw_data[\"tar\"])\n",
        "dec_gt = tokenizer_tar.texts_to_sequences(raw_data[\"tar\"] + \"종\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-27T02:11:37.680230Z",
          "start_time": "2022-01-27T02:11:37.626346Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spk0qLNOa0jE",
        "outputId": "9a1a84f7-00eb-411e-b3e8-1f4e9ad11626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "길이가 가장 긴 문장의 길이는 305이고 길이가 86 이하인 문장이 전체의 99%를 차지합니다.\n",
            "길이가 가장 긴 문장의 길이는 240이고 길이가 72 이하인 문장이 전체의 99%를 차지합니다.\n"
          ]
        }
      ],
      "source": [
        "ratio = 0.99\n",
        "\n",
        "lens_enc = sorted([len(doc) for doc in enc_input])\n",
        "max_len_enc = int(np.quantile(lens_enc, ratio))\n",
        "print(f\"길이가 가장 긴 문장의 길이는 {np.max(lens_enc)}이고 길이가 {max_len_enc} 이하인 문장이 전체의 {ratio:.0%}를 차지합니다.\")\n",
        "\n",
        "lens_dec = sorted([len(doc) for doc in dec_input])\n",
        "max_len_dec = int(np.quantile(lens_dec, ratio))\n",
        "print(f\"길이가 가장 긴 문장의 길이는 {np.max(lens_dec)}이고 길이가 {max_len_dec} 이하인 문장이 전체의 {ratio:.0%}를 차지합니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-27T02:12:14.516699Z",
          "start_time": "2022-01-27T02:11:37.682202Z"
        },
        "id": "mPi-I91BYTOB"
      },
      "outputs": [],
      "source": [
        "enc_input = pad_sequences(enc_input, padding=\"post\", maxlen=max_len_enc)\n",
        "dec_input = pad_sequences(dec_input, padding=\"post\", maxlen=max_len_dec)\n",
        "dec_gt = pad_sequences(dec_gt, padding=\"post\", maxlen=max_len_dec)\n",
        "\n",
        "enc_input = to_categorical(enc_input)\n",
        "dec_input = to_categorical(dec_input)\n",
        "dec_gt = to_categorical(dec_gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDF2hXV9gkDg"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-27T02:12:29.523570Z",
          "start_time": "2022-01-27T02:12:15.440230Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "RMrzRNokgkDh",
        "outputId": "5b6f33e1-5b65-4738-9e5c-f16e46d25e3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5a68c494aa10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TRUE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minputs_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size_src\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Input_enc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './fra_eng_char-level_seq2seq_hist.npy'"
          ]
        }
      ],
      "source": [
        "name = \"./fra_eng_char-level_seq2seq\"\n",
        "model_path = f\"{name}.h5\"\n",
        "hist_path = f\"{name}_hist.npy\"\n",
        "if os.path.exists(model_path):\n",
        "    model = load_model(model_path)\n",
        "    hist = np.load(hist_path, allow_pickle=\"TRUE\").item()\n",
        "else:\n",
        "    inputs_enc = Input(shape=(max_len_enc, vocab_size_src + 1), name=\"Input_enc\")\n",
        "    inputs_dec = Input(shape=(max_len_dec, vocab_size_tar + 1), name=\"Input_dec\")\n",
        "    \n",
        "    _, h_state, c_state = LSTM(units=128, return_state=True, name=\"LSTM_enc\")(inputs_enc)\n",
        "    z, _, _ = LSTM(units=128, return_sequences=True, return_state=True, name=\"LSTM_dec\")(inputs_dec, initial_state=[h_state, c_state])\n",
        "    outputs = Dense(units=vocab_size_tar + 1, activation=\"softmax\", name=\"Dense_dec\")(z)\n",
        "\n",
        "    model = Model(inputs=[inputs_enc, inputs_dec], outputs=outputs)\n",
        "    \n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "    model.summary()\n",
        "\n",
        "    es = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", verbose=1, patience=1)\n",
        "    mc = ModelCheckpoint(filepath=model_path, monitor=\"val_acc\", mode=\"auto\", verbose=1, save_best_only=True)\n",
        "    hist = model.fit(x=[enc_input, dec_input], y=dec_gt, batch_size=2048, epochs=32, validation_split=0.3, callbacks=[es, mc])\n",
        "    \n",
        "    np.save(hist_path, hitst.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQqqHIhRYTOD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 학습이 맞게 됐는지 확인\n",
        "i = 110\n",
        "pred = model.predict([tf.expand_dims(enc_input[i], axis=0), tf.expand_dims(dec_input[i], axis=0)])\n",
        "\n",
        "sent = \"\"\n",
        "for idx in tf.argmax(dec_gt[i], axis=1).numpy():\n",
        "    if idx != 0:\n",
        "        sent += idx2char_tar[idx]\n",
        "print(sent)\n",
        "\n",
        "sent = \"\"\n",
        "for idx in tf.argmax(pred[0], axis=1).numpy():\n",
        "    if idx != 0:\n",
        "        sent += idx2char_tar[idx]\n",
        "print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDLJkvCYTOE"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD5vl9rcYTOF"
      },
      "source": [
        "- 우선 인코더를 정의합니다. enc_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용하는 것입니다. 이제 디코더를 설계해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "54KFid58fJoC",
        "outputId": "3a4c805b-effe-4f00-a98b-94e1a91603f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f97529d7438>,\n",
              " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f9752d01f60>,\n",
              " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f97529d7908>,\n",
              " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f9753533278>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f9753533cc0>]"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NGBHG4gTYTOF",
        "outputId": "e729e08f-5fd7-4d20-8568-4c2de1753ae1",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "inputs_enc = model.layers[0].output\n",
        "_, h_state, c_state = model.layers[2].output\n",
        "\n",
        "enc_model = tf.keras.Model(inputs=inputs_enc, outputs=[h_state, c_state])\n",
        "\n",
        "inputs_dec = model.layers[1].output\n",
        "h_state_bef = Input(shape=(256,))\n",
        "c_state_bef = Input(shape=(256,))\n",
        "# 문장의 다음 단어를 예측하기 위해서 initial_state를 이전 시점의 상태로 사용합니다.\n",
        "lstm_dec_layer = model.layers[3]\n",
        "lstm_dec, h_state_aft, c_state_aft = lstm_dec_layer(inputs_dec, initial_state=[h_state_bef, c_state_bef])\n",
        "dense_dec_layer = model.layers[4]\n",
        "dense_dec = dense_dec_layer(lstm_dec)\n",
        "\n",
        "dec_model = tf.keras.Model(inputs=[inputs_dec]+[h_state_bef, c_state_bef], outputs=[dense_dec]+[h_state_aft, c_state_aft])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MqbVBmExYTOG",
        "outputId": "9be7b630-3b2b-429e-a87a-7af26720a027"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def decode_seq(input_seq):\n",
        "# seq = enc_input[i:i+1]\n",
        "    enc_states = enc_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 OHE를 생성합니다.\n",
        "    seq = np.zeros((1, 1, len(char2idx_tar)+1))\n",
        "    seq[0, 0, char2idx_tar[\"<SOS>\"]] = 1\n",
        "\n",
        "    stop_cond = False\n",
        "    decoded_sent = \"\"\n",
        "    # stop_cond이 True가 될 때까지 반복합니다.\n",
        "    while not stop_cond:\n",
        "        # 이점 시점의 states를 현재 시점의 states로 사용합니다.\n",
        "        output_tokens, h_state, c_state = dec_model.predict([seq] + enc_states)\n",
        "        argmax = np.argmax(output_tokens[0, -1, :])\n",
        "    #     argmax = np.argmax(output_tokens[0, 0])\n",
        "        char = idx2char_tar[argmax]\n",
        "        decoded_sent += char\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장합니다.\n",
        "        seq = np.zeros((1, 1, len(char2idx_tar)+1))\n",
        "        seq[0, 0, argmax] = 1\n",
        "        enc_states = [h_state, c_state]\n",
        "        \n",
        "        # \"<EOS>\"에 도달하거나 최대 길이를 넘으면 stop_cond=True를 저장합니다.\n",
        "        if char == \"<EOS>\" or len(decoded_sent) == max_len_dec:\n",
        "            stop_cond = True\n",
        "    return decoded_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "ZbbUbHkjzpuk",
        "outputId": "d73b7b61-0b3a-46af-a650-beb4337ad829"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {white-space: pre-wrap;}\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "입력 문장 : why do you need change?\n",
            "정답 문장 : ourquoi as-tu besoin de changement \n",
            "번역 문장 : jz3lw(++1yj1…w+l1+jl…zp3+lœ1ljz… <EOS\n",
            "BLEU-1 : 0\n",
            "BLEU-2 : 0\n",
            "BLEU-3 : 0\n",
            "BLEU-4 : 0\n",
            "-----------------------------------\n",
            "입력 문장 : why did you change your mind?\n",
            "정답 문장 : ourquoi as-tu changé d'avis \n",
            "번역 문장 : j(l+z0êp+lœ;(s1êjl3;(lw(+l8j8ljê(s(p//8 <EOS\n",
            "BLEU-1 : 0\n",
            "BLEU-2 : 0\n",
            "BLEU-3 : 0\n",
            "BLEU-4 : 0\n",
            "-----------------------------------\n",
            "입력 문장 : we don't want to lose you.\n",
            "정답 문장 : ous ne voulons pas vous perdre\n",
            "번역 문장 : sz0+l(s1)ljz0jlœ1l+0pj1lùl/(l…(p+z3 <EOS\n",
            "BLEU-1 : 0\n",
            "BLEU-2 : 0\n",
            "BLEU-3 : 0\n",
            "BLEU-4 : 0\n",
            "-----------------------------------\n",
            "입력 문장 : see that this never happens again.\n",
            "정답 문장 : aites en sorte que ça ne se produise plus\n",
            "번역 문장 : èj1+ysz0+lwêèj1lùlc(pê1lh(l&<EOS\n",
            "BLEU-1 : 0\n",
            "BLEU-2 : 0\n",
            "BLEU-3 : 0\n",
            "BLEU-4 : 0\n",
            "-----------------------------------\n",
            "입력 문장 : you must not lose sight of your main object.\n",
            "정답 문장 : l ne faut pas que tu perdes de vue ton objectif principal\n",
            "번역 문장 : /1+lwêp4l+z3jl+0êl/1lwzp3jlœ1lw(ê/1êlœ1l/(l3z0êêpj0ê1l1jlœ1l/;(êç13j <EOS\n",
            "BLEU-1 : 0\n",
            "BLEU-2 : 0\n",
            "BLEU-3 : 0\n",
            "BLEU-4 : 0\n"
          ]
        }
      ],
      "source": [
        "actual, pred = list(), list()\n",
        "for seq_index in range(231, 236):\n",
        "    input_seq = enc_input[seq_index:seq_index+1]\n",
        "    decoded_sent = decode_seq(input_seq)\n",
        "    \n",
        "    actual.append([data[\"tar\"][seq_index][1:len(data[\"tar\"][seq_index])-1].split()])\n",
        "    pred.append(decoded_sent[:len(decoded_sent)-1].split())\n",
        "                  \n",
        "    print(35 * \"-\")\n",
        "    print(f\"입력 문장 : {data['src'][seq_index]}\")\n",
        "    print(f\"정답 문장 : {data['tar'][seq_index][1:len(data['tar'][seq_index])-1]}\")\n",
        "    print(f\"번역 문장 : {decoded_sent[:len(decoded_sent)-1]}\")\n",
        "    sf = SmoothingFunction()\n",
        "    print(f\"BLEU-1 : {corpus_bleu(actual, pred, weights=(1, 0, 0, 0),\\\n",
        "                                  smoothing_function=sf.method1)}\")\n",
        "    print(f\"BLEU-2 : {corpus_bleu(actual, pred, weights=(1/2, 1/2, 0, 0),\\\n",
        "                                  smoothing_function=sf.method1)}\")\n",
        "    print(f\"BLEU-3 : {corpus_bleu(actual, pred, weights=(1/3, 1/3, 1/3, 0),\\\n",
        "                                  smoothing_function=sf.method1)}\")\n",
        "    print(f\"BLEU-4 : {corpus_bleu(actual, pred, weights=(1/4, 1/4, 1/4, 1/4),\\\n",
        "                                  smoothing_function=sf.method1)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "fra-eng & Character-Level seq2seq (NMT).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "191.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}