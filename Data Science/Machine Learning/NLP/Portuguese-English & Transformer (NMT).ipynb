{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimRass/Programming/blob/master/Data%20Science/Machine%20Learning/NLP/Portuguese-English%20%26%20Transformer%20(NMT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktJMirEOT8iR",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Inference</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T10:58:40.004885Z",
          "start_time": "2022-02-11T10:58:37.957788Z"
        },
        "id": "C42H5OZMmJL1"
      },
      "outputs": [],
      "source": [
        "# Data Science/Machine Learning/NLP/Portuguese-English & Transformer (NMT).ipynb\n",
        "# !pip install -q tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:01:44.555173Z",
          "start_time": "2022-02-15T14:01:40.193688Z"
        },
        "id": "JS458ZL3T8iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474ec430-3543-4b39-b490-411562b371a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, LayerNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D, RepeatVector\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "# MeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
        "from tensorflow.keras import losses\n",
        "# MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.activations import linear, sigmoid, relu\n",
        "from tensorflow.keras.initializers import RandomNormal, glorot_uniform, he_uniform, Constant\n",
        "\n",
        "plt.style.use(\"dark_background\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:01:44.705640Z",
          "start_time": "2022-02-15T14:01:44.555173Z"
        },
        "id": "cv7qcd-GmMNY"
      },
      "outputs": [],
      "source": [
        "# `with_info`: If `True`, `tfds.load()` will return the tuple `(tf.data.Dataset, tfds.core.DatasetInfo)`, the latter containing the info associated with the builder.\n",
        "# `as_supervised`: If `True`, the returned `tf.data.Dataset` will have a 2-tuple structure `(input, label)` according to `builder.info.supervised_keys`. If `False`, the returned `tf.data.Dataset` will have a dictionary with all the features.\n",
        "dataset, metadata = tfds.load(\"ted_hrlr_translate/pt_to_en\", with_info=True, as_supervised=True)\n",
        "dataset_tr = dataset[\"train\"]\n",
        "dataset_val = dataset[\"validation\"]\n",
        "dataset_te = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:03:57.802577Z",
          "start_time": "2022-02-15T14:01:44.705640Z"
        },
        "id": "RHE5JQ1hs9rs"
      },
      "outputs": [],
      "source": [
        "tokenizer_src = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in dataset_tr), target_vocab_size=2**13)\n",
        "tokenizer_tar = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in dataset_tr), target_vocab_size=2**13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:03:57.954055Z",
          "start_time": "2022-02-15T14:03:57.802577Z"
        },
        "id": "yIMAMEIw3rcK"
      },
      "outputs": [],
      "source": [
        "max_len = 40\n",
        "buffer_size = tf.data.experimental.cardinality(dataset_tr).numpy()\n",
        "batch_size = 64\n",
        "\n",
        "def encode(lang1, lang2):\n",
        "    lang1 = [tokenizer_src.vocab_size] + tokenizer_src.encode(lang1.numpy()) + [tokenizer_src.vocab_size + 1]\n",
        "    lang2 = [tokenizer_tar.vocab_size] + tokenizer_tar.encode(lang2.numpy()) + [tokenizer_tar.vocab_size + 1]\n",
        "    return lang1, lang2\n",
        "\n",
        "def tf_encode(pt, en):\n",
        "    # `func`: A Python function that accepts `inp` as arguments, and returns a value (or list of values) whose type is described by `Tout`.\n",
        "    # `inpt`: Input arguments for func. A list whose elements are Tensors or a single Tensor.\n",
        "    result_pt, result_en = tf.py_function(func=encode, inp=[pt, en], Tout=[tf.int64, tf.int64])\n",
        "    result_pt.set_shape([None])\n",
        "    result_en.set_shape([None])\n",
        "    return result_pt, result_en\n",
        "\n",
        "def filter_max_len(x, y):\n",
        "    return tf.logical_and(tf.size(x) <= max_len, tf.size(y) <= max_len)\n",
        "\n",
        "# This transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. `map_func` can be used to change both the values and the structure of a dataset's elements.\n",
        "dataset_tr = dataset_tr.map(tf_encode)\n",
        "# `predicate`: A function mapping a dataset element to a boolean.\n",
        "# Returns the dataset containing the elements of this dataset for which `predicate` is `True`.\n",
        "dataset_tr = dataset_tr.filter(filter_max_len)\n",
        "# The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n",
        "# `filename`: When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to `cache()` will have no effect until the cache file is removed or the `filename` is changed. If a `filename` is not provided, the dataset will be cached in memory.\n",
        "dataset_tr = dataset_tr.cache()\n",
        "# For perfect shuffling, a `buffer_size` greater than or equal to the full size of the dataset is required.\n",
        "# If not, only the first `buffer_size` elements will be selected randomly.\n",
        "# `reshuffle_each_iteration` controls whether the shuffle order should be different for each epoch.\n",
        "dataset_tr = dataset_tr.shuffle(buffer_size)\n",
        "# Pad to the smallest per-`batch size` that fits all elements.\n",
        "# Unlike `batch()`, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in `padded_shapes`. The `padded_shapes` argument determines the resulting shape for each dimension of each component in an output element.\n",
        "# `padded_shapes`:\n",
        "    # If `None`: The dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.\n",
        "    # If not `None`: The dimension is a constant, the component will be padded out to that length in that dimension.\n",
        "# `padding_values`\n",
        "# `drop_remainder`\n",
        "dataset_tr = dataset_tr.padded_batch(batch_size)\n",
        "# Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n",
        "# `buffer_size`: The maximum number of elements that will be buffered when prefetching. If the value `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned.\n",
        "dataset_tr = dataset_tr.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "dataset_val = dataset_val.map(tf_encode)\n",
        "dataset_val = dataset_val.filter(filter_max_len)\n",
        "dataset_val = dataset_val.padded_batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CZ8bnfF3rcL"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:03:57.992307Z",
          "start_time": "2022-02-15T14:03:57.954055Z"
        },
        "id": "OHvG04aa3rcN"
      },
      "outputs": [],
      "source": [
        "n_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "n_heads = 8\n",
        "dk = d_model//n_heads\n",
        "\n",
        "def padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # `(batch_size, 1, 1, seq_len)`\n",
        "    return seq[:, None, None, :]\n",
        "\n",
        "def look_ahead_mask(tar):\n",
        "    size = tf.shape(tar)[1]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    p_mask = padding_mask(tar)\n",
        "    return tf.maximum(p_mask, mask)\n",
        "\n",
        "def positional_encoding_matrix():\n",
        "    a, b = np.meshgrid(np.arange(d_model), np.arange(max_len))\n",
        "    pe_mat = b/10000**(2*(a//2)/d_model)\n",
        "    pe_mat[:, 0::2] = np.sin(pe_mat[:, 0::2])\n",
        "    pe_mat[:, 1::2] = np.cos(pe_mat[:, 1::2])\n",
        "    pe_mat = pe_mat[None, ...]\n",
        "    pe_mat = tf.cast(pe_mat, dtype=tf.float32)\n",
        "    return pe_mat\n",
        "\n",
        "# `x`: `(batch_size, seq_len)`\n",
        "class Embedder(Layer):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "        self.dropout = Dropout(rate=0.1)\n",
        "        self.pe_mat = positional_encoding_matrix()\n",
        "        \n",
        "    def __call__(self, x, training):\n",
        "        # `(batch_size, seq_len, d_model)`\n",
        "        z = self.embedding(x)\n",
        "\n",
        "        # Positional Encoding\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        z = (d_model**0.5)*z + self.pe_mat[:, :seq_len, :]\n",
        "        z = self.dropout(z, training=training)\n",
        "        # `(batch_size, seq_len, d_model)`\n",
        "        return z\n",
        "\n",
        "class MultiheadAttention(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dense_q = Dense(units=d_model)\n",
        "        self.dense_k = Dense(units=d_model)\n",
        "        self.dense_v = Dense(units=d_model)\n",
        "        self.dense = Dense(units=d_model)\n",
        "        \n",
        "    def __call__(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        def split_heads(x):\n",
        "            x = tf.reshape(x, shape=(batch_size, -1, n_heads, dk))\n",
        "            return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # `(batch_size, seq_len_dec, d_model)`\n",
        "        queries = self.dense_q(queries)\n",
        "        # `(batch_size, seq_len_enc, d_model)`\n",
        "        keys = self.dense_k(keys)\n",
        "        # `(batch_size, seq_len_enc, d_model)`\n",
        "        values = self.dense_v(values)\n",
        "\n",
        "        # `(batch_size, n_heads, seq_len_dec, dk)`\n",
        "        queries = split_heads(queries)\n",
        "        # `(batch_size, n_heads, seq_len_enc, dk)`\n",
        "        keys = split_heads(keys)\n",
        "        # `(batch_size, n_heads, seq_len_enc, dk)`\n",
        "        values = split_heads(values)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        # `(batch_size, n_heads, seq_len_dec, seq_len_enc)`\n",
        "        attn_scores = tf.matmul(queries, keys, transpose_b=True)/dk**0.5\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores + (mask*-1e9)\n",
        "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)\n",
        "        # `(batch_size, n_heads, seq_len_dec, dk)`\n",
        "        context_vec = tf.matmul(attn_weights, values)\n",
        "\n",
        "        # `(batch_size, seq_len_dec, n_heads, dk)`\n",
        "        z = tf.transpose(context_vec, perm=[0, 2, 1, 3])\n",
        "        # `(batch_size, seq_len_dec, d_model)`\n",
        "        z = tf.reshape(z, shape=(batch_size, -1, d_model))\n",
        "        z = self.dense(z)\n",
        "        return z, attn_weights\n",
        "\n",
        "class PointwiseFFNN(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dense1 = Dense(units=dff, activation=\"relu\")\n",
        "        self.dense2 = Dense(units=d_model)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        # `(batch_size, seq_len, dff)`\n",
        "        z = self.dense1(x)\n",
        "        # `(batch_size, seq_len, d_model)`\n",
        "        z = self.dense2(z)\n",
        "        return z\n",
        "\n",
        "# ENCODER\n",
        "class Encoder(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.mha = MultiheadAttention()\n",
        "        self.layernormalization1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.ffnn = PointwiseFFNN()\n",
        "        self.layernormalization2 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def __call__(self, x, training, padding_mask):\n",
        "        ## \"Self-Attention\" Part\n",
        "        z1, _ = self.mha(queries=x, keys=x, values=x, mask=padding_mask) \n",
        "        z1 = Dropout(rate=0.1)(z1, training=training)\n",
        "        ## \"Add & Normalize\" Part\n",
        "        z1 = x + z1\n",
        "        z1 = self.layernormalization1(z1)\n",
        "        ## \"Feed Forward\" Part\n",
        "        z2 = self.ffnn(z1)\n",
        "        z2 = Dropout(rate=0.1)(z2, training=training)\n",
        "        ## \"Add & Normalize\" Part\n",
        "        z2 = z1 + z2\n",
        "        z2 = self.layernormalization2(z2)\n",
        "        # (batch_size, seq_len_enc, d_model)\n",
        "        return z2\n",
        "\n",
        "# DECODER\n",
        "class Decoder(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.mha1 = MultiheadAttention()\n",
        "        self.layernormalization1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.mha2 = MultiheadAttention()\n",
        "        self.layernormalization2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.ffnn = PointwiseFFNN()\n",
        "        self.layernormalization3 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, padding_mask, look_ahead_mask):\n",
        "        ## \"Self-Attention\" Part\n",
        "        z1, attn_weights1 = self.mha1(queries=x, keys=x, values=x, mask=look_ahead_mask)\n",
        "        z1 = Dropout(rate=0.1)(z1, training=training)\n",
        "        ## \"Add & Normalize\" Part\n",
        "        z1 = x + z1\n",
        "        z1 = self.layernormalization1(z1)\n",
        "        ## \"Encoder-Decoder Attention\" Part\n",
        "        z2, attn_weights2 = self.mha2(queries=z1, keys=enc_output, values=enc_output, mask=padding_mask)\n",
        "        z2 = Dropout(rate=0.1)(z2, training=training)\n",
        "        ## \"Add & Normalize\" Part\n",
        "        z2 = z1 + z2\n",
        "        z2 = self.layernormalization2(z2)\n",
        "        ## \"Feed Forward\" Part\n",
        "        z3 = self.ffnn(z2)\n",
        "        z3 = Dropout(rate=0.1)(z3, training=training)\n",
        "        ## \"Add & Normalize\" Part\n",
        "        z3 = z2 + z3\n",
        "        # (batch_size, seq_len_dec, d_model)\n",
        "        z3 = self.layernormalization3(z3)\n",
        "        return z3, attn_weights1, attn_weights2\n",
        "\n",
        "class Transformer(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedder_enc = Embedder(vocab_size=tokenizer_src.vocab_size + 2)\n",
        "        # self.embedder_enc = TransformerEmbedding(vocab_size=tokenizer_src.vocab_size + 2)\n",
        "        self.encoder = Encoder()\n",
        "        self.embedder_dec = Embedder(vocab_size=tokenizer_tar.vocab_size + 2)\n",
        "        # self.embedder_dec = TransformerEmbedding(vocab_size=tokenizer_tar.vocab_size + 2)\n",
        "        self.decoder = Decoder()\n",
        "        self.dense = Dense(units=tokenizer_tar.vocab_size + 2)\n",
        "        \n",
        "    def __call__(self, enc, dec, training):\n",
        "        enc_p_mask = padding_mask(enc)\n",
        "        dec_p_mask = padding_mask(enc)\n",
        "        l_mask = look_ahead_mask(dec)\n",
        "\n",
        "        z_enc = self.embedder_enc(enc, training=training)\n",
        "        for _ in range(n_layers):\n",
        "            z_enc = self.encoder(z_enc, training=training, padding_mask=enc_p_mask)\n",
        "\n",
        "        z_dec = self.embedder_dec(dec, training=training)\n",
        "#         dic = dict()\n",
        "        for i in range(n_layers):\n",
        "            z_dec, attn_weights1, attn_weights2 = self.decoder(z_dec, z_enc, training=training, padding_mask=dec_p_mask, look_ahead_mask=l_mask)\n",
        "#             dic[f\"self_attention_attn_weights_{i}\"] = attn_weights1\n",
        "#             dic[f\"encoder_decoder_attention_attn_weights_{i}\"] = attn_weights2\n",
        "\n",
        "        outputs = self.dense(z_dec)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zZ0hEBSD5ka"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer()\n",
        "\n",
        "class CustomSchedule(LearningRateSchedule):\n",
        "    def __init__(self, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        return (d_model**-0.5)*tf.math.minimum(step**-0.5, step*(self.warmup_steps**-1.5))\n",
        "\n",
        "learning_rate = CustomSchedule()\n",
        "optimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# Loss function\n",
        "loss_obj = losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "def loss_func(y_true, y_pred):\n",
        "    # `True` if not `0` else `False`\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    # `from_logits=True`: Computes loss from a logit (i.e, value in [int, int])\n",
        "    # `from_logits=False`: Computes loss from a probability (i.e, value in [0, 1])\n",
        "    # `reduction=\"none\"`: 계산된 Losses에 대해 별도의 연산을 하지 않습니다.\n",
        "    # `reduction=\"auto\"`: Batch에 대해 평균을 출력합니다.\n",
        "    # `reduction=\"sum\"`: Batch에 대해 합을 출력합니다.\n",
        "    loss = loss_obj(y_true, y_pred)\n",
        "    # `1` if True else `0`\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    loss *= mask\n",
        "    return tf.math.reduce_sum(loss)/tf.math.reduce_sum(mask)\n",
        "\n",
        "def acc_func(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "tr_loss = metrics.Mean(name=\"train_loss\")\n",
        "tr_acc = metrics.Mean(name=\"train_accuracy\")\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int64), tf.TensorSpec(shape=(None, None), dtype=tf.int64)])\n",
        "def train_step(enc, dec):\n",
        "    dec_input = dec[:, :-1]\n",
        "    dec_true = dec[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "        dec_pred = model(enc=enc, dec=dec_input, training=True)\n",
        "        loss = loss_func(dec_true, dec_pred)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    tr_loss(loss)\n",
        "    tr_acc(acc_func(dec_true, dec_pred))\n",
        "\n",
        "ckpt_path = \"drive/MyDrive/NLP/pt_to_en_transformer\"\n",
        "ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, directory=ckpt_path, max_to_keep=5)\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(save_path=ckpt_manager.latest_checkpoint)\n",
        "    print (\"Latest checkpoint restored!\")"
      ],
      "metadata": {
        "id": "QOqNrHtigWML"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-15T14:06:19.696327Z",
          "start_time": "2022-02-15T14:06:19.678569Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccV1tS2jU2sx",
        "outputId": "f53c1e5f-5f2e-4f23-d363-460c90807890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1 | Batch:     0 | Loss: 9.0148 | Accuracy: 0.0000\n",
            "Epoch:   1 | Batch:    50 | Loss: 8.9731 | Accuracy: 0.0004\n",
            "Epoch:   1 | Batch:   100 | Loss: 8.8829 | Accuracy: 0.0175\n",
            "Epoch:   1 | Batch:   150 | Loss: 8.7739 | Accuracy: 0.0292\n",
            "Epoch:   1 | Batch:   200 | Loss: 8.6456 | Accuracy: 0.0351\n",
            "Epoch:   1 | Batch:   250 | Loss: 8.4917 | Accuracy: 0.0398\n",
            "Epoch:   1 | Batch:   300 | Loss: 8.3174 | Accuracy: 0.0464\n",
            "Epoch:   1 | Batch:   350 | Loss: 8.1324 | Accuracy: 0.0550\n",
            "Epoch:   1 | Batch:   400 | Loss: 7.9519 | Accuracy: 0.0635\n",
            "Epoch:   1 | Batch:   450 | Loss: 7.7896 | Accuracy: 0.0701\n",
            "Epoch:   1 | Batch:   500 | Loss: 7.6470 | Accuracy: 0.0760\n",
            "Epoch:   1 | Batch:   550 | Loss: 7.5194 | Accuracy: 0.0812\n",
            "Epoch:   1 | Batch:   600 | Loss: 7.4000 | Accuracy: 0.0874\n",
            "Epoch:   1 | Batch:   650 | Loss: 7.2873 | Accuracy: 0.0944\n",
            "Epoch:   1 | Batch:   700 | Loss: 7.1774 | Accuracy: 0.1011\n",
            "Saving checkpoint for epoch 1 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-1\n",
            "Epoch:   1 | Loss: 7.1734 | Accuracy: 0.1013\n",
            "Time taken for 1 epoch:   178 secs\n",
            "\n",
            "Epoch:   2 | Batch:     0 | Loss: 5.7333 | Accuracy: 0.2023\n",
            "Epoch:   2 | Batch:    50 | Loss: 5.5721 | Accuracy: 0.2073\n",
            "Epoch:   2 | Batch:   100 | Loss: 5.5284 | Accuracy: 0.2117\n",
            "Epoch:   2 | Batch:   150 | Loss: 5.4760 | Accuracy: 0.2158\n",
            "Epoch:   2 | Batch:   200 | Loss: 5.4285 | Accuracy: 0.2206\n",
            "Epoch:   2 | Batch:   250 | Loss: 5.3857 | Accuracy: 0.2248\n",
            "Epoch:   2 | Batch:   300 | Loss: 5.3449 | Accuracy: 0.2286\n",
            "Epoch:   2 | Batch:   350 | Loss: 5.3056 | Accuracy: 0.2322\n",
            "Epoch:   2 | Batch:   400 | Loss: 5.2714 | Accuracy: 0.2353\n",
            "Epoch:   2 | Batch:   450 | Loss: 5.2349 | Accuracy: 0.2390\n",
            "Epoch:   2 | Batch:   500 | Loss: 5.2032 | Accuracy: 0.2424\n",
            "Epoch:   2 | Batch:   550 | Loss: 5.1741 | Accuracy: 0.2454\n",
            "Epoch:   2 | Batch:   600 | Loss: 5.1459 | Accuracy: 0.2485\n",
            "Epoch:   2 | Batch:   650 | Loss: 5.1217 | Accuracy: 0.2511\n",
            "Epoch:   2 | Batch:   700 | Loss: 5.0974 | Accuracy: 0.2537\n",
            "Saving checkpoint for epoch 2 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-2\n",
            "Epoch:   2 | Loss: 5.0966 | Accuracy: 0.2538\n",
            "Time taken for 1 epoch:    92 secs\n",
            "\n",
            "Epoch:   3 | Batch:     0 | Loss: 4.8039 | Accuracy: 0.2841\n",
            "Epoch:   3 | Batch:    50 | Loss: 4.6870 | Accuracy: 0.2958\n",
            "Epoch:   3 | Batch:   100 | Loss: 4.6713 | Accuracy: 0.2972\n",
            "Epoch:   3 | Batch:   150 | Loss: 4.6544 | Accuracy: 0.2987\n",
            "Epoch:   3 | Batch:   200 | Loss: 4.6404 | Accuracy: 0.2995\n",
            "Epoch:   3 | Batch:   250 | Loss: 4.6294 | Accuracy: 0.3008\n",
            "Epoch:   3 | Batch:   300 | Loss: 4.6177 | Accuracy: 0.3022\n",
            "Epoch:   3 | Batch:   350 | Loss: 4.6104 | Accuracy: 0.3029\n",
            "Epoch:   3 | Batch:   400 | Loss: 4.5958 | Accuracy: 0.3046\n",
            "Epoch:   3 | Batch:   450 | Loss: 4.5872 | Accuracy: 0.3054\n",
            "Epoch:   3 | Batch:   500 | Loss: 4.5767 | Accuracy: 0.3066\n",
            "Epoch:   3 | Batch:   550 | Loss: 4.5664 | Accuracy: 0.3075\n",
            "Epoch:   3 | Batch:   600 | Loss: 4.5555 | Accuracy: 0.3085\n",
            "Epoch:   3 | Batch:   650 | Loss: 4.5434 | Accuracy: 0.3100\n",
            "Epoch:   3 | Batch:   700 | Loss: 4.5304 | Accuracy: 0.3116\n",
            "Saving checkpoint for epoch 3 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-3\n",
            "Epoch:   3 | Loss: 4.5296 | Accuracy: 0.3118\n",
            "Time taken for 1 epoch:    92 secs\n",
            "\n",
            "Epoch:   4 | Batch:     0 | Loss: 4.3531 | Accuracy: 0.3225\n",
            "Epoch:   4 | Batch:    50 | Loss: 4.2346 | Accuracy: 0.3421\n",
            "Epoch:   4 | Batch:   100 | Loss: 4.2196 | Accuracy: 0.3433\n",
            "Epoch:   4 | Batch:   150 | Loss: 4.2132 | Accuracy: 0.3441\n",
            "Epoch:   4 | Batch:   200 | Loss: 4.2045 | Accuracy: 0.3454\n",
            "Epoch:   4 | Batch:   250 | Loss: 4.1969 | Accuracy: 0.3468\n",
            "Epoch:   4 | Batch:   300 | Loss: 4.1847 | Accuracy: 0.3483\n",
            "Epoch:   4 | Batch:   350 | Loss: 4.1752 | Accuracy: 0.3500\n",
            "Epoch:   4 | Batch:   400 | Loss: 4.1652 | Accuracy: 0.3515\n",
            "Epoch:   4 | Batch:   450 | Loss: 4.1492 | Accuracy: 0.3537\n",
            "Epoch:   4 | Batch:   500 | Loss: 4.1371 | Accuracy: 0.3555\n",
            "Epoch:   4 | Batch:   550 | Loss: 4.1234 | Accuracy: 0.3575\n",
            "Epoch:   4 | Batch:   600 | Loss: 4.1105 | Accuracy: 0.3594\n",
            "Epoch:   4 | Batch:   650 | Loss: 4.0985 | Accuracy: 0.3612\n",
            "Epoch:   4 | Batch:   700 | Loss: 4.0862 | Accuracy: 0.3631\n",
            "Saving checkpoint for epoch 4 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-4\n",
            "Epoch:   4 | Loss: 4.0855 | Accuracy: 0.3632\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:   5 | Batch:     0 | Loss: 3.6282 | Accuracy: 0.4221\n",
            "Epoch:   5 | Batch:    50 | Loss: 3.7019 | Accuracy: 0.4073\n",
            "Epoch:   5 | Batch:   100 | Loss: 3.7166 | Accuracy: 0.4060\n",
            "Epoch:   5 | Batch:   150 | Loss: 3.7092 | Accuracy: 0.4073\n",
            "Epoch:   5 | Batch:   200 | Loss: 3.7038 | Accuracy: 0.4087\n",
            "Epoch:   5 | Batch:   250 | Loss: 3.7003 | Accuracy: 0.4094\n",
            "Epoch:   5 | Batch:   300 | Loss: 3.6996 | Accuracy: 0.4101\n",
            "Epoch:   5 | Batch:   350 | Loss: 3.6903 | Accuracy: 0.4112\n",
            "Epoch:   5 | Batch:   400 | Loss: 3.6770 | Accuracy: 0.4132\n",
            "Epoch:   5 | Batch:   450 | Loss: 3.6676 | Accuracy: 0.4148\n",
            "Epoch:   5 | Batch:   500 | Loss: 3.6590 | Accuracy: 0.4162\n",
            "Epoch:   5 | Batch:   550 | Loss: 3.6524 | Accuracy: 0.4173\n",
            "Epoch:   5 | Batch:   600 | Loss: 3.6450 | Accuracy: 0.4186\n",
            "Epoch:   5 | Batch:   650 | Loss: 3.6357 | Accuracy: 0.4199\n",
            "Epoch:   5 | Batch:   700 | Loss: 3.6242 | Accuracy: 0.4215\n",
            "Saving checkpoint for epoch 5 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-5\n",
            "Epoch:   5 | Loss: 3.6241 | Accuracy: 0.4215\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:   6 | Batch:     0 | Loss: 3.3044 | Accuracy: 0.4424\n",
            "Epoch:   6 | Batch:    50 | Loss: 3.2575 | Accuracy: 0.4602\n",
            "Epoch:   6 | Batch:   100 | Loss: 3.2697 | Accuracy: 0.4602\n",
            "Epoch:   6 | Batch:   150 | Loss: 3.2742 | Accuracy: 0.4598\n",
            "Epoch:   6 | Batch:   200 | Loss: 3.2789 | Accuracy: 0.4593\n",
            "Epoch:   6 | Batch:   250 | Loss: 3.2751 | Accuracy: 0.4598\n",
            "Epoch:   6 | Batch:   300 | Loss: 3.2723 | Accuracy: 0.4607\n",
            "Epoch:   6 | Batch:   350 | Loss: 3.2678 | Accuracy: 0.4613\n",
            "Epoch:   6 | Batch:   400 | Loss: 3.2664 | Accuracy: 0.4617\n",
            "Epoch:   6 | Batch:   450 | Loss: 3.2670 | Accuracy: 0.4616\n",
            "Epoch:   6 | Batch:   500 | Loss: 3.2563 | Accuracy: 0.4631\n",
            "Epoch:   6 | Batch:   550 | Loss: 3.2485 | Accuracy: 0.4642\n",
            "Epoch:   6 | Batch:   600 | Loss: 3.2396 | Accuracy: 0.4656\n",
            "Epoch:   6 | Batch:   650 | Loss: 3.2352 | Accuracy: 0.4663\n",
            "Epoch:   6 | Batch:   700 | Loss: 3.2287 | Accuracy: 0.4672\n",
            "Saving checkpoint for epoch 6 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-6\n",
            "Epoch:   6 | Loss: 3.2288 | Accuracy: 0.4672\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:   7 | Batch:     0 | Loss: 2.8814 | Accuracy: 0.5017\n",
            "Epoch:   7 | Batch:    50 | Loss: 2.8569 | Accuracy: 0.5066\n",
            "Epoch:   7 | Batch:   100 | Loss: 2.8627 | Accuracy: 0.5075\n",
            "Epoch:   7 | Batch:   150 | Loss: 2.8507 | Accuracy: 0.5073\n",
            "Epoch:   7 | Batch:   200 | Loss: 2.8540 | Accuracy: 0.5076\n",
            "Epoch:   7 | Batch:   250 | Loss: 2.8581 | Accuracy: 0.5077\n",
            "Epoch:   7 | Batch:   300 | Loss: 2.8604 | Accuracy: 0.5075\n",
            "Epoch:   7 | Batch:   350 | Loss: 2.8590 | Accuracy: 0.5079\n",
            "Epoch:   7 | Batch:   400 | Loss: 2.8563 | Accuracy: 0.5085\n",
            "Epoch:   7 | Batch:   450 | Loss: 2.8526 | Accuracy: 0.5093\n",
            "Epoch:   7 | Batch:   500 | Loss: 2.8509 | Accuracy: 0.5097\n",
            "Epoch:   7 | Batch:   550 | Loss: 2.8454 | Accuracy: 0.5105\n",
            "Epoch:   7 | Batch:   600 | Loss: 2.8411 | Accuracy: 0.5113\n",
            "Epoch:   7 | Batch:   650 | Loss: 2.8372 | Accuracy: 0.5119\n",
            "Epoch:   7 | Batch:   700 | Loss: 2.8310 | Accuracy: 0.5128\n",
            "Saving checkpoint for epoch 7 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-7\n",
            "Epoch:   7 | Loss: 2.8314 | Accuracy: 0.5128\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:   8 | Batch:     0 | Loss: 2.5528 | Accuracy: 0.5415\n",
            "Epoch:   8 | Batch:    50 | Loss: 2.4884 | Accuracy: 0.5474\n",
            "Epoch:   8 | Batch:   100 | Loss: 2.4786 | Accuracy: 0.5516\n",
            "Epoch:   8 | Batch:   150 | Loss: 2.4768 | Accuracy: 0.5526\n",
            "Epoch:   8 | Batch:   200 | Loss: 2.4886 | Accuracy: 0.5518\n",
            "Epoch:   8 | Batch:   250 | Loss: 2.4950 | Accuracy: 0.5512\n",
            "Epoch:   8 | Batch:   300 | Loss: 2.5018 | Accuracy: 0.5505\n",
            "Epoch:   8 | Batch:   350 | Loss: 2.5009 | Accuracy: 0.5509\n",
            "Epoch:   8 | Batch:   400 | Loss: 2.4992 | Accuracy: 0.5510\n",
            "Epoch:   8 | Batch:   450 | Loss: 2.5004 | Accuracy: 0.5510\n",
            "Epoch:   8 | Batch:   500 | Loss: 2.5011 | Accuracy: 0.5511\n",
            "Epoch:   8 | Batch:   550 | Loss: 2.4999 | Accuracy: 0.5514\n",
            "Epoch:   8 | Batch:   600 | Loss: 2.4980 | Accuracy: 0.5519\n",
            "Epoch:   8 | Batch:   650 | Loss: 2.4980 | Accuracy: 0.5521\n",
            "Epoch:   8 | Batch:   700 | Loss: 2.4975 | Accuracy: 0.5523\n",
            "Saving checkpoint for epoch 8 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-8\n",
            "Epoch:   8 | Loss: 2.4973 | Accuracy: 0.5523\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:   9 | Batch:     0 | Loss: 2.1733 | Accuracy: 0.5946\n",
            "Epoch:   9 | Batch:    50 | Loss: 2.2063 | Accuracy: 0.5868\n",
            "Epoch:   9 | Batch:   100 | Loss: 2.2091 | Accuracy: 0.5868\n",
            "Epoch:   9 | Batch:   150 | Loss: 2.2099 | Accuracy: 0.5876\n",
            "Epoch:   9 | Batch:   200 | Loss: 2.2192 | Accuracy: 0.5865\n",
            "Epoch:   9 | Batch:   250 | Loss: 2.2246 | Accuracy: 0.5857\n",
            "Epoch:   9 | Batch:   300 | Loss: 2.2251 | Accuracy: 0.5859\n",
            "Epoch:   9 | Batch:   350 | Loss: 2.2312 | Accuracy: 0.5849\n",
            "Epoch:   9 | Batch:   400 | Loss: 2.2379 | Accuracy: 0.5839\n",
            "Epoch:   9 | Batch:   450 | Loss: 2.2387 | Accuracy: 0.5839\n",
            "Epoch:   9 | Batch:   500 | Loss: 2.2424 | Accuracy: 0.5832\n",
            "Epoch:   9 | Batch:   550 | Loss: 2.2457 | Accuracy: 0.5826\n",
            "Epoch:   9 | Batch:   600 | Loss: 2.2495 | Accuracy: 0.5821\n",
            "Epoch:   9 | Batch:   650 | Loss: 2.2522 | Accuracy: 0.5819\n",
            "Epoch:   9 | Batch:   700 | Loss: 2.2557 | Accuracy: 0.5815\n",
            "Saving checkpoint for epoch 9 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-9\n",
            "Epoch:   9 | Loss: 2.2551 | Accuracy: 0.5816\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  10 | Batch:     0 | Loss: 2.0346 | Accuracy: 0.6105\n",
            "Epoch:  10 | Batch:    50 | Loss: 2.0186 | Accuracy: 0.6080\n",
            "Epoch:  10 | Batch:   100 | Loss: 2.0118 | Accuracy: 0.6098\n",
            "Epoch:  10 | Batch:   150 | Loss: 2.0190 | Accuracy: 0.6095\n",
            "Epoch:  10 | Batch:   200 | Loss: 2.0344 | Accuracy: 0.6074\n",
            "Epoch:  10 | Batch:   250 | Loss: 2.0334 | Accuracy: 0.6080\n",
            "Epoch:  10 | Batch:   300 | Loss: 2.0405 | Accuracy: 0.6072\n",
            "Epoch:  10 | Batch:   350 | Loss: 2.0448 | Accuracy: 0.6065\n",
            "Epoch:  10 | Batch:   400 | Loss: 2.0478 | Accuracy: 0.6064\n",
            "Epoch:  10 | Batch:   450 | Loss: 2.0554 | Accuracy: 0.6055\n",
            "Epoch:  10 | Batch:   500 | Loss: 2.0595 | Accuracy: 0.6050\n",
            "Epoch:  10 | Batch:   550 | Loss: 2.0621 | Accuracy: 0.6048\n",
            "Epoch:  10 | Batch:   600 | Loss: 2.0655 | Accuracy: 0.6045\n",
            "Epoch:  10 | Batch:   650 | Loss: 2.0696 | Accuracy: 0.6041\n",
            "Epoch:  10 | Batch:   700 | Loss: 2.0730 | Accuracy: 0.6037\n",
            "Saving checkpoint for epoch 10 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-10\n",
            "Epoch:  10 | Loss: 2.0729 | Accuracy: 0.6037\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  11 | Batch:     0 | Loss: 1.8281 | Accuracy: 0.6275\n",
            "Epoch:  11 | Batch:    50 | Loss: 1.8680 | Accuracy: 0.6299\n",
            "Epoch:  11 | Batch:   100 | Loss: 1.8494 | Accuracy: 0.6332\n",
            "Epoch:  11 | Batch:   150 | Loss: 1.8596 | Accuracy: 0.6308\n",
            "Epoch:  11 | Batch:   200 | Loss: 1.8701 | Accuracy: 0.6293\n",
            "Epoch:  11 | Batch:   250 | Loss: 1.8780 | Accuracy: 0.6285\n",
            "Epoch:  11 | Batch:   300 | Loss: 1.8861 | Accuracy: 0.6273\n",
            "Epoch:  11 | Batch:   350 | Loss: 1.8942 | Accuracy: 0.6262\n",
            "Epoch:  11 | Batch:   400 | Loss: 1.9005 | Accuracy: 0.6256\n",
            "Epoch:  11 | Batch:   450 | Loss: 1.9061 | Accuracy: 0.6248\n",
            "Epoch:  11 | Batch:   500 | Loss: 1.9123 | Accuracy: 0.6241\n",
            "Epoch:  11 | Batch:   550 | Loss: 1.9184 | Accuracy: 0.6235\n",
            "Epoch:  11 | Batch:   600 | Loss: 1.9215 | Accuracy: 0.6231\n",
            "Epoch:  11 | Batch:   650 | Loss: 1.9277 | Accuracy: 0.6222\n",
            "Epoch:  11 | Batch:   700 | Loss: 1.9310 | Accuracy: 0.6219\n",
            "Saving checkpoint for epoch 11 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-11\n",
            "Epoch:  11 | Loss: 1.9312 | Accuracy: 0.6219\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  12 | Batch:     0 | Loss: 1.5393 | Accuracy: 0.6829\n",
            "Epoch:  12 | Batch:    50 | Loss: 1.7142 | Accuracy: 0.6513\n",
            "Epoch:  12 | Batch:   100 | Loss: 1.7224 | Accuracy: 0.6488\n",
            "Epoch:  12 | Batch:   150 | Loss: 1.7312 | Accuracy: 0.6468\n",
            "Epoch:  12 | Batch:   200 | Loss: 1.7399 | Accuracy: 0.6458\n",
            "Epoch:  12 | Batch:   250 | Loss: 1.7488 | Accuracy: 0.6452\n",
            "Epoch:  12 | Batch:   300 | Loss: 1.7551 | Accuracy: 0.6444\n",
            "Epoch:  12 | Batch:   350 | Loss: 1.7657 | Accuracy: 0.6428\n",
            "Epoch:  12 | Batch:   400 | Loss: 1.7759 | Accuracy: 0.6415\n",
            "Epoch:  12 | Batch:   450 | Loss: 1.7845 | Accuracy: 0.6403\n",
            "Epoch:  12 | Batch:   500 | Loss: 1.7902 | Accuracy: 0.6395\n",
            "Epoch:  12 | Batch:   550 | Loss: 1.7973 | Accuracy: 0.6385\n",
            "Epoch:  12 | Batch:   600 | Loss: 1.8043 | Accuracy: 0.6378\n",
            "Epoch:  12 | Batch:   650 | Loss: 1.8122 | Accuracy: 0.6366\n",
            "Epoch:  12 | Batch:   700 | Loss: 1.8173 | Accuracy: 0.6360\n",
            "Saving checkpoint for epoch 12 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-12\n",
            "Epoch:  12 | Loss: 1.8176 | Accuracy: 0.6359\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  13 | Batch:     0 | Loss: 1.6844 | Accuracy: 0.6638\n",
            "Epoch:  13 | Batch:    50 | Loss: 1.6101 | Accuracy: 0.6650\n",
            "Epoch:  13 | Batch:   100 | Loss: 1.6315 | Accuracy: 0.6619\n",
            "Epoch:  13 | Batch:   150 | Loss: 1.6457 | Accuracy: 0.6594\n",
            "Epoch:  13 | Batch:   200 | Loss: 1.6538 | Accuracy: 0.6583\n",
            "Epoch:  13 | Batch:   250 | Loss: 1.6612 | Accuracy: 0.6576\n",
            "Epoch:  13 | Batch:   300 | Loss: 1.6695 | Accuracy: 0.6565\n",
            "Epoch:  13 | Batch:   350 | Loss: 1.6788 | Accuracy: 0.6550\n",
            "Epoch:  13 | Batch:   400 | Loss: 1.6824 | Accuracy: 0.6544\n",
            "Epoch:  13 | Batch:   450 | Loss: 1.6874 | Accuracy: 0.6537\n",
            "Epoch:  13 | Batch:   500 | Loss: 1.6928 | Accuracy: 0.6531\n",
            "Epoch:  13 | Batch:   550 | Loss: 1.7006 | Accuracy: 0.6518\n",
            "Epoch:  13 | Batch:   600 | Loss: 1.7073 | Accuracy: 0.6508\n",
            "Epoch:  13 | Batch:   650 | Loss: 1.7138 | Accuracy: 0.6499\n",
            "Epoch:  13 | Batch:   700 | Loss: 1.7208 | Accuracy: 0.6491\n",
            "Saving checkpoint for epoch 13 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-13\n",
            "Epoch:  13 | Loss: 1.7210 | Accuracy: 0.6491\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  14 | Batch:     0 | Loss: 1.4781 | Accuracy: 0.6855\n",
            "Epoch:  14 | Batch:    50 | Loss: 1.5281 | Accuracy: 0.6754\n",
            "Epoch:  14 | Batch:   100 | Loss: 1.5465 | Accuracy: 0.6737\n",
            "Epoch:  14 | Batch:   150 | Loss: 1.5590 | Accuracy: 0.6723\n",
            "Epoch:  14 | Batch:   200 | Loss: 1.5687 | Accuracy: 0.6705\n",
            "Epoch:  14 | Batch:   250 | Loss: 1.5747 | Accuracy: 0.6692\n",
            "Epoch:  14 | Batch:   300 | Loss: 1.5867 | Accuracy: 0.6669\n",
            "Epoch:  14 | Batch:   350 | Loss: 1.5949 | Accuracy: 0.6658\n",
            "Epoch:  14 | Batch:   400 | Loss: 1.6022 | Accuracy: 0.6650\n",
            "Epoch:  14 | Batch:   450 | Loss: 1.6103 | Accuracy: 0.6641\n",
            "Epoch:  14 | Batch:   500 | Loss: 1.6182 | Accuracy: 0.6628\n",
            "Epoch:  14 | Batch:   550 | Loss: 1.6242 | Accuracy: 0.6621\n",
            "Epoch:  14 | Batch:   600 | Loss: 1.6299 | Accuracy: 0.6612\n",
            "Epoch:  14 | Batch:   650 | Loss: 1.6361 | Accuracy: 0.6603\n",
            "Epoch:  14 | Batch:   700 | Loss: 1.6401 | Accuracy: 0.6597\n",
            "Saving checkpoint for epoch 14 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-14\n",
            "Epoch:  14 | Loss: 1.6412 | Accuracy: 0.6596\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  15 | Batch:     0 | Loss: 1.5383 | Accuracy: 0.6828\n",
            "Epoch:  15 | Batch:    50 | Loss: 1.4612 | Accuracy: 0.6849\n",
            "Epoch:  15 | Batch:   100 | Loss: 1.4695 | Accuracy: 0.6846\n",
            "Epoch:  15 | Batch:   150 | Loss: 1.4909 | Accuracy: 0.6809\n",
            "Epoch:  15 | Batch:   200 | Loss: 1.4976 | Accuracy: 0.6801\n",
            "Epoch:  15 | Batch:   250 | Loss: 1.5086 | Accuracy: 0.6783\n",
            "Epoch:  15 | Batch:   300 | Loss: 1.5177 | Accuracy: 0.6766\n",
            "Epoch:  15 | Batch:   350 | Loss: 1.5262 | Accuracy: 0.6759\n",
            "Epoch:  15 | Batch:   400 | Loss: 1.5325 | Accuracy: 0.6750\n",
            "Epoch:  15 | Batch:   450 | Loss: 1.5385 | Accuracy: 0.6742\n",
            "Epoch:  15 | Batch:   500 | Loss: 1.5461 | Accuracy: 0.6729\n",
            "Epoch:  15 | Batch:   550 | Loss: 1.5523 | Accuracy: 0.6720\n",
            "Epoch:  15 | Batch:   600 | Loss: 1.5587 | Accuracy: 0.6713\n",
            "Epoch:  15 | Batch:   650 | Loss: 1.5641 | Accuracy: 0.6708\n",
            "Epoch:  15 | Batch:   700 | Loss: 1.5691 | Accuracy: 0.6700\n",
            "Saving checkpoint for epoch 15 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-15\n",
            "Epoch:  15 | Loss: 1.5688 | Accuracy: 0.6701\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  16 | Batch:     0 | Loss: 1.3964 | Accuracy: 0.7028\n",
            "Epoch:  16 | Batch:    50 | Loss: 1.4038 | Accuracy: 0.6965\n",
            "Epoch:  16 | Batch:   100 | Loss: 1.4127 | Accuracy: 0.6941\n",
            "Epoch:  16 | Batch:   150 | Loss: 1.4299 | Accuracy: 0.6914\n",
            "Epoch:  16 | Batch:   200 | Loss: 1.4368 | Accuracy: 0.6893\n",
            "Epoch:  16 | Batch:   250 | Loss: 1.4468 | Accuracy: 0.6877\n",
            "Epoch:  16 | Batch:   300 | Loss: 1.4540 | Accuracy: 0.6864\n",
            "Epoch:  16 | Batch:   350 | Loss: 1.4618 | Accuracy: 0.6853\n",
            "Epoch:  16 | Batch:   400 | Loss: 1.4688 | Accuracy: 0.6841\n",
            "Epoch:  16 | Batch:   450 | Loss: 1.4779 | Accuracy: 0.6828\n",
            "Epoch:  16 | Batch:   500 | Loss: 1.4867 | Accuracy: 0.6813\n",
            "Epoch:  16 | Batch:   550 | Loss: 1.4946 | Accuracy: 0.6802\n",
            "Epoch:  16 | Batch:   600 | Loss: 1.5005 | Accuracy: 0.6793\n",
            "Epoch:  16 | Batch:   650 | Loss: 1.5065 | Accuracy: 0.6785\n",
            "Epoch:  16 | Batch:   700 | Loss: 1.5104 | Accuracy: 0.6779\n",
            "Saving checkpoint for epoch 16 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-16\n",
            "Epoch:  16 | Loss: 1.5108 | Accuracy: 0.6779\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  17 | Batch:     0 | Loss: 1.3532 | Accuracy: 0.7129\n",
            "Epoch:  17 | Batch:    50 | Loss: 1.3354 | Accuracy: 0.7060\n",
            "Epoch:  17 | Batch:   100 | Loss: 1.3567 | Accuracy: 0.7019\n",
            "Epoch:  17 | Batch:   150 | Loss: 1.3630 | Accuracy: 0.7015\n",
            "Epoch:  17 | Batch:   200 | Loss: 1.3809 | Accuracy: 0.6980\n",
            "Epoch:  17 | Batch:   250 | Loss: 1.3902 | Accuracy: 0.6965\n",
            "Epoch:  17 | Batch:   300 | Loss: 1.3978 | Accuracy: 0.6949\n",
            "Epoch:  17 | Batch:   350 | Loss: 1.4035 | Accuracy: 0.6941\n",
            "Epoch:  17 | Batch:   400 | Loss: 1.4089 | Accuracy: 0.6930\n",
            "Epoch:  17 | Batch:   450 | Loss: 1.4190 | Accuracy: 0.6914\n",
            "Epoch:  17 | Batch:   500 | Loss: 1.4270 | Accuracy: 0.6900\n",
            "Epoch:  17 | Batch:   550 | Loss: 1.4342 | Accuracy: 0.6887\n",
            "Epoch:  17 | Batch:   600 | Loss: 1.4413 | Accuracy: 0.6878\n",
            "Epoch:  17 | Batch:   650 | Loss: 1.4481 | Accuracy: 0.6868\n",
            "Epoch:  17 | Batch:   700 | Loss: 1.4542 | Accuracy: 0.6860\n",
            "Saving checkpoint for epoch 17 at drive/MyDrive/NLP/pt_to_en_transformer/ckpt-17\n",
            "Epoch:  17 | Loss: 1.4544 | Accuracy: 0.6860\n",
            "Time taken for 1 epoch:    91 secs\n",
            "\n",
            "Epoch:  18 | Batch:     0 | Loss: 1.3211 | Accuracy: 0.6935\n",
            "Epoch:  18 | Batch:    50 | Loss: 1.3057 | Accuracy: 0.7095\n",
            "Epoch:  18 | Batch:   100 | Loss: 1.3056 | Accuracy: 0.7098\n"
          ]
        }
      ],
      "source": [
        "epochs = 24\n",
        "for epoch in range(1, epochs + 1):\n",
        "    start = time.time()\n",
        "    tr_loss.reset_states()\n",
        "    tr_acc.reset_states()\n",
        "    for (batch, (src, tar)) in enumerate(dataset_tr):\n",
        "        train_step(src, tar)\n",
        "\n",
        "        if batch%50 == 0:\n",
        "            print(f\"Epoch: {epoch:3d} | Batch: {batch:5d} | Loss: {tr_loss.result():5.4f} | Accuracy: {tr_acc.result():5.4f}\")\n",
        "\n",
        "    if epoch%1 == 0:\n",
        "        # Every time `ckpt_manager.save()` is called, `save_counter` is increased.\n",
        "        # `save_path`: The path to the new checkpoint. It is also recorded in the `checkpoints` and `latest_checkpoint` properties. `None` if no checkpoint is saved.\n",
        "        save_path = ckpt_manager.save()\n",
        "        print (f\"Saving checkpoint for epoch {epoch} at {save_path}\")\n",
        "        print(f\"Epoch: {epoch:3d} | Loss: {tr_loss.result():5.4f} | Accuracy: {tr_acc.result():5.4f}\")\n",
        "        print (f\"Time taken for 1 epoch: {time.time() - start:5.0f} secs\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45sJJKmdL8u-"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T11:01:09.737712Z",
          "start_time": "2022-02-11T10:58:37.985Z"
        },
        "id": "fdaL7LLfGzaQ"
      },
      "outputs": [],
      "source": [
        "def evaluate(src_sentence):\n",
        "    # src_sentence : 문자 (string)\n",
        "    bos = [tokenizer_src.vocab_size]\n",
        "    eos = [tokenizer_src.vocab_size + 1]\n",
        "\n",
        "    src_sentence = bos + tokenizer_src.encode(src_sentence) + eos\n",
        "    encoder_input = tf.expand_dims(src_sentence, 0)\n",
        "\n",
        "    decoder_input = [tokenizer_tar.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    for i in range(max_len):\n",
        "        # (batch_size, seq_len, vocab_size)\n",
        "        predictions = model(encoder_input, output, False)\n",
        "        # 예측 결과에서 마지막 부분만 추출\n",
        "        # (batch_size, 1, vocab_size)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        if predicted_id == tokenizer_tar.vocab_size + 1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        # 예측된 단어를 전 단어와 결합하여 다음 예측에 써먹음\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        return tf.squeeze(output, axis=0)\n",
        "\n",
        "def translate(sentence):\n",
        "    result= evaluate(sentence)\n",
        "    predicted_sentence = tokenizer_tar.decode([i for i in result if i < tokenizer_tar.vocab_size])  \n",
        "\n",
        "    print(\"input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T11:01:09.739714Z",
          "start_time": "2022-02-11T10:58:37.988Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czjSk64HHCq5",
        "outputId": "a951436c-8010-484a-9141-2ff1105e63a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: este é um problema que temos que resolver.\n",
            "Predicted translation: and \n",
            "Real translation: this is a problem we have to solve .\n"
          ]
        }
      ],
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\n",
        "print(\"Real translation: this is a problem we have to solve .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T11:01:09.740715Z",
          "start_time": "2022-02-11T10:58:37.991Z"
        },
        "id": "WkQuQyvjHcbJ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1a44b7-e371-4603-a382-af68629ec989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: os meus vizinhos ouviram sobre esta ideia.\n",
            "Predicted translation: and \n",
            "Real translation: and my neighboring homes heard about this idea .\n"
          ]
        }
      ],
      "source": [
        "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
        "print(\"Real translation: and my neighboring homes heard about this idea .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T11:01:09.742718Z",
          "start_time": "2022-02-11T10:58:37.995Z"
        },
        "id": "76ZizBuyPbd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4849d281-f54c-4ccf-89f8-d71ca3d3b8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
            "Predicted translation: but \n",
            "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
          ]
        }
      ],
      "source": [
        "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
        "print(\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-11T11:01:09.744714Z",
          "start_time": "2022-02-11T10:58:37.998Z"
        },
        "id": "29vxXk81PlbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e707dc-04a5-4efe-af6c-f76158c61975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: este é o primeiro livro que eu fiz.\n",
            "Predicted translation: and \n",
            "Real translation: this is the first book i've ever done.\n"
          ]
        }
      ],
      "source": [
        "translate(\"este é o primeiro livro que eu fiz.\")\n",
        "print (\"Real translation: this is the first book i've ever done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn44IonrZhnV"
      },
      "source": [
        "출처  \n",
        "http://jalammar.github.io/illustrated-gpt2/  \n",
        "https://d2l.ai/chapter_recurrent-modern/seq2seq.html  \n",
        "https://www.tensorflow.org/tutorials/text/transformer\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Portuguese-English & Transformer (NMT).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}