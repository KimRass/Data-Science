{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIEwBs6S0iw8"
   },
   "source": [
    "# BiLingual Evaluation Understudy Score\n",
    "- BLEU는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법입니다. 측정 기준은 n-gram에 기반합니다.\n",
    "- 번역된 문장을 `cand`(candidate), 완벽한 번역 문장을 `ref`(Reference)라고 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-03T14:02:05.298305Z",
     "start_time": "2022-01-03T14:02:03.863235Z"
    },
    "id": "kb9QnE3_0iw-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7BhqvQ6xLTi"
   },
   "source": [
    "## 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-03T13:48:14.143625Z",
     "start_time": "2022-01-03T13:48:12.248Z"
    },
    "id": "Yi7AfUmRxYgZ"
   },
   "outputs": [],
   "source": [
    "def get_ngram2cnt(cand, n):\n",
    "    return Counter(nltk.ngrams(cand, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-03T13:48:14.143625Z",
     "start_time": "2022-01-03T13:48:12.456Z"
    },
    "id": "ytGG39gC0ixA",
    "outputId": "3799c0ac-2575-4a7b-dfda-f9de4bece902"
   },
   "outputs": [],
   "source": [
    "get_ngram2cnt(cand1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoNmktwjxieM"
   },
   "source": [
    "### N-gram Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_zjPuyW0ixC"
   },
   "outputs": [],
   "source": [
    "def get_ngram_precision(cand, refs, n):\n",
    "    ngram2cnt_refs = Counter()\n",
    "    for ref in refs:\n",
    "        ngram2cnt_refs += get_ngram2cnt(ref, n)\n",
    "    ngrams_in_refs = 0\n",
    "    len_cand = 0\n",
    "    for ngram, cnt in get_ngram2cnt(cand, n).items():\n",
    "        if ngram in ngram2cnt_refs:\n",
    "            ngrams_in_refs += cnt \n",
    "        len_cand += cnt\n",
    "    return ngrams_in_refs/len_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTPdHH_e0ixD",
    "outputId": "5c027715-6d09-4942-8dea-a548df56653a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(get_ngram_precision(cand1, refs, 1))\n",
    "print(get_ngram_precision(cand2, refs, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSXUjRrN0ixE"
   },
   "source": [
    "### Modified N-gram Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vz7FgLi0ixE"
   },
   "outputs": [],
   "source": [
    "def get_modified_ngram_precision(cand, refs, n):\n",
    "    def get_count_clip(ngram, cand, refs, n):\n",
    "        def get_max_ref_count(ngram, refs, n):\n",
    "            temp = list()\n",
    "            for ref in refs:\n",
    "                ngram2cnt_ref = get_ngram2cnt(ref, n)\n",
    "                temp.append(ngram2cnt_ref[ngram])\n",
    "            return max(temp)    \n",
    "\n",
    "        def get_count(ngram, cand, n):\n",
    "            return get_ngram2cnt(cand, 1)[ngram]\n",
    "\n",
    "        return min(get_count(ngram, cand, n), get_max_ref_count(ngram, refs, n))\n",
    "    \n",
    "    sum_countclip = 0\n",
    "    len_cand = 0\n",
    "    for ngram, cnt in get_ngram2cnt(cand, n).items():\n",
    "        sum_countclip += get_count_clip(ngram, cand, refs, n)\n",
    "        len_cand += cnt\n",
    "    return sum_countclip/len_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri4LNA260ixG",
    "outputId": "ba5b1486-9477-48d1-f504-7e6373635219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(get_modified_ngram_precision(cand1, refs, 1))\n",
    "print(get_modified_ngram_precision(cand2, refs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PiWATOk0ixH",
    "outputId": "c9ceba5c-33b0-45c0-96c5-17b8d026db46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "cand = \"the the the the the the the\"\n",
    "print(get_ngram_precision(cand.split(\" \"), refs, 1))\n",
    "print(get_modified_ngram_precision(cand.split(\" \"), refs, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgsirDfSkpEF"
   },
   "source": [
    "### 짧은 문장 길이에 대한 패널티(Brevity Penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ9K0Zqzk_xU"
   },
   "source": [
    "Ref가 1개라면 Ca와 Ref의 두 문장의 길이만을 가지고 계산하면 되겠지만 여기서는 Ref가 여러 개일 때를 가정하고 있으므로 r은 \"모든 Ref들 중에서 Ca와 가장 길이 차이가 작은 Ref의 길이\"로 합니다. r을 구하는 코드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQUUVtYXkhET"
   },
   "outputs": [],
   "source": [
    "def closest_ref_length(cand, ref_list): # Ca 길이와 가장 근접한 Ref의 길이를 리턴하는 함수\n",
    "    ca_len = len(cand) # ca 길이\n",
    "    ref_lens = (len(ref) for ref in ref_list) # Ref들의 길이\n",
    "    closest_ref_len = min(ref_lens, key=lambda ref_len: (abs(ref_len - ca_len), ref_len))\n",
    "    # 길이 차이를 최소화하는 Ref를 찾아서 Ref의 길이를 리턴\n",
    "    return closest_ref_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z3PfH-2lDpS"
   },
   "source": [
    "만약 Ca와 길이가 정확히 동일한 Ref가 있다면 길이 차이가 0인 최고 수준의 매치(best match length)입니다. 또한 만약 서로 다른 길이의 Ref이지만 Ca와 길이 차이가 동일한 경우에는 더 작은 길이의 Ref를 택합니다. 예를 들어 Ca가 길이가 10인데, Ref 1, 2가 각각 9와 11이라면 길이 차이는 동일하게 1밖에 나지 않지만 9를 택합니다. closest_ref_length 함수를 통해 r을 구했다면, 이제 BP를 구하는 함수 brevity_penalty를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQhnL1IFlFhj"
   },
   "outputs": [],
   "source": [
    "def brevity_penalty(cand, ref_list):\n",
    "    ca_len = len(cand)\n",
    "    ref_len = closest_ref_length(cand, ref_list)\n",
    "\n",
    "    if ca_len > ref_len:\n",
    "        return 1\n",
    "    elif ca_len == 0 :\n",
    "    # cand가 비어있다면 BP = 0 → BLEU = 0.0\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(1 - ref_len/ca_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-3GR1lhlNLN"
   },
   "source": [
    "## BLEU 함수 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57DM-GuqlILF"
   },
   "source": [
    "이제 최종적으로 BLEU 점수를 계산하는 함수 bleu_score를 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9veUzn1lGP6"
   },
   "outputs": [],
   "source": [
    "def bleu_score(cand, ref_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    bp = brevity_penalty(cand, ref_list) # 브레버티 패널티, BP\n",
    "\n",
    "    p_n = [modified_precision(cand, ref_list, n=n) for n, _ in enumerate(weights,start=1)] \n",
    "    #p1, p2, p3, ..., pn\n",
    "    score = np.sum([w_i * np.log(p_i) if p_i != 0 else 0 for w_i, p_i in zip(weights, p_n)])\n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbUorUGolQ4x"
   },
   "source": [
    "위 함수가 동작하기 위해서는 앞서 구현한 get_ngram2cnt, count_clip, modified_precision, brevity_penalty 4개의 함수 또한 모두 구현되어져 있어야 합니다. 지금까지 구현한 BLEU 코드로 계산된 점수와 NLTK 패키지에 이미 구현되어져 있는 BLEU 코드로 계산된 점수를 비교해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJrYyJOqxE5b"
   },
   "source": [
    "## nltk.translate.bleu_score.sentence_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DwlomX-d1TSa",
    "outputId": "e733e272-62e5-4542-8778-3e20acdd5b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ref = [[\"this\", \"is\", \"a\", \"test\"], [\"this\", \"is\" \"test\"]]\n",
    "cand = [\"this\", \"is\", \"a\", \"test\"]\n",
    "score = nltk.translate.bleu_score.sentence_bleu(ref, cand)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "QpDcrpU-26Lu",
    "outputId": "f7fc1f05-e620-4c07-ddec-a647d45b1b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "refs = [[[\"this\", \"is\", \"a\", \"test\"], [\"this\", \"is\" \"test\"]]]\n",
    "cands = [[\"this\", \"is\", \"a\", \"test\"]]\n",
    "score = nltk.translate.bleu_score.corpus_bleu(refs, cands)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3xs_bcp34V9"
   },
   "source": [
    "- 각 N-gram(N = 1, 2, 3, 4)에 대해 가중치를 서로 다르게 설정해 weighted geometric mean을 구할 수도 있습니다.\n",
    "- 몇 개의 N-gram을 사용하느냐에 따라 BLEU-1, BLEU-2, BLEU-3, BLEU-4라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "nJkNdjXA6AVO",
    "outputId": "b7e4fbc8-3689-47f9-a33d-bf123939efcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-2 = 0.49999999999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5CG7092POZ\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\5CG7092POZ\\Anaconda3\\envs\\tf2.3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# cumulative BLEU scores\n",
    "ref = [[\"this\", \"is\", \"small\", \"test\"]]\n",
    "cand = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "print(f\"BLEU-2 = {nltk.translate.bleu_score.sentence_bleu(ref, cand, weights=(1/2, 1/2, 0, 0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3LGHmUPlUWz"
   },
   "source": [
    "## NLTK의 BLEU Vs. 구현한 BLEU 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "h0uOo_H-lXbz",
    "outputId": "6ad2db98-ac10-4462-b3b0-c2a217fc06ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5045666840058485\n",
      "0.5045666840058485\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "\n",
    "cand = \"It is a guide to action which ensures that the military always obeys the commands of the party\"\n",
    "refs = [\n",
    "    \"It is a guide to action that ensures that the military will forever heed Party commands\",\n",
    "    \"It is the guiding principle which guarantees the military forces always being under the command of the Party\",\n",
    "    \"It is the practical guide for the army always to heed the directions of the party\"\n",
    "]\n",
    "\n",
    "# 이번 챕터에서 구현한 코드로 계산한 BLEU 점수\n",
    "print(bleu_score(cand.split(),list(map(lambda ref: ref.split(), refs))))\n",
    "# NLTK 패키지 구현되어져 있는 코드로 계산한 BLEU 점수\n",
    "print(bleu.nltk.translate.bleu_score.sentence_bleu(list(map(lambda ref: ref.split(), refs)),cand.split()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BLEU(BiLingual Evaluation Understudy) Score",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
